{
  "2026-01-01": "1. **Generate Synthetic Time Series Data**: Create a pandas DataFrame `df` with 800 daily entries. It should have a `date` column (daily dates starting from '2023-01-01') and a `sales_amount` column. Populate `sales_amount` with a synthetic time series, for example, a combination of a linear trend, a yearly seasonality (using `np.sin`), and some random noise (using `np.random.normal`).\n2. **Feature Engineering - Lag Features**: Create two new features in the DataFrame: `sales_amount_lag_1` (representing the sales from the previous day) and `sales_amount_lag_7` (representing sales from 7 days prior) using pandas' `shift()` method.\n3. **Feature Engineering - Rolling Statistics**: Create two more new features: `rolling_mean_3_days` (a 3-day rolling mean of `sales_amount`) and `rolling_std_7_days` (a 7-day rolling standard deviation of `sales_amount`) using pandas' `rolling()` method. Ensure you handle potential `NaN` values from rolling operations (e.g., `min_periods=1` if you want to keep early rows, or simply let `NaN`s propagate and drop later).\n4. **Handle Missing Values and Prepare for Modeling**: After creating all engineered features, drop any rows that contain `NaN` values (which will typically appear at the beginning of the DataFrame due to `shift` and `rolling` operations). Then, define the target variable `y` as the `sales_amount` column of the processed DataFrame, and the features `X` as all the engineered lag and rolling features. Display the head of the final `X` and `y` DataFrames to show the prepared dataset.\n5. **Visualize Feature-Target Relationship**: Pick one of the engineered features (e.g., `sales_amount_lag_1`) and plot its relationship with the `sales_amount` target using a `seaborn.lineplot` or `seaborn.scatterplot`. Ensure the plot has appropriate labels and a title.",
  "2026-01-02": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1500 samples, 8 features (e.g., `n_informative=6`), and 2 classes (set `random_state` for reproducibility).\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Create two distinct `sklearn.pipeline.Pipeline` objects for classification:\n    *   `pipeline_lr`: Consisting of `StandardScaler` followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'`).\n    *   `pipeline_gb`: Consisting of `StandardScaler` followed by `GradientBoostingClassifier` (set `random_state=42`).\n4. For each pipeline, define a small, focused hyperparameter grid for `sklearn.model_selection.GridSearchCV`:\n    *   For `pipeline_lr`: Tune `C` (e.g., `[0.1, 1, 10]`).\n    *   For `pipeline_gb`: Tune `n_estimators` (e.g., `[50, 100]`) and `learning_rate` (e.g., `[0.05, 0.1]`).\n5. Perform `GridSearchCV` *separately* for `pipeline_lr` and `pipeline_gb` on the training data. Use 3-fold cross-validation and `scoring='roc_auc'` for both. (Set `n_jobs=-1` for faster execution).\n6. Report the `best_params_` and `best_score_` (ROC AUC) for each model (Logistic Regression and Gradient Boosting Classifier).\n7. Using the *best estimators* obtained from `GridSearchCV` for both models, predict probabilities for the positive class (class 1) on the test set.\n8. Plot the Receiver Operating Characteristic (ROC) curve for *both* models on the *same* plot using `sklearn.metrics.RocCurveDisplay.from_estimator` (or `from_predictions` if preferred). Ensure the plot has a clear title, a legend indicating which curve belongs to which model, and displays the AUC score for each model.",
  "2026-01-03": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 4 informative features, and 2 classes (set `random_state` for reproducibility).\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Standardize the features using `sklearn.preprocessing.StandardScaler` on the training data, then transform both training and testing sets.\n4. Build a simple sequential neural network model using `tensorflow.keras.models.Sequential` for binary classification. The model should have:\n    *   An input layer matching the number of features.\n    *   One hidden `Dense` layer with `relu` activation (e.g., 16-32 units).\n    *   An output `Dense` layer with a single unit and `sigmoid` activation.\n5. Compile the model using the `adam` optimizer and `binary_crossentropy` loss.\n6. Train the model on the scaled training data for a suitable number of epochs (e.g., 30-50) and a batch size.\n7. Predict probabilities on the scaled test set. Convert these probabilities to binary class labels (0 or 1) using a threshold (e.g., 0.5).\n8. Print the `sklearn.metrics.classification_report` for the test set predictions.\n9. Generate and plot a confusion matrix using `sklearn.metrics.ConfusionMatrixDisplay.from_predictions` for the test set, clearly labeling the plot with a title (e.g., 'Confusion Matrix for Keras Binary Classifier').",
  "2026-01-04": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 5 informative features, and 2 classes (set `random_state` for reproducibility).\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Define a function `create_keras_model(optimizer='adam', units=32)` that builds and compiles a `tensorflow.keras.models.Sequential` model. This model should have an input layer matching the number of features, one `Dense` hidden layer with `relu` activation (using `units` as a parameter), and a `Dense` output layer with `sigmoid` activation. Compile it with `binary_crossentropy` loss.\n4. Wrap this Keras model using `tensorflow.keras.wrappers.scikit_learn.KerasClassifier`.\n5. Construct an `sklearn.pipeline.Pipeline` that first applies `sklearn.preprocessing.StandardScaler` and then uses the wrapped `KerasClassifier`.\n6. Define a hyperparameter grid for `sklearn.model_selection.GridSearchCV` to tune the following parameters of the Keras model within the pipeline:\n    *   `kerasclassifier__batch_size` (e.g., `[32, 64]`)\n    *   `kerasclassifier__epochs` (e.g., `[10, 20]`)\n    *   `kerasclassifier__model__units` (e.g., `[16, 32]`)\n    *   `kerasclassifier__optimizer` (e.g., `['adam', 'rmsprop']`)\n7. Perform `GridSearchCV` with 3-fold cross-validation and `scoring='roc_auc'` on the training data. (Set `n_jobs=-1` for faster execution).\n8. Report the `best_params_` and `best_score_` from `GridSearchCV`. Then, using the `best_estimator_`, predict class labels on the test set and print the `sklearn.metrics.classification_report`.",
  "2026-01-05": "1. **Generate Synthetic Time Series Data**: Create a pandas DataFrame `df` with 500-700 daily entries. It should have a `date` column (daily dates starting from '2022-01-01') and a `value` column. Populate `value` with a synthetic time series exhibiting a linear trend, strong weekly seasonality (e.g., higher values on weekends), and some random noise.\n2. **Feature Engineering - Advanced Time-Based Features**: Create the following new features in the DataFrame:\n    *   `day_of_week`: Extract the day of the week (0-6 or names) from the `date` column.\n    *   `is_weekend`: A binary flag (1 if weekend, 0 otherwise).\n    *   `exponential_moving_average_7d`: Calculate a 7-day Exponential Moving Average (EMA) of the `value` column using `df['value'].ewm(span=7, adjust=False).mean()`.\n3. **Handle Missing Values and Prepare for Modeling**: Drop any rows that contain `NaN` values (due to EMA calculation). Define the target variable `y` as the `value` column, and features `X` as `day_of_week`, `is_weekend`, and `exponential_moving_average_7d`. Convert `day_of_week` into one-hot encoded features using `pd.get_dummies()`.\n4. **Visualize Engineered Features**: Create a plot (e.g., using `seaborn.lineplot` or `seaborn.boxplot`) to visualize the relationship between `day_of_week` (or `is_weekend`) and the original `value` to confirm the seasonality. Also, plot the original `value` and the `exponential_moving_average_7d` on the same time-series plot to show the smoothing effect.\n5. **Build and Evaluate a Regression Model**: Split the dataset into training and testing sets (e.g., 80/20 split). Train a `LinearRegression` model using the engineered features. Evaluate its performance on the test set using `sklearn.metrics.mean_absolute_error` and `sklearn.metrics.r2_score`. Report both metrics.",
  "2026-01-06": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 6 informative features, and 2 classes (set `random_state` for reproducibility). Convert the features (`X`) and target (`y`) into a pandas DataFrame.\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Create two distinct `sklearn.pipeline.Pipeline` objects for classification:\n    *   `pipeline_baseline`: Consisting of `StandardScaler` followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   `pipeline_polynomial`: Consisting of `StandardScaler`, then `PolynomialFeatures(degree=2, include_bias=False)`, followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n4. Evaluate both pipelines using 5-fold cross-validation (`sklearn.model_selection.cross_val_score`) on the training data. Use `scoring='roc_auc'` for performance comparison.\n5. Report the mean and standard deviation of the ROC AUC scores for both the `pipeline_baseline` and `pipeline_polynomial`. Based on these results, briefly discuss the impact of including polynomial features on model performance for this dataset.",
  "2026-01-07": "1. **Generate Synthetic Data**: Create two pandas DataFrames:\n    *   `customers_df`: With 100-150 rows. Columns: `customer_id` (unique integers), `customer_name` (e.g., 'Customer A'), `registration_date` (random dates over the last 3 years).\n    *   `orders_df`: With 800-1000 rows. Columns: `order_id` (unique integers), `customer_id` (randomly sampled from `customers_df` IDs, ensuring some customers have no orders and others have many), `order_date` (random dates after their `registration_date`), `amount` (random floats between 10 and 1000).\n2. **Load into SQLite**: Create an in-memory SQLite database using `sqlite3` and load both `customers_df` and `orders_df` into tables named `customers` and `orders` respectively.\n3. **SQL Query - Customer Sales Aggregation and Segmentation**: Write a single SQL query that performs the following:\n    *   **Joins** the `customers` and `orders` tables.\n    *   **Aggregates** to calculate the `total_sales_amount` and `number_of_orders` for each customer. Ensure that customers with no orders are still included in the result, showing 0 for sales and orders.\n    *   **Categorizes** each customer into a `customer_segment` based on their `total_sales_amount`:\n        *   'High-Value' if `total_sales_amount` > 5000\n        *   'Medium-Value' if `total_sales_amount` between 1000 and 5000 (inclusive)\n        *   'Low-Value' if `total_sales_amount` between 1 and 999 (inclusive)\n        *   'No-Orders' if `total_sales_amount` is 0 or NULL.\n    *   The query should return `customer_id`, `customer_name`, `registration_date`, `total_sales_amount`, `number_of_orders`, and `customer_segment`.\n4. **Retrieve and Display**: Fetch the results into a pandas DataFrame. Display its head and then print the count of customers within each `customer_segment` to verify your segmentation logic.",
  "2026-01-08": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with 1000 samples, 4 numerical features, and 2 classes (`random_state=42`). Convert `X` into a pandas DataFrame and add a categorical feature named `color` (e.g., 'red', 'blue', 'green', 'yellow' with random distribution, ensuring a good mix).\n2. Introduce missing values into the `color` feature by randomly replacing approximately 15% of its values with `np.nan`.\n3. Create an `sklearn.pipeline.Pipeline` that first applies a `sklearn.compose.ColumnTransformer` for preprocessing and then fits a `sklearn.linear_model.LogisticRegression` model (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   **Inside the `ColumnTransformer`**:\n        *   For the numerical features: Apply `SimpleImputer(strategy='mean')` followed by `StandardScaler`.\n        *   For the `color` categorical feature: Apply `SimpleImputer(strategy='most_frequent')` followed by `OneHotEncoder(handle_unknown='ignore')`.\n4. Evaluate the complete pipeline's performance using 5-fold cross-validation (`sklearn.model_selection.cross_val_score`) with `accuracy` as the scoring metric.\n5. Report the mean accuracy and its standard deviation from the cross-validation.",
  "2026-01-09": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 10 features (e.g., `n_informative=5`, `n_redundant=3`, `n_repeated=2`), and 2 classes (set `random_state=42`).\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Create two distinct `sklearn.pipeline.Pipeline` objects:\n    *   `pipeline_no_fs`: Consisting of `StandardScaler` followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   `pipeline_with_fs`: Consisting of `StandardScaler`, then `sklearn.feature_selection.SelectKBest` (e.g., select `k=5` features using `score_func=sklearn.feature_selection.f_classif`), followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n4. Train both `pipeline_no_fs` and `pipeline_with_fs` on the training data.\n5. Predict probabilities for the positive class (class 1) on the test set using both trained pipelines.\n6. Calculate and report the `sklearn.metrics.roc_auc_score` for both models on the test set.\n7. Briefly discuss the impact of including the feature selection step on the model's performance for this dataset, based on the reported ROC AUC scores.",
  "2026-01-10": "1. Generate a synthetic transactional pandas DataFrame (`transactions_df`) with 1200 rows. Columns should include:\n    *   `transaction_id` (unique integers starting from 10000)\n    *   `customer_id` (e.g., 100-150 distinct customer IDs, formatted as 'C' followed by a number)\n    *   `transaction_date` (daily dates spanning 3 years, starting from '2021-01-01', ensuring multiple transactions per customer over time)\n    *   `product_category` (e.g., 5 distinct string categories: 'Electronics', 'Books', 'Groceries', 'Clothing', 'Home Goods' with varying proportions)\n    *   `amount` (random float values between 20.0 and 1000.0)\n    *   Ensure the DataFrame is sorted by `customer_id` and then `transaction_date`.\n2. Calculate the total revenue generated by each unique `customer_id`. Display the top 5 customers with the highest total revenue (customer ID and their total revenue).\n3. Create a pivot table named `monthly_category_sales` that shows the total `amount` for each `product_category` (as columns) for each month and year combination (as index). Fill any missing values in the pivot table with 0. Display the head of this pivot table.\n4. For each `customer_id`, calculate their 3-month rolling average `amount` (mean of `amount`) based on `transaction_date`. Store this as a new column `rolling_avg_3m_spend` in the original `transactions_df`. Ensure the rolling average is calculated *within each customer group* and correctly considers the time ordering. Display the head of the `transactions_df` including this new column.",
  "2026-01-11": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with 1000 samples, 10 informative features, and 2 classes (set `random_state=42`).\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Create two distinct `sklearn.pipeline.Pipeline` objects for classification:\n    *   `pipeline_no_pca`: Consisting of `StandardScaler` followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   `pipeline_with_pca`: Consisting of `StandardScaler`, then `sklearn.decomposition.PCA(n_components=2)`, followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n4. Train both `pipeline_no_pca` and `pipeline_with_pca` on the training data.\n5. Predict probabilities for the positive class (class 1) on the test set using both trained pipelines. Calculate and report the `sklearn.metrics.roc_auc_score` for both models on the test set.\n6. **Visualize the transformed data**: Apply the `StandardScaler` and `PCA(n_components=2)` steps from `pipeline_with_pca` (using `.fit_transform()` on training and `.transform()` on testing data) to the *test set features*. Create a scatter plot of the two principal components, coloring the points by their actual class labels (`y_test`). Add appropriate titles and labels.\n7. Briefly discuss the impact of including PCA on the model's performance for this dataset and what the visualization reveals about the data separation.",
  "2026-01-12": "1. Generate a synthetic 2D dataset for clustering using `sklearn.datasets.make_blobs` with 1000 samples, 2 features, and 4 cluster centers. Set `random_state=42` for reproducibility.\n2. Apply `sklearn.preprocessing.StandardScaler` to the generated features.\n3. Perform K-Means clustering on the scaled data. Initialize `sklearn.cluster.KMeans` with `n_clusters=4` and `random_state=42` (set `n_init='auto'` or `n_init=10` to suppress warnings for older scikit-learn versions). Fit the model and obtain the cluster labels.\n4. Calculate and report the `sklearn.metrics.silhouette_score` using the scaled features and the obtained cluster labels.\n5. Create a scatter plot of the 2D features (either original or scaled), coloring each data point according to its assigned K-Means cluster. Ensure the plot has appropriate axis labels, a clear title (e.g., 'K-Means Clusters (Silhouette Score: X.XX)'), and a legend (if distinct colors are used).\n6. Briefly explain what the Silhouette Score measures and why it's a useful metric for evaluating clustering results, especially when true labels are not available.",
  "2026-01-13": "1. **Generate Synthetic Time Series Data**: Create a pandas DataFrame `df` with 700-900 daily entries. It should have a `date` column (daily dates starting from '2022-01-01') and a `sales` column. Populate `sales` with a synthetic time series exhibiting a clear linear trend, a strong monthly seasonality (e.g., using `np.sin` or `np.cos` with a 30-day period), and some random noise.\n2. **Feature Engineering for Forecasting**: Create the following new features in the DataFrame:\n    *   `day_of_week`: Integer representing the day of the week (0=Monday, 6=Sunday).\n    *   `month`: Integer representing the month.\n    *   `day_of_year`: Integer representing the day of the year.\n    *   `sales_lag_7`: The `sales` value from 7 days prior.\n3. **Prepare Data and Chronological Split**: Drop any rows that contain `NaN` values resulting from lag feature creation. Define features `X` (all engineered features) and target `y` (`sales`). Then, split the data into training and testing sets *chronologically*, using the last 90 days of data for the test set. Ensure `X_train`, `X_test`, `y_train`, `y_test` are correctly defined.\n4. **Train and Evaluate a Regression Model**: Train an `sklearn.ensemble.RandomForestRegressor` (set `random_state=42`, `n_estimators=100`) on the training data. Predict `sales` for the test set. Calculate and report the `sklearn.metrics.mean_absolute_error` (MAE) and `sklearn.metrics.r2_score` of the model on the test set.\n5. **Visualize the Forecast**: Create a single line plot showing the actual `sales` values for the test period and the model's predicted `sales` values for the same period. Label the axes, add a title like 'Sales Forecast vs. Actuals', and include a legend to distinguish between actual and predicted lines.",
  "2026-01-14": "1. Generate a synthetic imbalanced binary classification dataset using `sklearn.datasets.make_classification` with 1200 samples, 8 features, 2 classes, and `weights=[0.9, 0.1]` to create an imbalance. Set `random_state=42` for reproducibility.\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Create an `imblearn.pipeline.Pipeline` that includes the following steps:\n    *   `sklearn.preprocessing.StandardScaler`\n    *   `imblearn.over_sampling.SMOTE` (set `random_state=42`)\n    *   `sklearn.linear_model.LogisticRegression` (set `random_state=42`, `solver='liblinear'`)\n4. Train the pipeline on the training data.\n5. Predict probabilities for the positive class (class 1) on the scaled test set. Then, convert these probabilities to binary class labels (0 or 1) using a threshold of 0.5.\n6. Calculate and print the `sklearn.metrics.classification_report` for the test set predictions.\n7. Plot the Precision-Recall curve for the model on the test set using `sklearn.metrics.PrecisionRecallDisplay.from_predictions`. Ensure the plot has a clear title, labels, and displays the average precision score."
}