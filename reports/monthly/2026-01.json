{
  "2026-01-01": "1. **Generate Synthetic Time Series Data**: Create a pandas DataFrame `df` with 800 daily entries. It should have a `date` column (daily dates starting from '2023-01-01') and a `sales_amount` column. Populate `sales_amount` with a synthetic time series, for example, a combination of a linear trend, a yearly seasonality (using `np.sin`), and some random noise (using `np.random.normal`).\n2. **Feature Engineering - Lag Features**: Create two new features in the DataFrame: `sales_amount_lag_1` (representing the sales from the previous day) and `sales_amount_lag_7` (representing sales from 7 days prior) using pandas' `shift()` method.\n3. **Feature Engineering - Rolling Statistics**: Create two more new features: `rolling_mean_3_days` (a 3-day rolling mean of `sales_amount`) and `rolling_std_7_days` (a 7-day rolling standard deviation of `sales_amount`) using pandas' `rolling()` method. Ensure you handle potential `NaN` values from rolling operations (e.g., `min_periods=1` if you want to keep early rows, or simply let `NaN`s propagate and drop later).\n4. **Handle Missing Values and Prepare for Modeling**: After creating all engineered features, drop any rows that contain `NaN` values (which will typically appear at the beginning of the DataFrame due to `shift` and `rolling` operations). Then, define the target variable `y` as the `sales_amount` column of the processed DataFrame, and the features `X` as all the engineered lag and rolling features. Display the head of the final `X` and `y` DataFrames to show the prepared dataset.\n5. **Visualize Feature-Target Relationship**: Pick one of the engineered features (e.g., `sales_amount_lag_1`) and plot its relationship with the `sales_amount` target using a `seaborn.lineplot` or `seaborn.scatterplot`. Ensure the plot has appropriate labels and a title.",
  "2026-01-02": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1500 samples, 8 features (e.g., `n_informative=6`), and 2 classes (set `random_state` for reproducibility).\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Create two distinct `sklearn.pipeline.Pipeline` objects for classification:\n    *   `pipeline_lr`: Consisting of `StandardScaler` followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'`).\n    *   `pipeline_gb`: Consisting of `StandardScaler` followed by `GradientBoostingClassifier` (set `random_state=42`).\n4. For each pipeline, define a small, focused hyperparameter grid for `sklearn.model_selection.GridSearchCV`:\n    *   For `pipeline_lr`: Tune `C` (e.g., `[0.1, 1, 10]`).\n    *   For `pipeline_gb`: Tune `n_estimators` (e.g., `[50, 100]`) and `learning_rate` (e.g., `[0.05, 0.1]`).\n5. Perform `GridSearchCV` *separately* for `pipeline_lr` and `pipeline_gb` on the training data. Use 3-fold cross-validation and `scoring='roc_auc'` for both. (Set `n_jobs=-1` for faster execution).\n6. Report the `best_params_` and `best_score_` (ROC AUC) for each model (Logistic Regression and Gradient Boosting Classifier).\n7. Using the *best estimators* obtained from `GridSearchCV` for both models, predict probabilities for the positive class (class 1) on the test set.\n8. Plot the Receiver Operating Characteristic (ROC) curve for *both* models on the *same* plot using `sklearn.metrics.RocCurveDisplay.from_estimator` (or `from_predictions` if preferred). Ensure the plot has a clear title, a legend indicating which curve belongs to which model, and displays the AUC score for each model.",
  "2026-01-03": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 4 informative features, and 2 classes (set `random_state` for reproducibility).\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Standardize the features using `sklearn.preprocessing.StandardScaler` on the training data, then transform both training and testing sets.\n4. Build a simple sequential neural network model using `tensorflow.keras.models.Sequential` for binary classification. The model should have:\n    *   An input layer matching the number of features.\n    *   One hidden `Dense` layer with `relu` activation (e.g., 16-32 units).\n    *   An output `Dense` layer with a single unit and `sigmoid` activation.\n5. Compile the model using the `adam` optimizer and `binary_crossentropy` loss.\n6. Train the model on the scaled training data for a suitable number of epochs (e.g., 30-50) and a batch size.\n7. Predict probabilities on the scaled test set. Convert these probabilities to binary class labels (0 or 1) using a threshold (e.g., 0.5).\n8. Print the `sklearn.metrics.classification_report` for the test set predictions.\n9. Generate and plot a confusion matrix using `sklearn.metrics.ConfusionMatrixDisplay.from_predictions` for the test set, clearly labeling the plot with a title (e.g., 'Confusion Matrix for Keras Binary Classifier').",
  "2026-01-04": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 5 informative features, and 2 classes (set `random_state` for reproducibility).\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Define a function `create_keras_model(optimizer='adam', units=32)` that builds and compiles a `tensorflow.keras.models.Sequential` model. This model should have an input layer matching the number of features, one `Dense` hidden layer with `relu` activation (using `units` as a parameter), and a `Dense` output layer with `sigmoid` activation. Compile it with `binary_crossentropy` loss.\n4. Wrap this Keras model using `tensorflow.keras.wrappers.scikit_learn.KerasClassifier`.\n5. Construct an `sklearn.pipeline.Pipeline` that first applies `sklearn.preprocessing.StandardScaler` and then uses the wrapped `KerasClassifier`.\n6. Define a hyperparameter grid for `sklearn.model_selection.GridSearchCV` to tune the following parameters of the Keras model within the pipeline:\n    *   `kerasclassifier__batch_size` (e.g., `[32, 64]`)\n    *   `kerasclassifier__epochs` (e.g., `[10, 20]`)\n    *   `kerasclassifier__model__units` (e.g., `[16, 32]`)\n    *   `kerasclassifier__optimizer` (e.g., `['adam', 'rmsprop']`)\n7. Perform `GridSearchCV` with 3-fold cross-validation and `scoring='roc_auc'` on the training data. (Set `n_jobs=-1` for faster execution).\n8. Report the `best_params_` and `best_score_` from `GridSearchCV`. Then, using the `best_estimator_`, predict class labels on the test set and print the `sklearn.metrics.classification_report`.",
  "2026-01-05": "1. **Generate Synthetic Time Series Data**: Create a pandas DataFrame `df` with 500-700 daily entries. It should have a `date` column (daily dates starting from '2022-01-01') and a `value` column. Populate `value` with a synthetic time series exhibiting a linear trend, strong weekly seasonality (e.g., higher values on weekends), and some random noise.\n2. **Feature Engineering - Advanced Time-Based Features**: Create the following new features in the DataFrame:\n    *   `day_of_week`: Extract the day of the week (0-6 or names) from the `date` column.\n    *   `is_weekend`: A binary flag (1 if weekend, 0 otherwise).\n    *   `exponential_moving_average_7d`: Calculate a 7-day Exponential Moving Average (EMA) of the `value` column using `df['value'].ewm(span=7, adjust=False).mean()`.\n3. **Handle Missing Values and Prepare for Modeling**: Drop any rows that contain `NaN` values (due to EMA calculation). Define the target variable `y` as the `value` column, and features `X` as `day_of_week`, `is_weekend`, and `exponential_moving_average_7d`. Convert `day_of_week` into one-hot encoded features using `pd.get_dummies()`.\n4. **Visualize Engineered Features**: Create a plot (e.g., using `seaborn.lineplot` or `seaborn.boxplot`) to visualize the relationship between `day_of_week` (or `is_weekend`) and the original `value` to confirm the seasonality. Also, plot the original `value` and the `exponential_moving_average_7d` on the same time-series plot to show the smoothing effect.\n5. **Build and Evaluate a Regression Model**: Split the dataset into training and testing sets (e.g., 80/20 split). Train a `LinearRegression` model using the engineered features. Evaluate its performance on the test set using `sklearn.metrics.mean_absolute_error` and `sklearn.metrics.r2_score`. Report both metrics.",
  "2026-01-06": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 6 informative features, and 2 classes (set `random_state` for reproducibility). Convert the features (`X`) and target (`y`) into a pandas DataFrame.\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Create two distinct `sklearn.pipeline.Pipeline` objects for classification:\n    *   `pipeline_baseline`: Consisting of `StandardScaler` followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   `pipeline_polynomial`: Consisting of `StandardScaler`, then `PolynomialFeatures(degree=2, include_bias=False)`, followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n4. Evaluate both pipelines using 5-fold cross-validation (`sklearn.model_selection.cross_val_score`) on the training data. Use `scoring='roc_auc'` for performance comparison.\n5. Report the mean and standard deviation of the ROC AUC scores for both the `pipeline_baseline` and `pipeline_polynomial`. Based on these results, briefly discuss the impact of including polynomial features on model performance for this dataset.",
  "2026-01-07": "1. **Generate Synthetic Data**: Create two pandas DataFrames:\n    *   `customers_df`: With 100-150 rows. Columns: `customer_id` (unique integers), `customer_name` (e.g., 'Customer A'), `registration_date` (random dates over the last 3 years).\n    *   `orders_df`: With 800-1000 rows. Columns: `order_id` (unique integers), `customer_id` (randomly sampled from `customers_df` IDs, ensuring some customers have no orders and others have many), `order_date` (random dates after their `registration_date`), `amount` (random floats between 10 and 1000).\n2. **Load into SQLite**: Create an in-memory SQLite database using `sqlite3` and load both `customers_df` and `orders_df` into tables named `customers` and `orders` respectively.\n3. **SQL Query - Customer Sales Aggregation and Segmentation**: Write a single SQL query that performs the following:\n    *   **Joins** the `customers` and `orders` tables.\n    *   **Aggregates** to calculate the `total_sales_amount` and `number_of_orders` for each customer. Ensure that customers with no orders are still included in the result, showing 0 for sales and orders.\n    *   **Categorizes** each customer into a `customer_segment` based on their `total_sales_amount`:\n        *   'High-Value' if `total_sales_amount` > 5000\n        *   'Medium-Value' if `total_sales_amount` between 1000 and 5000 (inclusive)\n        *   'Low-Value' if `total_sales_amount` between 1 and 999 (inclusive)\n        *   'No-Orders' if `total_sales_amount` is 0 or NULL.\n    *   The query should return `customer_id`, `customer_name`, `registration_date`, `total_sales_amount`, `number_of_orders`, and `customer_segment`.\n4. **Retrieve and Display**: Fetch the results into a pandas DataFrame. Display its head and then print the count of customers within each `customer_segment` to verify your segmentation logic.",
  "2026-01-08": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with 1000 samples, 4 numerical features, and 2 classes (`random_state=42`). Convert `X` into a pandas DataFrame and add a categorical feature named `color` (e.g., 'red', 'blue', 'green', 'yellow' with random distribution, ensuring a good mix).\n2. Introduce missing values into the `color` feature by randomly replacing approximately 15% of its values with `np.nan`.\n3. Create an `sklearn.pipeline.Pipeline` that first applies a `sklearn.compose.ColumnTransformer` for preprocessing and then fits a `sklearn.linear_model.LogisticRegression` model (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   **Inside the `ColumnTransformer`**:\n        *   For the numerical features: Apply `SimpleImputer(strategy='mean')` followed by `StandardScaler`.\n        *   For the `color` categorical feature: Apply `SimpleImputer(strategy='most_frequent')` followed by `OneHotEncoder(handle_unknown='ignore')`.\n4. Evaluate the complete pipeline's performance using 5-fold cross-validation (`sklearn.model_selection.cross_val_score`) with `accuracy` as the scoring metric.\n5. Report the mean accuracy and its standard deviation from the cross-validation.",
  "2026-01-09": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 10 features (e.g., `n_informative=5`, `n_redundant=3`, `n_repeated=2`), and 2 classes (set `random_state=42`).\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Create two distinct `sklearn.pipeline.Pipeline` objects:\n    *   `pipeline_no_fs`: Consisting of `StandardScaler` followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   `pipeline_with_fs`: Consisting of `StandardScaler`, then `sklearn.feature_selection.SelectKBest` (e.g., select `k=5` features using `score_func=sklearn.feature_selection.f_classif`), followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n4. Train both `pipeline_no_fs` and `pipeline_with_fs` on the training data.\n5. Predict probabilities for the positive class (class 1) on the test set using both trained pipelines.\n6. Calculate and report the `sklearn.metrics.roc_auc_score` for both models on the test set.\n7. Briefly discuss the impact of including the feature selection step on the model's performance for this dataset, based on the reported ROC AUC scores.",
  "2026-01-10": "1. Generate a synthetic transactional pandas DataFrame (`transactions_df`) with 1200 rows. Columns should include:\n    *   `transaction_id` (unique integers starting from 10000)\n    *   `customer_id` (e.g., 100-150 distinct customer IDs, formatted as 'C' followed by a number)\n    *   `transaction_date` (daily dates spanning 3 years, starting from '2021-01-01', ensuring multiple transactions per customer over time)\n    *   `product_category` (e.g., 5 distinct string categories: 'Electronics', 'Books', 'Groceries', 'Clothing', 'Home Goods' with varying proportions)\n    *   `amount` (random float values between 20.0 and 1000.0)\n    *   Ensure the DataFrame is sorted by `customer_id` and then `transaction_date`.\n2. Calculate the total revenue generated by each unique `customer_id`. Display the top 5 customers with the highest total revenue (customer ID and their total revenue).\n3. Create a pivot table named `monthly_category_sales` that shows the total `amount` for each `product_category` (as columns) for each month and year combination (as index). Fill any missing values in the pivot table with 0. Display the head of this pivot table.\n4. For each `customer_id`, calculate their 3-month rolling average `amount` (mean of `amount`) based on `transaction_date`. Store this as a new column `rolling_avg_3m_spend` in the original `transactions_df`. Ensure the rolling average is calculated *within each customer group* and correctly considers the time ordering. Display the head of the `transactions_df` including this new column.",
  "2026-01-11": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with 1000 samples, 10 informative features, and 2 classes (set `random_state=42`).\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Create two distinct `sklearn.pipeline.Pipeline` objects for classification:\n    *   `pipeline_no_pca`: Consisting of `StandardScaler` followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   `pipeline_with_pca`: Consisting of `StandardScaler`, then `sklearn.decomposition.PCA(n_components=2)`, followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n4. Train both `pipeline_no_pca` and `pipeline_with_pca` on the training data.\n5. Predict probabilities for the positive class (class 1) on the test set using both trained pipelines. Calculate and report the `sklearn.metrics.roc_auc_score` for both models on the test set.\n6. **Visualize the transformed data**: Apply the `StandardScaler` and `PCA(n_components=2)` steps from `pipeline_with_pca` (using `.fit_transform()` on training and `.transform()` on testing data) to the *test set features*. Create a scatter plot of the two principal components, coloring the points by their actual class labels (`y_test`). Add appropriate titles and labels.\n7. Briefly discuss the impact of including PCA on the model's performance for this dataset and what the visualization reveals about the data separation.",
  "2026-01-12": "1. Generate a synthetic 2D dataset for clustering using `sklearn.datasets.make_blobs` with 1000 samples, 2 features, and 4 cluster centers. Set `random_state=42` for reproducibility.\n2. Apply `sklearn.preprocessing.StandardScaler` to the generated features.\n3. Perform K-Means clustering on the scaled data. Initialize `sklearn.cluster.KMeans` with `n_clusters=4` and `random_state=42` (set `n_init='auto'` or `n_init=10` to suppress warnings for older scikit-learn versions). Fit the model and obtain the cluster labels.\n4. Calculate and report the `sklearn.metrics.silhouette_score` using the scaled features and the obtained cluster labels.\n5. Create a scatter plot of the 2D features (either original or scaled), coloring each data point according to its assigned K-Means cluster. Ensure the plot has appropriate axis labels, a clear title (e.g., 'K-Means Clusters (Silhouette Score: X.XX)'), and a legend (if distinct colors are used).\n6. Briefly explain what the Silhouette Score measures and why it's a useful metric for evaluating clustering results, especially when true labels are not available.",
  "2026-01-13": "1. **Generate Synthetic Time Series Data**: Create a pandas DataFrame `df` with 700-900 daily entries. It should have a `date` column (daily dates starting from '2022-01-01') and a `sales` column. Populate `sales` with a synthetic time series exhibiting a clear linear trend, a strong monthly seasonality (e.g., using `np.sin` or `np.cos` with a 30-day period), and some random noise.\n2. **Feature Engineering for Forecasting**: Create the following new features in the DataFrame:\n    *   `day_of_week`: Integer representing the day of the week (0=Monday, 6=Sunday).\n    *   `month`: Integer representing the month.\n    *   `day_of_year`: Integer representing the day of the year.\n    *   `sales_lag_7`: The `sales` value from 7 days prior.\n3. **Prepare Data and Chronological Split**: Drop any rows that contain `NaN` values resulting from lag feature creation. Define features `X` (all engineered features) and target `y` (`sales`). Then, split the data into training and testing sets *chronologically*, using the last 90 days of data for the test set. Ensure `X_train`, `X_test`, `y_train`, `y_test` are correctly defined.\n4. **Train and Evaluate a Regression Model**: Train an `sklearn.ensemble.RandomForestRegressor` (set `random_state=42`, `n_estimators=100`) on the training data. Predict `sales` for the test set. Calculate and report the `sklearn.metrics.mean_absolute_error` (MAE) and `sklearn.metrics.r2_score` of the model on the test set.\n5. **Visualize the Forecast**: Create a single line plot showing the actual `sales` values for the test period and the model's predicted `sales` values for the same period. Label the axes, add a title like 'Sales Forecast vs. Actuals', and include a legend to distinguish between actual and predicted lines.",
  "2026-01-14": "1. Generate a synthetic imbalanced binary classification dataset using `sklearn.datasets.make_classification` with 1200 samples, 8 features, 2 classes, and `weights=[0.9, 0.1]` to create an imbalance. Set `random_state=42` for reproducibility.\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Create an `imblearn.pipeline.Pipeline` that includes the following steps:\n    *   `sklearn.preprocessing.StandardScaler`\n    *   `imblearn.over_sampling.SMOTE` (set `random_state=42`)\n    *   `sklearn.linear_model.LogisticRegression` (set `random_state=42`, `solver='liblinear'`)\n4. Train the pipeline on the training data.\n5. Predict probabilities for the positive class (class 1) on the scaled test set. Then, convert these probabilities to binary class labels (0 or 1) using a threshold of 0.5.\n6. Calculate and print the `sklearn.metrics.classification_report` for the test set predictions.\n7. Plot the Precision-Recall curve for the model on the test set using `sklearn.metrics.PrecisionRecallDisplay.from_predictions`. Ensure the plot has a clear title, labels, and displays the average precision score.",
  "2026-01-15": "1. **Generate Synthetic Transactional Data**: Create a pandas DataFrame named `transactions_df` with 1000-1200 rows. Columns should include:\n    *   `transaction_id` (unique integer IDs starting from 10000).\n    *   `customer_id` (e.g., 100-150 distinct customer IDs, formatted as 'C' followed by a number).\n    *   `transaction_date` (daily dates spanning 3 years, starting from '2021-01-01', ensuring multiple transactions per customer over time).\n    *   `amount` (random float values, e.g., between 20.0 and 1000.0).\n    *   Ensure the DataFrame is sorted by `customer_id` and then `transaction_date`.\n2. **Load into SQLite**: Create an in-memory SQLite database using `sqlite3` and load the `transactions_df` into a table named `transactions`.\n3. **SQL Analytics (Advanced Window Functions - LAG/LEAD and Date Difference)**: Write a single SQL query that performs the following for *each customer*, ordered by their `transaction_date`:\n    *   Calculates `previous_transaction_amount`: The `amount` of the immediately preceding transaction.\n    *   Calculates `next_transaction_amount`: The `amount` of the immediately following transaction.\n    *   Calculates `days_since_prev_transaction`: The number of days between the current `transaction_date` and the `transaction_date` of the immediately preceding transaction. If it's the first transaction for a customer, this should be `NULL`.\n    *   The query should return `transaction_id`, `customer_id`, `transaction_date`, `amount`, `previous_transaction_amount`, `next_transaction_amount`, and `days_since_prev_transaction`.\n4. **Retrieve and Display**: Fetch the results into a pandas DataFrame. Display its head and briefly describe what the `LAG` and `LEAD` functions achieve in this context.",
  "2026-01-16": "1. **Generate Synthetic Data**: Create a pandas DataFrame `X_df` with 1000 samples for a binary classification problem using `sklearn.datasets.make_classification` (e.g., 5 informative features, `random_state=42`). Let `y` be the target variable. Add two new columns to `X_df`:\n    *   `product_code`: A categorical string feature with variations like 'TYPE-A_v1', 'type-B-v2', 'TYPE-A v3', 'TYPE_C_v1', 'Type-A'. Ensure a mix of 3-4 distinct 'types' (A, B, C, D) with messy suffixes/formats and inconsistent casing.\n    *   `description`: A free-text string column. Populate it with short descriptions that sometimes include keywords like 'urgent', 'fragile', 'standard' (e.g., 'Product with urgent delivery', 'Standard item', 'Fragile glass product').\n2. **Feature Engineering - String and Text Processing**: Create new features in `X_df` (or a copy) based on the new string columns:\n    *   `product_type_clean`: Extract the clean 'TYPE-X' part from `product_code` (e.g., 'TYPE-A', 'TYPE-B', 'TYPE-C') and convert it to consistent uppercase. (Hint: Use regex to extract 'TYPE-letter').\n    *   `is_urgent`: A binary (0 or 1) feature, 1 if 'urgent' is found in `description` (case-insensitive), 0 otherwise.\n3. **ML Pipeline with ColumnTransformer**: Split the processed `X_df` (including your new engineered features) and `y` into training and testing sets (e.g., 70/30 split) using `train_test_split`.\n    Create an `sklearn.pipeline.Pipeline` that uses `sklearn.compose.ColumnTransformer` for preprocessing:\n    *   For the original numerical features (from `make_classification`): Apply `StandardScaler`.\n    *   For `product_type_clean`: Apply `OneHotEncoder(handle_unknown='ignore')`.\n    *   For `is_urgent`: Pass through (no transformation).\n    *   Follow the `ColumnTransformer` with `sklearn.linear_model.LogisticRegression` (set `random_state=42`, `solver='liblinear'`).\n4. **Evaluate Performance**: Evaluate the complete pipeline's performance using 5-fold cross-validation (`sklearn.model_selection.cross_val_score`) on the *training data*. Use `scoring='roc_auc'` as the metric. Report the mean and standard deviation of the ROC AUC scores.",
  "2026-01-17": "1. **Generate Synthetic Transactional Data**: Create a pandas DataFrame `transactions_df` with 800-1000 rows. Columns should include:\n    *   `transaction_id` (unique integers)\n    *   `transaction_date` (daily dates spanning 2-3 years, starting from '2022-01-01')\n    *   `product_category` (e.g., 4-5 distinct string categories like 'Electronics', 'Books', 'Groceries', 'Clothing', but with inconsistent casing and minor variations, e.g., 'electronics', 'Book', 'groceries item', 'clothes').\n    *   `description` (short text descriptions that sometimes contain keywords like 'discount', 'premium', 'sale').\n    *   `amount` (random float values between 10.0 and 500.0).\n    Ensure the DataFrame is sorted by `product_category` and then `transaction_date`.\n2. **Advanced Feature Engineering**: \n    *   **Clean Categorical**: Create a new column `clean_category` by standardizing the `product_category` names (e.g., 'electronics', 'Electronics', 'ELEC' all map to 'Electronics').\n    *   **Datetime Features**: Extract `month` and `day_of_week` (integer 0-6) from `transaction_date`.\n    *   **Text Feature**: Create `is_discount` (binary: 1 if 'discount' or 'sale' is found in `description` case-insensitively, 0 otherwise).\n    *   **Grouped Lag Feature**: For each `clean_category`, calculate `lagged_amount_1d`, which is the `amount` from the previous day for that *specific category*. Fill `NaN` values (e.g., with 0, or by propagating the last valid observation forward and then filling remaining with 0 if needed for initial values).\n3. **Chronological Data Split**: Define features `X` (all engineered features created in step 2) and target `y` (the original `amount` column). Split the data chronologically, using the last 60 days of data for the test set. Ensure `X_train`, `X_test`, `y_train`, `y_test` are correctly defined.\n4. **ML Pipeline with ColumnTransformer**: Create an `sklearn.pipeline.Pipeline` that uses `sklearn.compose.ColumnTransformer` for preprocessing and `sklearn.ensemble.RandomForestRegressor` as the final estimator (set `random_state=42`, `n_estimators=100`).\n    *   **Inside the `ColumnTransformer`**:\n        *   For numerical features (`lagged_amount_1d`): Apply `SimpleImputer(strategy='mean')` followed by `StandardScaler`.\n        *   For categorical features (`clean_category`, `month`, `day_of_week`): Apply `OneHotEncoder(handle_unknown='ignore')`.\n        *   For the binary feature (`is_discount`): Use `Passthrough`.\n5. **Train, Predict, and Evaluate**: Train the pipeline on the training data (`X_train`, `y_train`). Predict `amount` for the test set (`X_test`). Calculate and report the `sklearn.metrics.mean_absolute_error` (MAE) and `sklearn.metrics.r2_score`.\n6. **Visualize Forecast**: Create a single line plot showing the actual `amount` values for the test period and the model's predicted `amount` values for the same period. Label the axes, add a title like 'Actual vs. Predicted Amounts for Test Set', and include a legend.",
  "2026-01-18": "1. **Generate Synthetic Data (pandas/numpy)**: Create two pandas DataFrames:\n    *   `customer_profiles_df`: With 500 rows. Columns: `customer_id` (unique integers), `age` (random ints 18-70), `income` (random floats 20000-150000), `is_churn` (binary target, 0 or 1, with a slight imbalance, e.g., 80/20 split).\n    *   `transactions_df`: With 2000-3000 rows. Columns: `transaction_id` (unique integers), `customer_id` (randomly sampled from `customer_profiles_df` IDs, ensuring some customers have no transactions and others have many), `transaction_date` (random dates over the last 3 years), `amount` (random floats between 10.0 and 1000.0).\n2. **Load into SQLite**: Create an in-memory SQLite database using `sqlite3` and load `customer_profiles_df` into a table named `customers` and `transactions_df` into `transactions`.\n3. **SQL Feature Engineering**: Write a single SQL query that performs the following:\n    *   **Joins** the `customers` and `transactions` tables.\n    *   **Aggregates** to calculate `total_spend`, `avg_spend_per_transaction`, and `num_transactions` for each customer.\n    *   **Ensures** that customers with no transactions are still included in the result, showing 0 for their aggregated spend/transaction counts.\n    *   Returns `customer_id`, `total_spend`, `avg_spend_per_transaction`, `num_transactions`.\n4. **Retrieve and Merge (pandas)**: Fetch the results of the SQL query into a pandas DataFrame. Then, merge this aggregated DataFrame with the original `customer_profiles_df` on `customer_id`.\n5. **Data Visualization**: Create a histogram or kernel density plot of `total_spend`, differentiated by `is_churn` (e.g., using `hue` in seaborn or `plt.hist` with different colors) to visually inspect the relationship.\n6. **ML Pipeline & Evaluation**: \n    *   Define features `X` (`age`, `income`, `total_spend`, `avg_spend_per_transaction`, `num_transactions`) and target `y` (`is_churn`) from the merged DataFrame.\n    *   Split the data into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n    *   Create an `sklearn.pipeline.Pipeline` consisting of `sklearn.preprocessing.StandardScaler` followed by `sklearn.linear_model.LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   Train the pipeline on the training data. Predict probabilities for the positive class (class 1) on the test set.\n    *   Calculate and print the `sklearn.metrics.roc_auc_score` for the test set predictions.\n    *   Generate and display an ROC curve for the model using `sklearn.metrics.RocCurveDisplay.from_estimator` with the trained pipeline and test data.",
  "2026-01-19": "1. **Generate Synthetic Data**: Create two pandas DataFrames:\n    *   `users_df`: With 500 rows. Columns: `user_id` (unique integers), `signup_date` (random dates over the last 3 years), `region` (e.g., 'North', 'South', 'East', 'West' with random distribution), `plan_type` (e.g., 'Basic', 'Premium', 'Pro' with random distribution), `churn` (binary target, 0 or 1, with an approximate 20% churn rate).\n    *   `events_df`: With 3000-5000 rows. Columns: `event_id` (unique integers), `user_id` (randomly sampled from `users_df` IDs, ensuring some users have many events and a few have no events), `event_date` (random dates occurring *after* their respective `signup_date`), `event_type` (e.g., 'login', 'view_item', 'purchase', 'cancel_subscription' with varying frequencies).\n2. **Load into SQLite**: Create an in-memory SQLite database using `sqlite3` and load `users_df` into a table named `users` and `events_df` into `events`.\n3. **SQL Feature Engineering (User Activity Aggregation)**: First, determine the `analysis_end_date` by finding the maximum `event_date` in your `events_df` (using pandas). Then, write a single SQL query that performs the following:\n    *   **Joins** `users` and `events` tables.\n    *   **Aggregates** user-level features: `total_events` (count of all events), `days_since_last_event` (number of days between the `analysis_end_date` and the user's latest `event_date`), `num_logins` (count of 'login' events), `num_purchases` (count of 'purchase' events).\n    *   **Ensures** that all users are included, even those with no events, showing appropriate default values (e.g., 0 for counts, `NULL` for `days_since_last_event` if no events).\n    *   The query should return `user_id`, `total_events`, `days_since_last_event`, `num_logins`, `num_purchases`.\n4. **Retrieve, Merge, and Final Data Prep (Pandas)**:\n    *   Fetch the SQL query results into a pandas DataFrame.\n    *   Merge this aggregated DataFrame with the original `users_df` on `user_id`.\n    *   Handle `NaN` values resulting from the SQL query: Fill `total_events`, `num_logins`, `num_purchases` with 0 for users with no events. For `days_since_last_event` (for users with no events), fill with a large sentinel value (e.g., `365 * 5` or 1825 to represent 5 years of inactivity).\n    *   Create a new feature `account_age_days`: Calculate the number of days between `signup_date` and the `analysis_end_date` (from step 3).\n    *   Define features `X` (all numerical + `region`, `plan_type`) and target `y` (`churn`). Split into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n5. **Data Visualization**: Create two separate violin plots (or box plots) to visualize the relationship between key engineered features and churn:\n    *   `days_since_last_event` vs `churn`.\n    *   `num_purchases` vs `churn`.\n    Ensure plots have appropriate labels and titles.\n6. **ML Pipeline & Evaluation**: \n    *   Create an `sklearn.pipeline.Pipeline` with a `ColumnTransformer` for preprocessing:\n        *   For numerical features (e.g., `total_events`, `days_since_last_event`, `num_logins`, `num_purchases`, `account_age_days`): Apply `SimpleImputer(strategy='mean')` followed by `StandardScaler`.\n        *   For categorical features (`region`, `plan_type`): Apply `OneHotEncoder(handle_unknown='ignore')`.\n    *   The final estimator in the pipeline should be `sklearn.linear_model.LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   Train the pipeline on the training data. Predict on the test set.\n    *   Calculate and print the `sklearn.metrics.roc_auc_score` and `sklearn.metrics.accuracy_score` for the test set predictions.",
  "2026-01-20": "1. **Generate Synthetic Data**: Create three pandas DataFrames:\n    *   `products_df`: 100-150 rows. Columns: `product_id` (unique int), `category` (e.g., 'Electronics', 'books', 'Home Goods', 'CLOTHING', 'elec' with inconsistent casing/formats), `base_price` (random floats 50-500).\n    *   `orders_df`: 800-1000 rows. Columns: `order_id` (unique int), `product_id` (randomly sampled from `products_df` IDs), `order_date` (random dates over last 2 years), `quantity` (random int 1-5).\n    *   `reviews_df`: 700-900 rows. Columns: `review_id` (unique int), `order_id` (randomly sampled from `orders_df` IDs, ensuring some orders have no review and some products have multiple reviews), `rating` (random int 1-5, this will be our target), `review_text` (short text containing keywords like 'good', 'excellent', 'fast', 'slow', 'defective', 'broken' randomly, mixed with generic words).\n\n2. **Load into SQLite & SQL Analytics**: Create an in-memory SQLite database and load `products_df`, `orders_df`, and `reviews_df` into tables. Write a single SQL query that joins these three tables. For each review, calculate the `line_item_total` (`orders.quantity * products.base_price`). The query should return `review_id`, `rating`, `review_text`, `order_date`, `category`, and `line_item_total`.\n\n3. **Feature Engineering (Pandas)**: Fetch the SQL query results into a pandas DataFrame. Create the following new features:\n    *   `clean_category`: Standardize `category` names (e.g., 'Electronics', 'Books', 'Home Goods', 'Clothing') by converting to title case and handling variations (e.g., 'elec' -> 'Electronics'). (Hint: Use string methods like `.str.title()` and `.str.replace()` or a custom mapping function).\n    *   `has_positive_feedback`: Binary (1 if 'good' or 'excellent' (case-insensitive) is found in `review_text`, 0 otherwise).\n    *   `has_negative_feedback`: Binary (1 if 'slow', 'defective', or 'broken' (case-insensitive) is found in `review_text`, 0 otherwise).\n    *   `days_since_order`: Calculate the number of days between `order_date` and a fixed `analysis_date` (e.g., `max(order_date)` + 30 days from your generated data, after converting `order_date` to datetime objects).\n\n4. **Data Visualization**: Create two visualizations:\n    *   A histogram showing the distribution of the `rating` column.\n    *   A box plot or violin plot showing the distribution of `rating` for each `clean_category`.\n\n5. **ML Pipeline & Evaluation**: \n    *   Define features `X` (`line_item_total`, `days_since_order`, `clean_category`, `has_positive_feedback`, `has_negative_feedback`) and target `y` (`rating`).\n    *   Split the data into training and testing sets (e.g., 80/20 split) using `sklearn.model_selection.train_test_split` (set `random_state=42`).\n    *   Create an `sklearn.pipeline.Pipeline` with a `ColumnTransformer` for preprocessing:\n        *   Numerical features (`line_item_total`, `days_since_order`): Apply `SimpleImputer(strategy='mean')` followed by `StandardScaler`.\n        *   Categorical features (`clean_category`): Apply `OneHotEncoder(handle_unknown='ignore')`.\n        *   Binary features (`has_positive_feedback`, `has_negative_feedback`): Use `Passthrough`.\n    *   The final estimator in the pipeline should be `sklearn.ensemble.RandomForestRegressor` (set `random_state=42`, `n_estimators=100`).\n    *   Train the pipeline on the training data. Predict `rating` for the test set.\n    *   Calculate and print the `sklearn.metrics.mean_absolute_error` (MAE) and `sklearn.metrics.r2_score` for the test set predictions.",
  "2026-01-21": "1. **Generate Synthetic Data (Pandas/Numpy)**: Create two pandas DataFrames:\n    *   `customers_df`: With 500 rows. Columns: `customer_id` (unique integers), `signup_date` (random dates over the last 5 years), `region` (e.g., 'North', 'South', 'East', 'West' with random distribution), `age` (random integers 18-70).\n    *   `transactions_df`: With 2000-3000 rows. Columns: `transaction_id` (unique integers), `customer_id` (randomly sampled from `customers_df` IDs, ensuring some customers have no transactions and others have many), `transaction_date` (random dates after their `signup_date`), `amount` (random floats between 10.0 and 1000.0).\n\n2. **Load into SQLite**: Create an in-memory SQLite database using `sqlite3` and load `customers_df` into a table named `customers` and `transactions_df` into `transactions`.\n\n3. **SQL Feature Engineering (Customer Value)**: First, determine an `analysis_date` (e.g., the maximum `transaction_date` in `transactions_df` plus 30 days, using pandas). Then, write a single SQL query that performs the following for *each customer*:\n    *   **Joins** `customers` and `transactions` tables.\n    *   **Aggregates** to calculate `total_spend`, `num_transactions`, `avg_transaction_value` (total_spend / num_transactions).\n    *   Calculates `days_since_last_purchase`: The number of days between the `analysis_date` and the customer's `MAX(transaction_date)`.\n    *   Calculates `days_since_first_purchase`: The number of days between the `analysis_date` and the customer's `MIN(transaction_date)`.\n    *   **Ensures** that all customers are included in the result, even those with no transactions, showing 0 or NULL for aggregated values as appropriate.\n    *   The query should return `customer_id`, `region`, `age`, `signup_date`, `total_spend`, `num_transactions`, `avg_transaction_value`, `days_since_last_purchase`, `days_since_first_purchase`.\n\n4. **Retrieve, Merge, and Pandas Feature Engineering**: \n    *   Fetch the SQL query results into a pandas DataFrame.\n    *   Calculate `account_age_days`: The number of days between `signup_date` and the `analysis_date` (from step 3).\n    *   Handle `NaN` values resulting from customers with no transactions:\n        *   Fill `total_spend`, `num_transactions`, `avg_transaction_value` with 0.\n        *   For `days_since_last_purchase` and `days_since_first_purchase` (for customers with no transactions), fill with a large sentinel value, e.g., `account_age_days` + 30 days (or 5 years in days).\n    *   Define features `X` (`age`, `region`, `account_age_days`, `total_spend`, `num_transactions`, `avg_transaction_value`, `days_since_last_purchase`, `days_since_first_purchase`) and target `y` (`total_spend`).\n    *   Split the data into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split` (set `random_state=42`).\n\n5. **Data Visualization**: Create two visualizations:\n    *   A histogram showing the distribution of the target variable `total_spend`.\n    *   A box plot or violin plot showing the distribution of `total_spend` for each `region`.\n\n6. **ML Pipeline & Evaluation**: \n    *   Create an `sklearn.pipeline.Pipeline` with a `ColumnTransformer` for preprocessing:\n        *   For numerical features (e.g., `age`, `account_age_days`, `total_spend`, `num_transactions`, `avg_transaction_value`, `days_since_last_purchase`, `days_since_first_purchase`): Apply `StandardScaler`.\n        *   For categorical features (`region`): Apply `OneHotEncoder(handle_unknown='ignore')`.\n    *   The final estimator in the pipeline should be `sklearn.ensemble.RandomForestRegressor` (set `random_state=42`, `n_estimators=100`).\n    *   Train the pipeline on the training data (`X_train`, `y_train`). Predict `total_spend` for the test set (`X_test`).\n    *   Calculate and print the `sklearn.metrics.mean_absolute_error` (MAE) and `sklearn.metrics.r2_score` for the test set predictions.",
  "2026-01-22": "1. **Generate Synthetic Data (Pandas/Numpy)**: Create two pandas DataFrames:\n    *   `users_df`: With 500 rows. Columns: `user_id` (unique integers), `signup_date` (random dates over the last 3 years), `country` (e.g., 'USA', 'Canada', 'UK', 'Australia' with random distribution), `device_type` (e.g., 'Mobile', 'Web', 'Desktop'), `engagement_level` (binary target, 0 or 1, with an approximate 70/30 split for class 0/1).\n    *   `activities_df`: With 3000-5000 rows. Columns: `activity_id` (unique integers), `user_id` (randomly sampled from `users_df` IDs, ensuring some users have many activities and a few have no activities), `activity_date` (random dates occurring *after* their respective `signup_date`), `activity_type` (e.g., 'login', 'logout', 'view_profile', 'send_message', 'post_content', 'like_content' with varying frequencies).\n2. **Load into SQLite**: Create an in-memory SQLite database using `sqlite3` and load `users_df` into a table named `users` and `activities_df` into `activities`.\n3. **SQL Feature Engineering (User Activity Aggregation)**: First, determine an `analysis_date` by finding the maximum `activity_date` in your `activities_df` (using pandas) and adding 30 days to it. Then, write a single SQL query that performs the following for *each user*:\n    *   **Joins** `users` and `activities` tables.\n    *   **Aggregates** user-level features: `total_activities` (count of all activities), `num_logins` (count of 'login' events), `num_messages_sent` (count of 'send_message' events), `num_content_posts` (count of 'post_content' events).\n    *   Calculates `first_activity_date` and `last_activity_date` for each user.\n    *   **Ensures** that all users are included, even those with no activities, showing appropriate default values (e.g., 0 for counts, `NULL` for dates).\n    *   The query should return `user_id`, `country`, `device_type`, `signup_date`, `total_activities`, `num_logins`, `num_messages_sent`, `num_content_posts`, `first_activity_date`, `last_activity_date`.\n4. **Retrieve, Merge, and Pandas Feature Engineering**: \n    *   Fetch the SQL query results into a pandas DataFrame.\n    *   Merge this aggregated DataFrame with the original `users_df` on `user_id`.\n    *   Handle `NaN` values resulting from the SQL query: Fill `total_activities`, `num_logins`, `num_messages_sent`, `num_content_posts` with 0 for users with no activities.\n    *   Convert all date columns to datetime objects. Calculate the following new features using the `analysis_date` from step 3:\n        *   `account_age_days`: Days between `signup_date` and `analysis_date`.\n        *   `days_since_last_activity`: Days between `last_activity_date` and `analysis_date`. For users with no activities, fill with a large sentinel value (e.g., `account_age_days` + 30).\n        *   `days_since_first_activity`: Days between `first_activity_date` and `analysis_date`. For users with no activities, fill with a large sentinel value (e.g., `account_age_days` + 30).\n    *   Define features `X` (`country`, `device_type`, `account_age_days`, `days_since_last_activity`, `days_since_first_activity`, `total_activities`, `num_logins`, `num_messages_sent`, `num_content_posts`) and target `y` (`engagement_level`). Split into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split` (set `random_state=42`).\n5. **Data Visualization**: Create two separate plots to visually inspect relationships with `engagement_level`:\n    *   A histogram showing the distribution of `total_activities` for each `engagement_level` (e.g., using `hue` in seaborn).\n    *   A bar plot or count plot showing the `engagement_level` counts across different `device_type`s.\n    Ensure plots have appropriate labels and titles.\n6. **ML Pipeline & Evaluation**: \n    *   Create an `sklearn.pipeline.Pipeline` with a `ColumnTransformer` for preprocessing:\n        *   For numerical features (e.g., `account_age_days`, `days_since_last_activity`, `days_since_first_activity`, `total_activities`, etc.): Apply `SimpleImputer(strategy='mean')` followed by `StandardScaler`.\n        *   For categorical features (`country`, `device_type`): Apply `OneHotEncoder(handle_unknown='ignore')`.\n    *   The final estimator in the pipeline should be `sklearn.linear_model.LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   Train the pipeline on the training data. Predict probabilities for the positive class (class 1) on the test set.\n    *   Calculate and print the `sklearn.metrics.roc_auc_score` and a `sklearn.metrics.classification_report` for the test set predictions.",
  "2026-01-23": "1. **Generate Synthetic Data**: Create two pandas DataFrames:\n    *   `products_df`: 100-150 rows. Columns: `product_id` (unique int), `category` (e.g., 'Electronics', 'Books', 'Home Goods', 'Clothing' with varying proportions), `price` (random floats between 50.0 and 500.0).\n    *   `reviews_df`: 800-1200 rows. Columns: `review_id` (unique int), `product_id` (randomly sampled from `products_df` IDs, ensuring multiple reviews per product), `review_text` (short text containing keywords like 'good', 'great', 'excellent', 'bad', 'terrible', 'slow', and '!' randomly, mixed with generic words), `rating` (random integers 1-5, biased towards 4-5).\n\n2. **Load into SQLite & SQL Analytics**: Create an in-memory SQLite database using `sqlite3`. Load `products_df` into a table named `products` and `reviews_df` into a table named `reviews`. Write a single SQL query that performs the following:\n    *   **Joins** `reviews` and `products` tables on `product_id`.\n    *   **Calculates `avg_product_rating`**: For each `review_id`, include the average `rating` for its corresponding `product_id` across *all* reviews for that product (using a window function `AVG(rating) OVER (PARTITION BY product_id)`).\n    *   The query should return `review_id`, `review_text`, `rating`, `category`, `price`, and `avg_product_rating`.\n\n3. **Pandas Feature Engineering & Target Creation**: Fetch the SQL query results into a pandas DataFrame. Create the following new features:\n    *   **Target Variable `sentiment`**: Create a binary target column `sentiment` where 1 if `rating >= 4` (positive) and 0 if `rating < 4` (negative). Drop the original `rating` column.\n    *   **Text Features from `review_text`**:\n        *   `review_length`: The length of the `review_text` string.\n        *   `has_exclamation`: Binary (1 if '!' is found in `review_text`, 0 otherwise).\n        *   `has_positive_word`: Binary (1 if 'good', 'great', or 'excellent' (case-insensitive) is found, 0 otherwise).\n        *   `has_negative_word`: Binary (1 if 'bad', 'terrible', or 'slow' (case-insensitive) is found, 0 otherwise).\n\n4. **Data Visualization**: Create two visualizations:\n    *   A box plot showing the distribution of `price` for each `sentiment` (0 and 1).\n    *   A bar plot (or count plot) showing the distribution of `sentiment` across different `category` values.\n    Ensure plots have appropriate labels and titles.\n\n5. **ML Pipeline & Evaluation**: \n    *   Define features `X` (all numerical, categorical, and binary text features created) and target `y` (`sentiment`) from the engineered DataFrame.\n    *   Split the data into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split` (set `random_state=42`).\n    *   Create an `sklearn.pipeline.Pipeline` with a `ColumnTransformer` for preprocessing:\n        *   For numerical features (`price`, `avg_product_rating`, `review_length`, `has_exclamation`): Apply `sklearn.preprocessing.StandardScaler`.\n        *   For the categorical feature (`category`): Apply `sklearn.preprocessing.OneHotEncoder(handle_unknown='ignore')`.\n        *   For binary features (`has_positive_word`, `has_negative_word`): Use `Passthrough`.\n    *   The final estimator in the pipeline should be `sklearn.linear_model.LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   Train the pipeline on the training data (`X_train`, `y_train`). Predict `sentiment` for the test set (`X_test`).\n    *   Calculate and print the `sklearn.metrics.accuracy_score` and `sklearn.metrics.classification_report` for the test set predictions.",
  "2026-01-24": "1. **Generate Synthetic Data**: Create two pandas DataFrames:\n    *   `users_df`: With 500 rows. Columns: `user_id` (unique integers), `signup_date` (random dates over the last 5 years), `country` (e.g., 'USA', 'Canada', 'UK', 'Germany' with random distribution), `subscription_type` (e.g., 'Free', 'Basic', 'Premium').\n    *   `activities_df`: With 3000-5000 rows. Columns: `activity_id` (unique integers), `user_id` (randomly sampled from `users_df` IDs), `activity_date` (random dates *after* `signup_date`), `activity_type` (e.g., 'login', 'view_item', 'post_comment', 'upgrade_plan' with varying frequencies).\n\n2. **Load into SQLite & SQL Analytics**: Create an in-memory SQLite database. Load `users_df` into a table named `users` and `activities_df` into a table named `activities`. Determine an `analysis_date` (e.g., `max(activity_date)` from `activities_df` + 30 days). Write a single SQL query that performs the following for *each user*:\n    *   **Joins** `users` and `activities` tables.\n    *   **Aggregates** user-level features: `total_activities` (count of all activities), `num_logins` (count of 'login' events), `num_comments_posted` (count of 'post_comment' events).\n    *   Calculates `days_since_last_activity`: Days between `analysis_date` (passed as a parameter or calculated within SQL if possible via date functions) and `MAX(activity_date)`.\n    *   **Ensures** all users are included, showing 0 for counts and `NULL` for `days_since_last_activity` if no activities.\n    *   Returns `user_id`, `country`, `subscription_type`, `signup_date`, `total_activities`, `num_logins`, `num_comments_posted`, `days_since_last_activity`.\n\n3. **Pandas Feature Engineering & Multi-Class Target Creation**: Fetch the SQL query results into a pandas DataFrame. \n    *   Merge with `users_df` (if necessary, to ensure all original user features like `country`, `subscription_type` are present even for users with no activities) using a left join on `user_id`.\n    *   Handle `NaN` values: Fill `total_activities`, `num_logins`, `num_comments_posted` with 0. For `days_since_last_activity` (for users with no activities), fill with a large sentinel value (e.g., `365 * 5` or 1825 days).\n    *   Calculate `account_age_days`: Days between `signup_date` and the `analysis_date` (from step 2).\n    *   **Create the multi-class target `activity_segment`**: Based on `total_activities` and `days_since_last_activity`. First, calculate quantiles for `total_activities`. Then, define segments:\n        *   'High_Activity': `total_activities` is in the top 33% *and* `days_since_last_activity` < 30 days.\n        *   'Medium_Activity': `total_activities` is in the middle 34-66% *or* `days_since_last_activity` between 30 and 90 days (excluding those already classified as 'High_Activity').\n        *   'Low_Activity': All remaining users (including those with 0 activities or `days_since_last_activity` >= 90 days).\n    *   Define features `X` (numerical: `account_age_days`, `total_activities`, `num_logins`, `num_comments_posted`, `days_since_last_activity`; categorical: `country`, `subscription_type`) and target `y` (`activity_segment`). Split into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split` (set `random_state=42`).\n\n4. **Data Visualization**: Create two separate plots to visually inspect relationships with `activity_segment`:\n    *   A violin plot or box plot showing the distribution of `days_since_last_activity` for each `activity_segment`.\n    *   A bar plot or count plot showing the distribution of `activity_segment` across different `subscription_type`s.\n    Ensure plots have appropriate labels and titles.\n\n5. **ML Pipeline & Evaluation (Multi-Class)**: \n    *   Create an `sklearn.pipeline.Pipeline` with a `ColumnTransformer` for preprocessing:\n        *   For numerical features: Apply `StandardScaler`.\n        *   For categorical features: Apply `OneHotEncoder(handle_unknown='ignore')`.\n    *   The final estimator should be `sklearn.ensemble.RandomForestClassifier` (set `random_state=42`, `n_estimators=100`, `class_weight='balanced'` for potential class imbalance).\n    *   Train the pipeline on `X_train`, `y_train`. Predict `activity_segment` for `X_test`.\n    *   Calculate and print the `sklearn.metrics.accuracy_score` and `sklearn.metrics.classification_report` for the test set predictions.",
  "2026-01-25": "1. **Generate Synthetic Data (Pandas/Numpy)**: Create two pandas DataFrames:\n    *   `customers_df`: With 500 rows. Columns: `customer_id` (unique integers), `signup_date` (random dates over the last 5 years), `region` (e.g., 'North', 'South', 'East', 'West'), `age` (random integers 18-70).\n    *   `transactions_df`: With 3000-5000 rows. Columns: `transaction_id` (unique integers), `customer_id` (randomly sampled from `customers_df` IDs, ensuring some customers have only 1 transaction, others 2, and many more), `transaction_date` (random dates *after* their respective `signup_date`), `amount` (random floats between 10.0 and 1000.0), `item_count` (random integers 1-10).\n    Ensure `transactions_df` is sorted by `customer_id` and `transaction_date`.\n\n2. **Load into SQLite & SQL Feature Engineering (Initial Buyer Behavior)**: Create an in-memory SQLite database. Load `customers_df` into a table named `customers` and `transactions_df` into a table named `transactions`. Define an `initial_period_days` (e.g., 30 days).\n    Write a single SQL query that performs the following for *each customer*:\n    *   **Joins** `customers` and `transactions` tables.\n    *   **Aggregates** initial purchase behavior for transactions occurring within the `initial_period_days` from their `signup_date`:\n        *   `initial_total_spend` (sum of `amount`)\n        *   `initial_num_transactions` (count of transactions)\n        *   `initial_avg_item_count` (average `item_count`)\n    *   **Ensures** all customers are included, showing 0 for aggregates if no transactions in the initial period.\n    *   The query should return `customer_id`, `region`, `age`, `signup_date`, `initial_total_spend`, `initial_num_transactions`, `initial_avg_item_count`.\n\n3. **Pandas Feature Engineering & Target Creation (Repeat Buyer)**: Fetch the SQL query results into a pandas DataFrame. Merge it with the original `transactions_df` to get full transaction history (if not already joined properly).\n    *   Handle `NaN` values: Fill `initial_total_spend`, `initial_num_transactions`, `initial_avg_item_count` with 0 for customers with no initial transactions.\n    *   Calculate `account_age_days`: The number of days between `signup_date` and the latest `transaction_date` in the *entire* `transactions_df`.\n    *   **Create Binary Target `is_repeat_buyer`**: A customer is a `repeat_buyer` (1) if they have made at least 2 transactions *and* the `MAX(transaction_date)` is at least 60 days after their `MIN(transaction_date)`. Otherwise, `0`.\n    *   Define features `X` (`region`, `age`, `account_age_days`, `initial_total_spend`, `initial_num_transactions`, `initial_avg_item_count`) and target `y` (`is_repeat_buyer`). Split into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split` (set `random_state=42`).\n\n4. **Data Visualization**: Create two separate plots to visually inspect relationships with `is_repeat_buyer`:\n    *   A violin plot or box plot showing the distribution of `initial_total_spend` for each `is_repeat_buyer` group.\n    *   A bar plot or count plot showing the distribution of `is_repeat_buyer` across different `region`s.\n    Ensure plots have appropriate labels and titles.\n\n5. **ML Pipeline & Evaluation**: Create an `sklearn.pipeline.Pipeline` with a `ColumnTransformer` for preprocessing:\n    *   For numerical features (`age`, `account_age_days`, `initial_total_spend`, `initial_num_transactions`, `initial_avg_item_count`): Apply `sklearn.preprocessing.StandardScaler`.\n    *   For categorical features (`region`): Apply `sklearn.preprocessing.OneHotEncoder(handle_unknown='ignore')`.\n    *   The final estimator should be `sklearn.linear_model.LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   Train the pipeline on `X_train`, `y_train`. Predict probabilities for the positive class (class 1) on `X_test`.\n    *   Calculate and print the `sklearn.metrics.roc_auc_score` and `sklearn.metrics.classification_report` for the test set predictions.",
  "2026-01-26": "1. **Generate Synthetic Data (Pandas/Numpy)**: Create two pandas DataFrames:\n    *   `customers_df`: With 500 rows. Columns: `customer_id` (unique integers), `signup_date` (random dates over the last 5 years), `region` (e.g., 'North', 'South', 'East', 'West'), `age` (random integers 18-70).\n    *   `transactions_df`: With 3000-5000 rows. Columns: `transaction_id` (unique integers), `customer_id` (randomly sampled from `customers_df` IDs, ensuring some customers have many transactions and a few have no transactions), `transaction_date` (random dates *after* their respective `signup_date`), `amount` (random floats between 10.0 and 1000.0).\n\n2. **Load into SQLite & SQL Feature Engineering (RFM)**: Create an in-memory SQLite database. Load `customers_df` into a table named `customers` and `transactions_df` into a table named `transactions`. Determine an `analysis_date` (e.g., the latest `transaction_date` from `transactions_df` + 30 days, using pandas).\n    Write a single SQL query that calculates Recency, Frequency, and Monetary (RFM) values for *each customer*:\n    *   `recency_days`: Number of days between the `analysis_date` and the customer's `MAX(transaction_date)`. If no transactions, this should be `NULL`.\n    *   `frequency`: Total number of transactions for the customer.\n    *   `monetary`: Sum of all `amount`s for the customer.\n    *   **Ensures** all customers are included, showing 0 for `frequency` and `monetary`, and `NULL` for `recency_days` if no transactions.\n    *   The query should return `customer_id`, `region`, `age`, `signup_date`, `recency_days`, `frequency`, `monetary`.\n\n3. **Pandas Feature Engineering & Target Creation**: Fetch the SQL query results into a pandas DataFrame. \n    *   Handle `NaN` values: Fill `frequency` and `monetary` with 0. For `recency_days` (for customers with no transactions), fill with a large sentinel value, e.g., `365 * 5` (1825 days).\n    *   Calculate `account_age_days`: Days between `signup_date` and the `analysis_date` (from step 2).\n    *   **Create Binary Target `is_high_value_customer`**: A customer is 'high value' (1) if their `monetary` value is in the top 30% *and* their `frequency` is in the top 30%. Otherwise, 0. (Hint: Use `quantile()` to find thresholds).\n    *   Define features `X` (`region`, `age`, `account_age_days`, `recency_days`, `frequency`, `monetary`) and target `y` (`is_high_value_customer`). Split into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split` (set `random_state=42`).\n\n4. **Data Visualization**: Create two separate plots to visually inspect relationships with `is_high_value_customer`:\n    *   A violin plot or box plot showing the distribution of `recency_days` for each `is_high_value_customer` group.\n    *   A bar plot or count plot showing the distribution of `is_high_value_customer` across different `region`s.\n    Ensure plots have appropriate labels and titles.\n\n5. **ML Pipeline & Evaluation**: \n    *   Create an `sklearn.pipeline.Pipeline` with a `ColumnTransformer` for preprocessing:\n        *   For numerical features (`age`, `account_age_days`, `recency_days`, `frequency`, `monetary`): Apply `sklearn.preprocessing.StandardScaler`.\n        *   For the categorical feature (`region`): Apply `sklearn.preprocessing.OneHotEncoder(handle_unknown='ignore')`.\n    *   The final estimator in the pipeline should be `sklearn.linear_model.LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   Train the pipeline on the training data (`X_train`, `y_train`). Predict probabilities for the positive class (class 1) on the test set (`X_test`).\n    *   Calculate and print the `sklearn.metrics.roc_auc_score` for the test set predictions.\n    *   Generate and display an ROC curve for the model using `sklearn.metrics.RocCurveDisplay.from_estimator` with the trained pipeline and test data."
}