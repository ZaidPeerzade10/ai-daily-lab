{
  "2025-11-30": "Given a pandas DataFrame containing at least four numerical features (`feature_A`, `feature_B`, `feature_C`, `feature_D`):\n1.  Create two new interaction features: `interaction_AB` (the product of `feature_A` and `feature_B`) and `interaction_CD` (the product of `feature_C` and `feature_D`).\n2.  Identify all numerical features (original and new) that have a skewness value greater than 0.75.\n3.  For each identified skewed feature, apply a `np.log1p` transformation, replacing the original feature column with its transformed version.\n4.  Display the head of the modified DataFrame and the skewness of all features after transformation.",
  "2025-12-01": "1. Generate a synthetic classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 5 numerical features, and 2 categorical features (one with 3 unique values, one with 5 unique values). Introduce missing values (e.g., `np.nan`) into two of the numerical features.\n2. Create an `sklearn.compose.ColumnTransformer` to preprocess the data:\n    *   For numerical features: Impute missing values with the mean, then apply `StandardScaler`.\n    *   For categorical features: Apply `OneHotEncoder`.\n3. Construct an `sklearn.pipeline.Pipeline` that first applies this `ColumnTransformer` and then trains a `RandomForestClassifier`.\n4. Evaluate the complete pipeline's performance using 5-fold cross-validation (`sklearn.model_selection.cross_val_score`) and report the mean accuracy and its standard deviation.",
  "2025-12-02": "1. Create an in-memory SQLite database using the `sqlite3` module.\n2. Create two tables: `customers` (columns: `customer_id` (INTEGER PRIMARY KEY), `name` (TEXT), `city` (TEXT)) and `orders` (columns: `order_id` (INTEGER PRIMARY KEY), `customer_id` (INTEGER), `amount` (REAL), `order_date` (TEXT)). Ensure `customer_id` in `orders` is a foreign key referencing `customers`.\n3. Insert sample data into both tables (at least 5 distinct customers and 10-15 orders, ensuring some customers have multiple orders).\n4. Using a single SQL query, calculate the total purchase `amount` for each customer, retrieving the customer's `name` and their `total_revenue`, joining the `customers` and `orders` tables.\n5. Retrieve these aggregated results directly into a pandas DataFrame and display the top 3 customers by their `total_revenue`.",
  "2025-12-03": "1. Generate a synthetic regression dataset using `sklearn.datasets.make_regression` with at least 500 samples and 5 features.\n2. Create an `sklearn.pipeline.Pipeline` that first applies `StandardScaler` to the features and then fits a `Ridge` regressor.\n3. Define a hyperparameter grid for the `Ridge` regressor within the pipeline, tuning the `alpha` parameter across at least 3 distinct values (e.g., `[0.1, 1.0, 10.0]`).\n4. Use `sklearn.model_selection.GridSearchCV` with the pipeline and the defined parameter grid to find the best hyperparameters. Use `neg_mean_squared_error` as the scoring metric and apply 3-fold cross-validation.\n5. Report the best hyperparameters found by `GridSearchCV` and the corresponding best score (remembering to convert the `neg_mean_squared_error` back to a positive MSE value).",
  "2025-12-04": "1. Generate a synthetic dataset using `sklearn.datasets.make_blobs` with at least 500 samples, 4 numerical features, and 3 distinct clusters. Convert this into a pandas DataFrame, including the cluster labels as a feature (e.g., `cluster_id`).\n2. Add a new categorical feature to the DataFrame (e.g., `group`) with 2-3 distinct values, randomly assigned.\n3. Using `seaborn` and `matplotlib.pyplot`, create the following visualizations to explore the data:\n    *   A pair plot (`sns.pairplot`) for the numerical features, coloring the points by the `cluster_id`.\n    *   A set of histograms (or KDE plots) for `feature_1` and `feature_2`, separated for each unique value of the newly created `group` categorical feature (e.g., using `sns.FacetGrid` or `sns.histplot` with `hue` and `col`).\n    *   A box plot (or violin plot) showing the distribution of `feature_3` across the different `cluster_id`s.\n4. Ensure all plots have appropriate titles and labels.",
  "2025-12-05": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` (e.g., 1000 samples, 10 features, 2 informative features, 2 classes).\n2. Split the dataset into training and testing sets (e.g., 80/20 split) using `train_test_split`.\n3. Train a `LogisticRegression` model on the training data.\n4. Predict class labels and class probabilities for the positive class on the test set.\n5. Calculate and print the following evaluation metrics for the test set predictions: Accuracy, Precision, Recall, F1-score, and ROC AUC score.\n6. Plot the Receiver Operating Characteristic (ROC) curve for the model using `matplotlib.pyplot`, clearly labeling axes and adding a title. Include the AUC score in the plot legend.",
  "2025-12-06": "1. Generate a synthetic regression dataset using `sklearn.datasets.make_regression` with at least 500 samples, 3 informative features, and a small amount of noise.\n2. Create two distinct `sklearn.pipeline.Pipeline` objects:\n    *   `pipeline_simple`: Consisting of `StandardScaler` followed by `LinearRegression`.\n    *   `pipeline_poly`: Consisting of `PolynomialFeatures` (set `degree=2`), then `StandardScaler`, then `LinearRegression`.\n3. Evaluate both pipelines using `sklearn.model_selection.cross_val_score` with 5-fold cross-validation and `neg_mean_squared_error` as the scoring metric.\n4. Print the mean and standard deviation of the Mean Squared Error (MSE) for both pipelines, clearly indicating which result belongs to which pipeline. (Remember to convert `neg_mean_squared_error` to positive MSE values for interpretability).",
  "2025-12-07": "1. Generate a pandas DataFrame with a `timestamp` column (daily data for 2-3 years) and a `value` column (e.g., synthetic sales data with some trend and seasonality, using `np.sin` or similar).\n2. From the `timestamp` column, create new features: `month` (numerical month), `day_of_week` (name of the day, e.g., 'Monday'), and `is_weekend` (boolean).\n3. Calculate the average `value` aggregated by `month` and by `day_of_week`.\n4. Create two visualizations using `seaborn` and `matplotlib.pyplot`:\n    *   A line plot showing the average `value` trend across months.\n    *   A bar plot showing the average `value` for each `day_of_week`.\n5. Display the head of the DataFrame with the new features and print the aggregated dataframes for both monthly and daily trends.",
  "2025-12-08": "1. Generate a synthetic dataset suitable for clustering using `sklearn.datasets.make_blobs` with at least 700 samples, 6 numerical features, and 4 distinct clusters (do not use the true cluster labels for modeling).\n2. Apply `sklearn.cluster.KMeans` to the generated features to discover 4 clusters. Initialize KMeans with a `random_state` for reproducibility.\n3. Evaluate the quality of the discovered clusters by calculating the `sklearn.metrics.silhouette_score`.\n4. To visualize the clustering, reduce the dimensionality of the original features to 2 using `sklearn.decomposition.PCA`.\n5. Create a scatter plot using `matplotlib.pyplot` or `seaborn` of the 2 principal components, coloring the points based on the clusters identified by KMeans. Title the plot with the calculated Silhouette Score.",
  "2025-12-09": "1. Generate a synthetic binary classification dataset (e.g., using `sklearn.datasets.make_classification`) with at least 1000 samples, 5 numerical features, and 1 conceptual 'high-cardinality' categorical feature. To create this categorical feature, generate a numerical feature with a large number of unique integer values (e.g., 50-100) and then convert it to string type.\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `train_test_split`.\n3. Create two distinct `sklearn.pipeline.Pipeline` objects for preprocessing and modeling:\n    *   `pipeline_onehot_encoding`: Use `sklearn.compose.ColumnTransformer`. For the numerical features, apply `StandardScaler`. For the high-cardinality categorical feature, apply `OneHotEncoder(handle_unknown='ignore')`.\n    *   `pipeline_ordinal_encoding`: Use `sklearn.compose.ColumnTransformer`. For the numerical features, apply `StandardScaler`. For the high-cardinality categorical feature, apply `OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)`.\n4. Both pipelines should then fit a `LogisticRegression` model (using `solver='liblinear'` and a `random_state` for reproducibility).\n5. Train both pipelines on the training data and evaluate their performance on the test set. Report the `accuracy_score` and `f1_score` for each pipeline, clearly stating which encoding strategy yielded which result.",
  "2025-12-10": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_moons` with at least 1000 samples, `noise=0.1`, and `random_state=42`.\n2. Split the dataset into training and testing sets (e.g., 80/20 split) using `sklearn.model_selection.train_test_split`.\n3. Build a simple feedforward neural network using `tf.keras.Sequential`:\n    *   An input `tf.keras.layers.Dense` layer suitable for the number of features.\n    *   A hidden `Dense` layer with 32 units and `relu` activation.\n    *   Another hidden `Dense` layer with 16 units and `relu` activation.\n    *   An output `Dense` layer with 1 unit and `sigmoid` activation.\n4. Compile the model with `optimizer='adam'`, `loss='binary_crossentropy'`, and `metrics=['accuracy']`.\n5. Train the model on the training data for a fixed number of epochs (e.g., 50) with a batch size (e.g., 32), storing the training history.\n6. Evaluate the trained model on the test set and print the test accuracy.\n7. Plot the training and validation accuracy and loss over epochs from the training history using `matplotlib.pyplot`, clearly labeling the axes and providing a title.",
  "2025-12-11": "1. Generate a synthetic regression dataset using `sklearn.datasets.make_regression` with at least 500 samples, 7 features, and a small amount of noise.\n2. Implement a custom `sklearn` transformer (inheriting from `BaseEstimator`, `TransformerMixin`) named `CustomPolynomialFeatures`.\n   This transformer should take a list of feature names (or indices) as an initialization argument. Its `transform` method should apply `PolynomialFeatures` (with `degree=2`, `include_bias=False`) only to the specified features, and pass through other features unchanged.\n3. Create an `sklearn.pipeline.Pipeline` that uses an `sklearn.compose.ColumnTransformer`.\n   *   Apply `StandardScaler` to all numerical features *not* handled by your custom transformer.\n   *   Apply your `CustomPolynomialFeatures` transformer to 3-4 specific numerical features.\n   *   The pipeline should then fit a `Ridge` regressor.\n4. Evaluate the pipeline's performance using `sklearn.model_selection.cross_val_score` with 5-fold cross-validation and `neg_mean_squared_error` as the scoring metric.\n5. Print the mean and standard deviation of the Mean Squared Error (MSE) for the pipeline (remembering to convert `neg_mean_squared_error` to positive MSE values).",
  "2025-12-12": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 4 numerical features, and 2 classes. Convert the features and target into a pandas DataFrame.\n2. Select two of the original numerical features (e.g., `feature_0`, `feature_1`) and apply `pd.cut` to each of them to discretize them into 3-4 bins (e.g., `['low', 'medium', 'high']`). These new binned features should be categorical and added to the DataFrame.\n3. Create an `sklearn.compose.ColumnTransformer` to preprocess the data:\n    *   For the *remaining original* numerical features (those not binned): Apply `StandardScaler`.\n    *   For the *newly binned* categorical features: Apply `OneHotEncoder(handle_unknown='ignore')`.\n4. Construct an `sklearn.pipeline.Pipeline` that first applies this `ColumnTransformer` and then trains a `GradientBoostingClassifier` (set `random_state` for reproducibility).\n5. Evaluate the complete pipeline's performance using 5-fold cross-validation (`sklearn.model_selection.cross_val_score`) with `accuracy` as the scoring metric. Report the mean accuracy and its standard deviation.",
  "2025-12-13": "1. Create an in-memory SQLite database using the `sqlite3` module.\n2. Create a `transactions` table with the following columns: `transaction_id` (INTEGER PRIMARY KEY), `customer_id` (INTEGER), `product_id` (INTEGER), `transaction_date` (TEXT in 'YYYY-MM-DD' format), and `amount` (REAL).\n3. Insert synthetic data into the `transactions` table. Include at least 5 distinct customers, 3 distinct products, and 20-30 transactions spanning a few months.\n4. Write a single SQL query that uses **window functions** to calculate the following for each transaction:\n    *   `customer_monthly_total`: The sum of `amount` for that specific `customer_id` within the month of the `transaction_date`.\n    *   `customer_monthly_avg_transaction`: The average `amount` for that specific `customer_id` within the month of the `transaction_date`.\n    *   `customer_cumulative_total`: The running total of `amount` for that specific `customer_id`, ordered by `transaction_date`.\n5. Retrieve the results of this SQL query into a pandas DataFrame. Display `transaction_date`, `customer_id`, `amount`, `customer_monthly_total`, `customer_monthly_avg_transaction`, and `customer_cumulative_total`. Show the head of the DataFrame.",
  "2025-12-14": "1. Generate a pandas DataFrame with synthetic transaction data, including `customer_id` (5-10 unique), `transaction_date` (spanning 6-12 months of daily data), `amount` (random float), and `product_category` (3-5 unique strings).\n2. For each transaction, calculate two new features using *window functions* (Pandas `groupby` + `rolling` or `expanding`):\n    *   `customer_30d_avg_spend`: The average `amount` for that specific `customer_id` over the *past 30 days*, inclusive of the current transaction date.\n    *   `customer_cumulative_transactions`: The running total count of transactions for that specific `customer_id`, ordered by `transaction_date`.\n3. Aggregate the data to find:\n    *   The total `amount` spent by each `customer_id` for each `month`.\n    *   The `product_category` with the highest total `amount` spent across *all* customers for each `month`.\n4. Display the head of the DataFrame with the new features and print the aggregated monthly customer spending and the top product categories per month.",
  "2025-12-15": "1. Generate a synthetic regression dataset using `sklearn.datasets.make_regression` with 1000 samples, 5 informative features, and a small amount of noise. Additionally, generate 5 completely random, uninformative features (e.g., using `np.random.rand`) and concatenate them to your original features, creating a feature matrix `X` with 10 features.\n2. Create an `sklearn.pipeline.Pipeline` that first applies `StandardScaler`, then uses `sklearn.feature_selection.SelectKBest` (with `f_regression` as the score function) for feature selection, and finally fits a `LinearRegression` model.\n3. Define a hyperparameter grid for `sklearn.model_selection.GridSearchCV` to tune the `k` parameter of `SelectKBest` (e.g., `[3, 5, 7, 10]` to explore different numbers of selected features).\n4. Use `GridSearchCV` with the pipeline and the defined parameter grid. Perform 3-fold cross-validation and use `neg_mean_squared_error` as the scoring metric.\n5. Report the best `k` value found, the corresponding best cross-validation score (converting `neg_mean_squared_error` to positive MSE), and the indices of the features selected by the best model (you might need to extract the `SelectKBest` step from the best estimator).",
  "2025-12-16": "1. Generate a pandas DataFrame with a `date` column (daily data for 2-3 years, starting from a fixed date like '2020-01-01'), a `value` column (synthetic time-series data with a linear trend, seasonality using `np.sin`, and some noise), and two additional numerical features (`feature_A`, `feature_B`) which can be random.\n2. Using `pandas` operations, create the following new features:\n    *   `lag_1_value`: The `value` from the previous day.\n    *   `rolling_7d_mean_feature_A`: A 7-day rolling mean of `feature_A`.\n    *   `day_of_week_num`: Numerical day of the week (0-6).\n    *   `month_num`: Numerical month (1-12).\n3. Handle any `NaN` values introduced by lag/rolling features (e.g., by dropping the first few rows).\n4. Split the dataset into training and testing sets based on time (e.g., use the first 80% of data for training and the remaining 20% for testing).\n5. Construct an `sklearn.pipeline.Pipeline` that first applies `StandardScaler` to all numerical features (including the newly engineered ones) and then trains a `Ridge` regressor.\n6. Train the pipeline on the training data and evaluate its performance on the test set, reporting the Mean Absolute Error (MAE) and R-squared score."
}