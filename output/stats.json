{
  "2025-11-30": "Given a pandas DataFrame containing at least four numerical features (`feature_A`, `feature_B`, `feature_C`, `feature_D`):\n1.  Create two new interaction features: `interaction_AB` (the product of `feature_A` and `feature_B`) and `interaction_CD` (the product of `feature_C` and `feature_D`).\n2.  Identify all numerical features (original and new) that have a skewness value greater than 0.75.\n3.  For each identified skewed feature, apply a `np.log1p` transformation, replacing the original feature column with its transformed version.\n4.  Display the head of the modified DataFrame and the skewness of all features after transformation.",
  "2025-12-01": "1. Generate a synthetic classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 5 numerical features, and 2 categorical features (one with 3 unique values, one with 5 unique values). Introduce missing values (e.g., `np.nan`) into two of the numerical features.\n2. Create an `sklearn.compose.ColumnTransformer` to preprocess the data:\n    *   For numerical features: Impute missing values with the mean, then apply `StandardScaler`.\n    *   For categorical features: Apply `OneHotEncoder`.\n3. Construct an `sklearn.pipeline.Pipeline` that first applies this `ColumnTransformer` and then trains a `RandomForestClassifier`.\n4. Evaluate the complete pipeline's performance using 5-fold cross-validation (`sklearn.model_selection.cross_val_score`) and report the mean accuracy and its standard deviation.",
  "2025-12-02": "1. Create an in-memory SQLite database using the `sqlite3` module.\n2. Create two tables: `customers` (columns: `customer_id` (INTEGER PRIMARY KEY), `name` (TEXT), `city` (TEXT)) and `orders` (columns: `order_id` (INTEGER PRIMARY KEY), `customer_id` (INTEGER), `amount` (REAL), `order_date` (TEXT)). Ensure `customer_id` in `orders` is a foreign key referencing `customers`.\n3. Insert sample data into both tables (at least 5 distinct customers and 10-15 orders, ensuring some customers have multiple orders).\n4. Using a single SQL query, calculate the total purchase `amount` for each customer, retrieving the customer's `name` and their `total_revenue`, joining the `customers` and `orders` tables.\n5. Retrieve these aggregated results directly into a pandas DataFrame and display the top 3 customers by their `total_revenue`.",
  "2025-12-03": "1. Generate a synthetic regression dataset using `sklearn.datasets.make_regression` with at least 500 samples and 5 features.\n2. Create an `sklearn.pipeline.Pipeline` that first applies `StandardScaler` to the features and then fits a `Ridge` regressor.\n3. Define a hyperparameter grid for the `Ridge` regressor within the pipeline, tuning the `alpha` parameter across at least 3 distinct values (e.g., `[0.1, 1.0, 10.0]`).\n4. Use `sklearn.model_selection.GridSearchCV` with the pipeline and the defined parameter grid to find the best hyperparameters. Use `neg_mean_squared_error` as the scoring metric and apply 3-fold cross-validation.\n5. Report the best hyperparameters found by `GridSearchCV` and the corresponding best score (remembering to convert the `neg_mean_squared_error` back to a positive MSE value).",
  "2025-12-04": "1. Generate a synthetic dataset using `sklearn.datasets.make_blobs` with at least 500 samples, 4 numerical features, and 3 distinct clusters. Convert this into a pandas DataFrame, including the cluster labels as a feature (e.g., `cluster_id`).\n2. Add a new categorical feature to the DataFrame (e.g., `group`) with 2-3 distinct values, randomly assigned.\n3. Using `seaborn` and `matplotlib.pyplot`, create the following visualizations to explore the data:\n    *   A pair plot (`sns.pairplot`) for the numerical features, coloring the points by the `cluster_id`.\n    *   A set of histograms (or KDE plots) for `feature_1` and `feature_2`, separated for each unique value of the newly created `group` categorical feature (e.g., using `sns.FacetGrid` or `sns.histplot` with `hue` and `col`).\n    *   A box plot (or violin plot) showing the distribution of `feature_3` across the different `cluster_id`s.\n4. Ensure all plots have appropriate titles and labels.",
  "2025-12-05": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` (e.g., 1000 samples, 10 features, 2 informative features, 2 classes).\n2. Split the dataset into training and testing sets (e.g., 80/20 split) using `train_test_split`.\n3. Train a `LogisticRegression` model on the training data.\n4. Predict class labels and class probabilities for the positive class on the test set.\n5. Calculate and print the following evaluation metrics for the test set predictions: Accuracy, Precision, Recall, F1-score, and ROC AUC score.\n6. Plot the Receiver Operating Characteristic (ROC) curve for the model using `matplotlib.pyplot`, clearly labeling axes and adding a title. Include the AUC score in the plot legend.",
  "2025-12-06": "1. Generate a synthetic regression dataset using `sklearn.datasets.make_regression` with at least 500 samples, 3 informative features, and a small amount of noise.\n2. Create two distinct `sklearn.pipeline.Pipeline` objects:\n    *   `pipeline_simple`: Consisting of `StandardScaler` followed by `LinearRegression`.\n    *   `pipeline_poly`: Consisting of `PolynomialFeatures` (set `degree=2`), then `StandardScaler`, then `LinearRegression`.\n3. Evaluate both pipelines using `sklearn.model_selection.cross_val_score` with 5-fold cross-validation and `neg_mean_squared_error` as the scoring metric.\n4. Print the mean and standard deviation of the Mean Squared Error (MSE) for both pipelines, clearly indicating which result belongs to which pipeline. (Remember to convert `neg_mean_squared_error` to positive MSE values for interpretability).",
  "2025-12-07": "1. Generate a pandas DataFrame with a `timestamp` column (daily data for 2-3 years) and a `value` column (e.g., synthetic sales data with some trend and seasonality, using `np.sin` or similar).\n2. From the `timestamp` column, create new features: `month` (numerical month), `day_of_week` (name of the day, e.g., 'Monday'), and `is_weekend` (boolean).\n3. Calculate the average `value` aggregated by `month` and by `day_of_week`.\n4. Create two visualizations using `seaborn` and `matplotlib.pyplot`:\n    *   A line plot showing the average `value` trend across months.\n    *   A bar plot showing the average `value` for each `day_of_week`.\n5. Display the head of the DataFrame with the new features and print the aggregated dataframes for both monthly and daily trends.",
  "2025-12-08": "1. Generate a synthetic dataset suitable for clustering using `sklearn.datasets.make_blobs` with at least 700 samples, 6 numerical features, and 4 distinct clusters (do not use the true cluster labels for modeling).\n2. Apply `sklearn.cluster.KMeans` to the generated features to discover 4 clusters. Initialize KMeans with a `random_state` for reproducibility.\n3. Evaluate the quality of the discovered clusters by calculating the `sklearn.metrics.silhouette_score`.\n4. To visualize the clustering, reduce the dimensionality of the original features to 2 using `sklearn.decomposition.PCA`.\n5. Create a scatter plot using `matplotlib.pyplot` or `seaborn` of the 2 principal components, coloring the points based on the clusters identified by KMeans. Title the plot with the calculated Silhouette Score.",
  "2025-12-09": "1. Generate a synthetic binary classification dataset (e.g., using `sklearn.datasets.make_classification`) with at least 1000 samples, 5 numerical features, and 1 conceptual 'high-cardinality' categorical feature. To create this categorical feature, generate a numerical feature with a large number of unique integer values (e.g., 50-100) and then convert it to string type.\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `train_test_split`.\n3. Create two distinct `sklearn.pipeline.Pipeline` objects for preprocessing and modeling:\n    *   `pipeline_onehot_encoding`: Use `sklearn.compose.ColumnTransformer`. For the numerical features, apply `StandardScaler`. For the high-cardinality categorical feature, apply `OneHotEncoder(handle_unknown='ignore')`.\n    *   `pipeline_ordinal_encoding`: Use `sklearn.compose.ColumnTransformer`. For the numerical features, apply `StandardScaler`. For the high-cardinality categorical feature, apply `OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)`.\n4. Both pipelines should then fit a `LogisticRegression` model (using `solver='liblinear'` and a `random_state` for reproducibility).\n5. Train both pipelines on the training data and evaluate their performance on the test set. Report the `accuracy_score` and `f1_score` for each pipeline, clearly stating which encoding strategy yielded which result.",
  "2025-12-10": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_moons` with at least 1000 samples, `noise=0.1`, and `random_state=42`.\n2. Split the dataset into training and testing sets (e.g., 80/20 split) using `sklearn.model_selection.train_test_split`.\n3. Build a simple feedforward neural network using `tf.keras.Sequential`:\n    *   An input `tf.keras.layers.Dense` layer suitable for the number of features.\n    *   A hidden `Dense` layer with 32 units and `relu` activation.\n    *   Another hidden `Dense` layer with 16 units and `relu` activation.\n    *   An output `Dense` layer with 1 unit and `sigmoid` activation.\n4. Compile the model with `optimizer='adam'`, `loss='binary_crossentropy'`, and `metrics=['accuracy']`.\n5. Train the model on the training data for a fixed number of epochs (e.g., 50) with a batch size (e.g., 32), storing the training history.\n6. Evaluate the trained model on the test set and print the test accuracy.\n7. Plot the training and validation accuracy and loss over epochs from the training history using `matplotlib.pyplot`, clearly labeling the axes and providing a title.",
  "2025-12-11": "1. Generate a synthetic regression dataset using `sklearn.datasets.make_regression` with at least 500 samples, 7 features, and a small amount of noise.\n2. Implement a custom `sklearn` transformer (inheriting from `BaseEstimator`, `TransformerMixin`) named `CustomPolynomialFeatures`.\n   This transformer should take a list of feature names (or indices) as an initialization argument. Its `transform` method should apply `PolynomialFeatures` (with `degree=2`, `include_bias=False`) only to the specified features, and pass through other features unchanged.\n3. Create an `sklearn.pipeline.Pipeline` that uses an `sklearn.compose.ColumnTransformer`.\n   *   Apply `StandardScaler` to all numerical features *not* handled by your custom transformer.\n   *   Apply your `CustomPolynomialFeatures` transformer to 3-4 specific numerical features.\n   *   The pipeline should then fit a `Ridge` regressor.\n4. Evaluate the pipeline's performance using `sklearn.model_selection.cross_val_score` with 5-fold cross-validation and `neg_mean_squared_error` as the scoring metric.\n5. Print the mean and standard deviation of the Mean Squared Error (MSE) for the pipeline (remembering to convert `neg_mean_squared_error` to positive MSE values).",
  "2025-12-12": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 4 numerical features, and 2 classes. Convert the features and target into a pandas DataFrame.\n2. Select two of the original numerical features (e.g., `feature_0`, `feature_1`) and apply `pd.cut` to each of them to discretize them into 3-4 bins (e.g., `['low', 'medium', 'high']`). These new binned features should be categorical and added to the DataFrame.\n3. Create an `sklearn.compose.ColumnTransformer` to preprocess the data:\n    *   For the *remaining original* numerical features (those not binned): Apply `StandardScaler`.\n    *   For the *newly binned* categorical features: Apply `OneHotEncoder(handle_unknown='ignore')`.\n4. Construct an `sklearn.pipeline.Pipeline` that first applies this `ColumnTransformer` and then trains a `GradientBoostingClassifier` (set `random_state` for reproducibility).\n5. Evaluate the complete pipeline's performance using 5-fold cross-validation (`sklearn.model_selection.cross_val_score`) with `accuracy` as the scoring metric. Report the mean accuracy and its standard deviation.",
  "2025-12-13": "1. Create an in-memory SQLite database using the `sqlite3` module.\n2. Create a `transactions` table with the following columns: `transaction_id` (INTEGER PRIMARY KEY), `customer_id` (INTEGER), `product_id` (INTEGER), `transaction_date` (TEXT in 'YYYY-MM-DD' format), and `amount` (REAL).\n3. Insert synthetic data into the `transactions` table. Include at least 5 distinct customers, 3 distinct products, and 20-30 transactions spanning a few months.\n4. Write a single SQL query that uses **window functions** to calculate the following for each transaction:\n    *   `customer_monthly_total`: The sum of `amount` for that specific `customer_id` within the month of the `transaction_date`.\n    *   `customer_monthly_avg_transaction`: The average `amount` for that specific `customer_id` within the month of the `transaction_date`.\n    *   `customer_cumulative_total`: The running total of `amount` for that specific `customer_id`, ordered by `transaction_date`.\n5. Retrieve the results of this SQL query into a pandas DataFrame. Display `transaction_date`, `customer_id`, `amount`, `customer_monthly_total`, `customer_monthly_avg_transaction`, and `customer_cumulative_total`. Show the head of the DataFrame.",
  "2025-12-14": "1. Generate a pandas DataFrame with synthetic transaction data, including `customer_id` (5-10 unique), `transaction_date` (spanning 6-12 months of daily data), `amount` (random float), and `product_category` (3-5 unique strings).\n2. For each transaction, calculate two new features using *window functions* (Pandas `groupby` + `rolling` or `expanding`):\n    *   `customer_30d_avg_spend`: The average `amount` for that specific `customer_id` over the *past 30 days*, inclusive of the current transaction date.\n    *   `customer_cumulative_transactions`: The running total count of transactions for that specific `customer_id`, ordered by `transaction_date`.\n3. Aggregate the data to find:\n    *   The total `amount` spent by each `customer_id` for each `month`.\n    *   The `product_category` with the highest total `amount` spent across *all* customers for each `month`.\n4. Display the head of the DataFrame with the new features and print the aggregated monthly customer spending and the top product categories per month.",
  "2025-12-15": "1. Generate a synthetic regression dataset using `sklearn.datasets.make_regression` with 1000 samples, 5 informative features, and a small amount of noise. Additionally, generate 5 completely random, uninformative features (e.g., using `np.random.rand`) and concatenate them to your original features, creating a feature matrix `X` with 10 features.\n2. Create an `sklearn.pipeline.Pipeline` that first applies `StandardScaler`, then uses `sklearn.feature_selection.SelectKBest` (with `f_regression` as the score function) for feature selection, and finally fits a `LinearRegression` model.\n3. Define a hyperparameter grid for `sklearn.model_selection.GridSearchCV` to tune the `k` parameter of `SelectKBest` (e.g., `[3, 5, 7, 10]` to explore different numbers of selected features).\n4. Use `GridSearchCV` with the pipeline and the defined parameter grid. Perform 3-fold cross-validation and use `neg_mean_squared_error` as the scoring metric.\n5. Report the best `k` value found, the corresponding best cross-validation score (converting `neg_mean_squared_error` to positive MSE), and the indices of the features selected by the best model (you might need to extract the `SelectKBest` step from the best estimator).",
  "2025-12-16": "1. Generate a pandas DataFrame with a `date` column (daily data for 2-3 years, starting from a fixed date like '2020-01-01'), a `value` column (synthetic time-series data with a linear trend, seasonality using `np.sin`, and some noise), and two additional numerical features (`feature_A`, `feature_B`) which can be random.\n2. Using `pandas` operations, create the following new features:\n    *   `lag_1_value`: The `value` from the previous day.\n    *   `rolling_7d_mean_feature_A`: A 7-day rolling mean of `feature_A`.\n    *   `day_of_week_num`: Numerical day of the week (0-6).\n    *   `month_num`: Numerical month (1-12).\n3. Handle any `NaN` values introduced by lag/rolling features (e.g., by dropping the first few rows).\n4. Split the dataset into training and testing sets based on time (e.g., use the first 80% of data for training and the remaining 20% for testing).\n5. Construct an `sklearn.pipeline.Pipeline` that first applies `StandardScaler` to all numerical features (including the newly engineered ones) and then trains a `Ridge` regressor.\n6. Train the pipeline on the training data and evaluate its performance on the test set, reporting the Mean Absolute Error (MAE) and R-squared score.",
  "2025-12-17": "1. Generate a synthetic regression dataset using `sklearn.datasets.make_regression` with at least 800 samples, 4 informative features, and a small amount of noise.\n2. Create a new numerical feature named `time_of_day` for each sample, ranging from 0 to 23 (e.g., using `np.random.randint`). Add this feature to your feature matrix `X`.\n3. From `time_of_day`, create two new features: `time_of_day_sin` and `time_of_day_cos`, applying sine and cosine transformations respectively (e.g., `np.sin(2 * np.pi * time_of_day / 24)`).\n4. Create two `sklearn.pipeline.Pipeline` objects:\n    *   `pipeline_raw_tod`: Uses `sklearn.compose.ColumnTransformer` to apply `StandardScaler` to all original `make_regression` features AND the raw `time_of_day` feature. Then fit a `Ridge` regressor.\n    *   `pipeline_cyclical_tod`: Uses `sklearn.compose.ColumnTransformer` to apply `StandardScaler` to all original `make_regression` features AND the `time_of_day_sin` and `time_of_day_cos` features. The raw `time_of_day` feature should *not* be used in this pipeline.\n5. Evaluate both pipelines using `sklearn.model_selection.cross_val_score` with 5-fold cross-validation and `r2` as the scoring metric.\n6. Print the mean and standard deviation of the R-squared scores for both pipelines, clearly indicating the performance difference due to cyclical feature encoding.",
  "2025-12-18": "1. Generate a synthetic multi-class classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 10 features (e.g., `n_informative=5`), and 3 distinct classes (set `random_state` for reproducibility).\n2. Split the dataset into training and testing sets (e.g., 80/20 split) using `sklearn.model_selection.train_test_split`.\n3. Train a `sklearn.ensemble.RandomForestClassifier` on the training data (set `random_state` for reproducibility).\n4. Predict class labels on the test set.\n5. Print a detailed classification report using `sklearn.metrics.classification_report` to show precision, recall, and f1-score for each class.\n6. Plot the confusion matrix for the test set predictions using `sklearn.metrics.ConfusionMatrixDisplay.from_estimator`, ensuring appropriate labels and a title.",
  "2025-12-19": "1. Generate a synthetic regression dataset using `sklearn.datasets.make_regression` with 1000 samples, 6 informative features, and a small amount of noise. Convert the features (`X`) into a pandas DataFrame, assigning generic column names (e.g., `feature_0`, `feature_1`, ..., `feature_5`).\n2. **Feature Engineering**: Create a new interaction feature named `feature_0_x_feature_1` by multiplying `feature_0` and `feature_1`. Add this new feature to a *copy* of your DataFrame to create `X_with_interaction`. Keep the original DataFrame as `X_original`.\n3. Create two `sklearn.pipeline.Pipeline` objects, both consisting of `StandardScaler` followed by `LinearRegression`.\n    *   `pipeline_no_interaction`\n    *   `pipeline_with_interaction`\n4. Evaluate both pipelines using `sklearn.model_selection.cross_val_score` with 5-fold cross-validation and `neg_mean_squared_error` as the scoring metric:\n    *   Evaluate `pipeline_no_interaction` using `X_original` and the target `y`.\n    *   Evaluate `pipeline_with_interaction` using `X_with_interaction` and the target `y`.\n    Print the mean and standard deviation of the Mean Squared Error (MSE) for both, clearly labeling results.\n5. **Feature Importance Visualization**: Train `pipeline_with_interaction` on the *entire* `X_with_interaction` and `y` dataset. Extract the coefficients from the `LinearRegression` model within this trained pipeline. Create a bar plot using `matplotlib.pyplot` or `seaborn` showing the *absolute magnitude* of these coefficients. Map these to their corresponding feature names from `X_with_interaction`. Title the plot appropriately, e.g., 'Linear Regression Coefficients (Absolute Magnitude)'.",
  "2025-12-20": "1. Create an in-memory SQLite database using the `sqlite3` module.\n2. Create two tables:\n    *   `customers` with columns: `customer_id` (INTEGER PRIMARY KEY), `name` (TEXT), `region` (TEXT).\n    *   `orders` with columns: `order_id` (INTEGER PRIMARY KEY), `customer_id` (INTEGER, FOREIGN KEY), `order_date` (TEXT in 'YYYY-MM-DD' format), `total_amount` (REAL).\n3. Insert synthetic data into both tables. Ensure you have at least 5 distinct customers, 3 distinct regions, and 20-30 orders spanning a few months, with some customers having multiple orders.\n4. Write a single SQL query to find the `customer_id`, `name`, `region`, and `total_spent` (sum of all their `total_amount`s) for customers whose `total_spent` is greater than the `average_order_value_per_region` (the average `total_amount` of all orders originating from their specific `region`). Order the results by `total_spent` in descending order.\n5. Retrieve the results of this SQL query into a pandas DataFrame and display the head of the DataFrame.",
  "2025-12-21": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 5 informative features, and a significant class imbalance (e.g., `weights=[0.9, 0.1]` for 90% majority, 10% minority). Set `random_state` for reproducibility.\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Construct an `imblearn.pipeline.Pipeline` that first applies `StandardScaler` to the features, then applies `imblearn.over_sampling.SMOTE` (set `random_state`), and finally fits a `LogisticRegression` model (set `random_state`, `solver='liblinear'`).\n4. Train the pipeline on the *training data*.\n5. Evaluate the trained model's performance on the *test data*. Print the `sklearn.metrics.classification_report` to show precision, recall, and f1-score for each class, paying close attention to the minority class.\n6. Plot the Precision-Recall curve for the minority class on the test set using `sklearn.metrics.PrecisionRecallDisplay.from_estimator`, clearly labeling the plot with a title.",
  "2025-12-22": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 5 numerical features, and 1 conceptual 'high-cardinality' categorical feature. To create this categorical feature, generate a numerical feature with a large number of unique integer values (e.g., 50-100) and then convert it to string type, adding it to your feature DataFrame.\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `train_test_split`.\n3. Create two distinct `sklearn.pipeline.Pipeline` objects for preprocessing and modeling:\n    *   `pipeline_onehot_encoding`: Use `sklearn.compose.ColumnTransformer`. For the numerical features, apply `StandardScaler`. For the high-cardinality categorical feature, apply `OneHotEncoder(handle_unknown='ignore')`.\n    *   `pipeline_feature_hashing`: Use `sklearn.compose.ColumnTransformer`. For the numerical features, apply `StandardScaler`. For the high-cardinality categorical feature, apply `FeatureHasher(n_features=15, input_type='string')` (you may adjust `n_features`).\n4. Both pipelines should then fit a `LogisticRegression` model (using `solver='liblinear'` and a `random_state` for reproducibility).\n5. Train both pipelines on the training data and evaluate their performance on the test set. Report the `accuracy_score` and `f1_score` for each pipeline, clearly stating which encoding strategy yielded which result.\n6. For both pipelines, calculate probability predictions on the test set. Create two plots using `sklearn.calibration.CalibrationDisplay.from_estimator` (one for each pipeline) to visualize model calibration. Arrange them side-by-side or clearly distinguish them with titles indicating the encoding method. Discuss briefly which model appears better calibrated based on the plots.",
  "2025-12-23": "1. Generate a synthetic dataset of 500-1000 short text documents (e.g., short sentences or phrases) belonging to 3 distinct categories. Ensure some keywords are strongly associated with each category.\n2. Split the dataset into training and testing sets (e.g., 70/30 split).\n3. Construct an `sklearn.pipeline.Pipeline` that first applies `sklearn.feature_extraction.text.TfidfVectorizer` to convert text into numerical features, and then trains an `sklearn.linear_model.LogisticRegression` model (set `random_state` and `solver='liblinear'` for reproducibility).\n4. Train the pipeline on the training data and make predictions on the test data.\n5. Print the `sklearn.metrics.classification_report` for the test set predictions.\n6. From the *trained* pipeline, extract the `TfidfVectorizer` and `LogisticRegression` steps. Identify and print the top 5 most important features (words) for *each class* based on the `LogisticRegression` coefficients (e.g., highest positive coefficients for each class). Briefly interpret what these features tell you about each class.",
  "2025-12-24": "1. Generate a synthetic regression dataset: Create a pandas DataFrame `X` with 1000 samples and 3 numerical features (e.g., `feature_A`, `feature_B`, `feature_C`) using `np.random.rand()` for values between 0 and 1. Generate a target variable `y` such that `y = 2 * X['feature_A'] + 3 * (X['feature_B']**2) - X['feature_C'] + np.random.normal(0, 0.5, size=1000)`. \n2. Visualize feature-target relationships: Use `seaborn.jointplot` or `seaborn.pairplot` to explore the relationships between each feature in `X` and the target `y`. Pay close attention to any non-linear patterns that might be present.\n3. Engineer a new feature: Based on your visual inspection from step 2, identify the feature that appears to have a non-linear relationship with `y` and create a new feature by squaring that specific feature (e.g., if `feature_B` shows a quadratic relationship, create `feature_B_squared`). Add this new feature to a copy of your DataFrame, named `X_engineered`.\n4. Build and compare pipelines: Create two `sklearn.pipeline.Pipeline` objects:\n    *   `pipeline_original`: Apply `StandardScaler` to the original DataFrame `X` and then fit a `LinearRegression` model.\n    *   `pipeline_engineered`: Apply `StandardScaler` to the `X_engineered` DataFrame (which includes original features plus the new squared feature) and then fit a `LinearRegression` model.\n5. Evaluate performance: Use `sklearn.model_selection.cross_val_score` with 5-fold cross-validation and `neg_mean_squared_error` as the scoring metric for both pipelines. Print the mean and standard deviation of the Mean Squared Error (MSE) for each, clearly indicating the performance difference achieved by the engineered feature (remember to convert `neg_mean_squared_error` to positive MSE values).",
  "2025-12-25": "1. Generate a synthetic dataset using `sklearn.datasets.make_blobs` with at least 1000 samples, 5 features, and 4 distinct centers (clusters). Set `random_state=42`.\n2. Apply `sklearn.preprocessing.StandardScaler` to the generated features.\n3. Implement the Elbow method and Silhouette analysis: For a range of K values (e.g., from 2 to 8), train `sklearn.cluster.KMeans` (set `random_state=42`). For each K, record the `inertia_` and the `silhouette_score` (using the scaled data and the predicted cluster labels).\n4. Plot both the `inertia_` values (Elbow curve) and `silhouette_score` values against the corresponding K values. Clearly label the axes and provide descriptive titles for both plots.\n5. Based on your plots, identify the optimal number of clusters (K). Train a final `KMeans` model using this optimal K on the scaled data.\n6. Reduce the dimensionality of the *scaled* features to 2 principal components using `sklearn.decomposition.PCA`. Transform the original scaled data and the centroids of your final `KMeans` model into this 2D PCA space.\n7. Visualize the final clusters in the 2D PCA space. Plot the data points, coloring them according to their assigned cluster labels from the final `KMeans` model. Overlay the transformed cluster centroids on the plot. Ensure axes are labeled and the plot has a clear title.",
  "2025-12-26": "1. Generate a pandas DataFrame with 800 samples, including:\n    *   Three numerical features: `amount` (random positive floats, intentionally introduce some high outliers to simulate skewed transactions), `age` (random integers between 18-65), `duration_months` (random integers between 1-120).\n    *   One categorical feature: `region` (e.g., 'North', 'South', 'East', 'West' with varying proportions).\n    *   Ensure `amount` has a right-skewed distribution with some clear outliers (e.g., using `np.random.exponential` or adding a few large values).\n2. Calculate and display comprehensive descriptive statistics for the numerical features, grouped by the `region` categorical feature (e.g., mean, median, standard deviation, min, max, quartiles).\n3. Create a set of subplots (e.g., 1 row, 2 columns) to visualize the distribution of `amount` and `duration_months` across different `region` categories. Use `seaborn.boxplot` or `seaborn.violinplot` for these visualizations. Ensure plots have appropriate titles and labels.\n4. Focus on the `amount` feature. Apply a `log1p` transformation (i.e., `np.log1p(feature)`) to this feature to mitigate its skewness and outliers. Create another set of side-by-side subplots showing:\n    *   A histogram or Kernel Density Estimate (KDE) plot of the *original* `amount` distribution.\n    *   A histogram or KDE plot of the *log1p-transformed* `amount` distribution.\n    Clearly label titles to highlight the effect of the transformation.\n5. Compute the pairwise correlation matrix for all numerical features in the DataFrame (using the *log1p-transformed* `amount` for the correlation calculation). Visualize this matrix using a `seaborn.heatmap` with annotations, ensuring a clear title.",
  "2025-12-27": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 6 numerical features, and 2 classes (set `random_state` for reproducibility). Convert the features (`X`) and target (`y`) into a pandas DataFrame.\n2. Introduce missing values into the DataFrame:\n    *   For `feature_0`: Randomly replace approximately 15% of its values with `np.nan`.\n    *   For `feature_1`: Randomly replace approximately 10% of its values with `np.nan`.\n    *   For `feature_2`: Randomly replace approximately 5% of its values with `np.nan`.\n3. Create an `sklearn.pipeline.Pipeline` that first applies a `sklearn.compose.ColumnTransformer` for preprocessing and then fits a `sklearn.ensemble.RandomForestClassifier` (set `random_state` for reproducibility).\n    *   **Inside the `ColumnTransformer`**:\n        *   For `feature_0`: Apply `SimpleImputer(strategy='mean')` followed by `StandardScaler`.\n        *   For `feature_1`: Apply `KNeighborsImputer(n_neighbors=5)` followed by `StandardScaler`.\n        *   For `feature_2`: Apply `SimpleImputer(strategy='median')` followed by `StandardScaler`.\n        *   For the *remaining numerical features* (`feature_3` to `feature_5`): Apply `StandardScaler` directly (no imputation needed).\n4. Evaluate the complete pipeline's performance using 5-fold cross-validation (`sklearn.model_selection.cross_val_score`) with `accuracy` as the scoring metric.\n5. Report the mean accuracy and its standard deviation from the cross-validation.",
  "2025-12-28": "1. Generate a 2D NumPy array (e.g., 64x64 pixels) initialized with zeros, representing a black grayscale image.\n2. Add several synthetic 'objects' to this image using NumPy slicing and broadcasting:\n    *   A bright square in the center (e.g., value 200).\n    *   A horizontal bright line (e.g., value 150).\n    *   A diagonal bright line (e.g., value 100) from top-left to bottom-right.\n3. Implement a basic 2D convolution function `manual_convolve2d(image, kernel)` using NumPy operations (avoid `scipy.signal.convolve2d` for this task). Your function should handle padding or border effects as you deem appropriate (e.g., 'same' padding).\n4. Define a 3x3 'edge detection' kernel (e.g., a simple Sobel or Laplacian approximation).\n5. Apply your `manual_convolve2d` function with the edge detection kernel to your generated image.\n6. Visualize the original synthetic image and the convolved (edge-detected) image side-by-side using `matplotlib.pyplot.imshow`. Use a 'gray' colormap and ensure both plots have appropriate titles (e.g., 'Original Image', 'Edge Detected Image').",
  "2025-12-29": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 5 informative features, and a significant class imbalance (e.g., `weights=[0.9, 0.1]` for 90% majority, 10% minority). Set `random_state` for reproducibility.\n2. Split the dataset into training and testing sets (e.g., 70/30 split).\n3. Define a custom scoring function using `sklearn.metrics.make_scorer` that prioritizes the F1-score for the *minority class* (class 1). Hint: use `average='binary'` and `pos_label=1` for `f1_score`.\n4. Construct an `sklearn.pipeline.Pipeline` that first applies `StandardScaler` to the features and then fits a `LogisticRegression` model (set `random_state`, `solver='liblinear'`).\n5. Define a hyperparameter distribution for `RandomizedSearchCV` to tune `LogisticRegression`'s `C` parameter (e.g., `scipy.stats.loguniform(1e-3, 1e2)`). Explore at least 10 different parameter settings (`n_iter=10`).\n6. Perform `RandomizedSearchCV` with the pipeline, the defined parameter distributions, 3-fold cross-validation, and your *custom minority class F1-scorer*.\n7. Report the best `C` value found, the corresponding best cross-validation score, and print a full `classification_report` for the test set using the best estimator found by `RandomizedSearchCV`.",
  "2025-12-30": "1. Generate a synthetic regression dataset using `sklearn.datasets.make_regression` with 1000 samples, 3 informative features. Modify the target `y` to introduce a non-linear relationship (e.g., `y_original + 2 * X[:, 0]**2 + np.random.normal(0, 0.5, size=1000)`).\n2. Split the dataset into training and testing sets (e.g., 80/20 split) using `sklearn.model_selection.train_test_split`.\n3. Apply `sklearn.preprocessing.StandardScaler` to the features (`X`) on the training data and transform both training and testing sets.\n4. Build a simple sequential neural network model using `tensorflow.keras.models.Sequential`. The model should have:\n    *   An input layer matching the number of features.\n    *   At least one hidden `Dense` layer with `relu` activation (e.g., 32 units).\n    *   An output `Dense` layer with a single unit and linear activation.\n5. Compile the model using the `adam` optimizer and `mean_squared_error` loss.\n6. Train the model on the scaled training data for a suitable number of epochs (e.g., 50-100) and a batch size.\n7. Evaluate the trained model's performance on the scaled test set, reporting the Mean Squared Error (MSE).\n8. Visualize the model's predictions against the actual test target values using a scatter plot. Add a perfect prediction line (y=x) for comparison and label axes appropriately.",
  "2025-12-31": "1. Generate a synthetic transactional dataset using `pandas`:\n    *   Create a DataFrame named `transactions_df` with 800-1000 rows.\n    *   Columns should include:\n        *   `transaction_id` (unique integer IDs).\n        *   `customer_id` (e.g., 50-100 distinct customer IDs).\n        *   `transaction_date` (daily dates spanning 1-2 years, starting from '2022-01-01', ensuring multiple transactions per day/customer).\n        *   `amount` (random float values, e.g., between 10.0 and 500.0).\n        *   `product_category` (3-5 distinct string categories, e.g., 'Electronics', 'Books', 'Groceries', 'Clothing').\n    *   Sort the DataFrame by `customer_id` and then `transaction_date`.\n2. Create an in-memory SQLite database using `sqlite3` and load the `transactions_df` into a table named `transactions`.\n3. **SQL Analytics (Window Function 1 - Running Total)**: Write an SQL query to calculate the `running_total_amount` for each customer, ordered by their `transaction_date`. The query should return `transaction_id`, `customer_id`, `transaction_date`, `amount`, and the new `running_total_amount` column. Retrieve the results into a pandas DataFrame and display its head.\n4. **SQL Analytics (Window Function 2 - Rank within Group)**: Write a *separate* SQL query to rank transactions by `amount` in descending order *within each `product_category`*. The query should return `transaction_id`, `product_category`, `amount`, and the new `rank_in_category` column. Retrieve these results into another pandas DataFrame and display its head.\n5. Briefly describe what each window function achieves and how it's useful in analytical contexts.",
  "2026-01-01": "1. **Generate Synthetic Time Series Data**: Create a pandas DataFrame `df` with 800 daily entries. It should have a `date` column (daily dates starting from '2023-01-01') and a `sales_amount` column. Populate `sales_amount` with a synthetic time series, for example, a combination of a linear trend, a yearly seasonality (using `np.sin`), and some random noise (using `np.random.normal`).\n2. **Feature Engineering - Lag Features**: Create two new features in the DataFrame: `sales_amount_lag_1` (representing the sales from the previous day) and `sales_amount_lag_7` (representing sales from 7 days prior) using pandas' `shift()` method.\n3. **Feature Engineering - Rolling Statistics**: Create two more new features: `rolling_mean_3_days` (a 3-day rolling mean of `sales_amount`) and `rolling_std_7_days` (a 7-day rolling standard deviation of `sales_amount`) using pandas' `rolling()` method. Ensure you handle potential `NaN` values from rolling operations (e.g., `min_periods=1` if you want to keep early rows, or simply let `NaN`s propagate and drop later).\n4. **Handle Missing Values and Prepare for Modeling**: After creating all engineered features, drop any rows that contain `NaN` values (which will typically appear at the beginning of the DataFrame due to `shift` and `rolling` operations). Then, define the target variable `y` as the `sales_amount` column of the processed DataFrame, and the features `X` as all the engineered lag and rolling features. Display the head of the final `X` and `y` DataFrames to show the prepared dataset.\n5. **Visualize Feature-Target Relationship**: Pick one of the engineered features (e.g., `sales_amount_lag_1`) and plot its relationship with the `sales_amount` target using a `seaborn.lineplot` or `seaborn.scatterplot`. Ensure the plot has appropriate labels and a title.",
  "2026-01-02": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1500 samples, 8 features (e.g., `n_informative=6`), and 2 classes (set `random_state` for reproducibility).\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Create two distinct `sklearn.pipeline.Pipeline` objects for classification:\n    *   `pipeline_lr`: Consisting of `StandardScaler` followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'`).\n    *   `pipeline_gb`: Consisting of `StandardScaler` followed by `GradientBoostingClassifier` (set `random_state=42`).\n4. For each pipeline, define a small, focused hyperparameter grid for `sklearn.model_selection.GridSearchCV`:\n    *   For `pipeline_lr`: Tune `C` (e.g., `[0.1, 1, 10]`).\n    *   For `pipeline_gb`: Tune `n_estimators` (e.g., `[50, 100]`) and `learning_rate` (e.g., `[0.05, 0.1]`).\n5. Perform `GridSearchCV` *separately* for `pipeline_lr` and `pipeline_gb` on the training data. Use 3-fold cross-validation and `scoring='roc_auc'` for both. (Set `n_jobs=-1` for faster execution).\n6. Report the `best_params_` and `best_score_` (ROC AUC) for each model (Logistic Regression and Gradient Boosting Classifier).\n7. Using the *best estimators* obtained from `GridSearchCV` for both models, predict probabilities for the positive class (class 1) on the test set.\n8. Plot the Receiver Operating Characteristic (ROC) curve for *both* models on the *same* plot using `sklearn.metrics.RocCurveDisplay.from_estimator` (or `from_predictions` if preferred). Ensure the plot has a clear title, a legend indicating which curve belongs to which model, and displays the AUC score for each model.",
  "2026-01-03": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 4 informative features, and 2 classes (set `random_state` for reproducibility).\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Standardize the features using `sklearn.preprocessing.StandardScaler` on the training data, then transform both training and testing sets.\n4. Build a simple sequential neural network model using `tensorflow.keras.models.Sequential` for binary classification. The model should have:\n    *   An input layer matching the number of features.\n    *   One hidden `Dense` layer with `relu` activation (e.g., 16-32 units).\n    *   An output `Dense` layer with a single unit and `sigmoid` activation.\n5. Compile the model using the `adam` optimizer and `binary_crossentropy` loss.\n6. Train the model on the scaled training data for a suitable number of epochs (e.g., 30-50) and a batch size.\n7. Predict probabilities on the scaled test set. Convert these probabilities to binary class labels (0 or 1) using a threshold (e.g., 0.5).\n8. Print the `sklearn.metrics.classification_report` for the test set predictions.\n9. Generate and plot a confusion matrix using `sklearn.metrics.ConfusionMatrixDisplay.from_predictions` for the test set, clearly labeling the plot with a title (e.g., 'Confusion Matrix for Keras Binary Classifier').",
  "2026-01-04": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 5 informative features, and 2 classes (set `random_state` for reproducibility).\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Define a function `create_keras_model(optimizer='adam', units=32)` that builds and compiles a `tensorflow.keras.models.Sequential` model. This model should have an input layer matching the number of features, one `Dense` hidden layer with `relu` activation (using `units` as a parameter), and a `Dense` output layer with `sigmoid` activation. Compile it with `binary_crossentropy` loss.\n4. Wrap this Keras model using `tensorflow.keras.wrappers.scikit_learn.KerasClassifier`.\n5. Construct an `sklearn.pipeline.Pipeline` that first applies `sklearn.preprocessing.StandardScaler` and then uses the wrapped `KerasClassifier`.\n6. Define a hyperparameter grid for `sklearn.model_selection.GridSearchCV` to tune the following parameters of the Keras model within the pipeline:\n    *   `kerasclassifier__batch_size` (e.g., `[32, 64]`)\n    *   `kerasclassifier__epochs` (e.g., `[10, 20]`)\n    *   `kerasclassifier__model__units` (e.g., `[16, 32]`)\n    *   `kerasclassifier__optimizer` (e.g., `['adam', 'rmsprop']`)\n7. Perform `GridSearchCV` with 3-fold cross-validation and `scoring='roc_auc'` on the training data. (Set `n_jobs=-1` for faster execution).\n8. Report the `best_params_` and `best_score_` from `GridSearchCV`. Then, using the `best_estimator_`, predict class labels on the test set and print the `sklearn.metrics.classification_report`.",
  "2026-01-05": "1. **Generate Synthetic Time Series Data**: Create a pandas DataFrame `df` with 500-700 daily entries. It should have a `date` column (daily dates starting from '2022-01-01') and a `value` column. Populate `value` with a synthetic time series exhibiting a linear trend, strong weekly seasonality (e.g., higher values on weekends), and some random noise.\n2. **Feature Engineering - Advanced Time-Based Features**: Create the following new features in the DataFrame:\n    *   `day_of_week`: Extract the day of the week (0-6 or names) from the `date` column.\n    *   `is_weekend`: A binary flag (1 if weekend, 0 otherwise).\n    *   `exponential_moving_average_7d`: Calculate a 7-day Exponential Moving Average (EMA) of the `value` column using `df['value'].ewm(span=7, adjust=False).mean()`.\n3. **Handle Missing Values and Prepare for Modeling**: Drop any rows that contain `NaN` values (due to EMA calculation). Define the target variable `y` as the `value` column, and features `X` as `day_of_week`, `is_weekend`, and `exponential_moving_average_7d`. Convert `day_of_week` into one-hot encoded features using `pd.get_dummies()`.\n4. **Visualize Engineered Features**: Create a plot (e.g., using `seaborn.lineplot` or `seaborn.boxplot`) to visualize the relationship between `day_of_week` (or `is_weekend`) and the original `value` to confirm the seasonality. Also, plot the original `value` and the `exponential_moving_average_7d` on the same time-series plot to show the smoothing effect.\n5. **Build and Evaluate a Regression Model**: Split the dataset into training and testing sets (e.g., 80/20 split). Train a `LinearRegression` model using the engineered features. Evaluate its performance on the test set using `sklearn.metrics.mean_absolute_error` and `sklearn.metrics.r2_score`. Report both metrics.",
  "2026-01-06": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 6 informative features, and 2 classes (set `random_state` for reproducibility). Convert the features (`X`) and target (`y`) into a pandas DataFrame.\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Create two distinct `sklearn.pipeline.Pipeline` objects for classification:\n    *   `pipeline_baseline`: Consisting of `StandardScaler` followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   `pipeline_polynomial`: Consisting of `StandardScaler`, then `PolynomialFeatures(degree=2, include_bias=False)`, followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n4. Evaluate both pipelines using 5-fold cross-validation (`sklearn.model_selection.cross_val_score`) on the training data. Use `scoring='roc_auc'` for performance comparison.\n5. Report the mean and standard deviation of the ROC AUC scores for both the `pipeline_baseline` and `pipeline_polynomial`. Based on these results, briefly discuss the impact of including polynomial features on model performance for this dataset.",
  "2026-01-07": "1. **Generate Synthetic Data**: Create two pandas DataFrames:\n    *   `customers_df`: With 100-150 rows. Columns: `customer_id` (unique integers), `customer_name` (e.g., 'Customer A'), `registration_date` (random dates over the last 3 years).\n    *   `orders_df`: With 800-1000 rows. Columns: `order_id` (unique integers), `customer_id` (randomly sampled from `customers_df` IDs, ensuring some customers have no orders and others have many), `order_date` (random dates after their `registration_date`), `amount` (random floats between 10 and 1000).\n2. **Load into SQLite**: Create an in-memory SQLite database using `sqlite3` and load both `customers_df` and `orders_df` into tables named `customers` and `orders` respectively.\n3. **SQL Query - Customer Sales Aggregation and Segmentation**: Write a single SQL query that performs the following:\n    *   **Joins** the `customers` and `orders` tables.\n    *   **Aggregates** to calculate the `total_sales_amount` and `number_of_orders` for each customer. Ensure that customers with no orders are still included in the result, showing 0 for sales and orders.\n    *   **Categorizes** each customer into a `customer_segment` based on their `total_sales_amount`:\n        *   'High-Value' if `total_sales_amount` > 5000\n        *   'Medium-Value' if `total_sales_amount` between 1000 and 5000 (inclusive)\n        *   'Low-Value' if `total_sales_amount` between 1 and 999 (inclusive)\n        *   'No-Orders' if `total_sales_amount` is 0 or NULL.\n    *   The query should return `customer_id`, `customer_name`, `registration_date`, `total_sales_amount`, `number_of_orders`, and `customer_segment`.\n4. **Retrieve and Display**: Fetch the results into a pandas DataFrame. Display its head and then print the count of customers within each `customer_segment` to verify your segmentation logic.",
  "2026-01-08": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with 1000 samples, 4 numerical features, and 2 classes (`random_state=42`). Convert `X` into a pandas DataFrame and add a categorical feature named `color` (e.g., 'red', 'blue', 'green', 'yellow' with random distribution, ensuring a good mix).\n2. Introduce missing values into the `color` feature by randomly replacing approximately 15% of its values with `np.nan`.\n3. Create an `sklearn.pipeline.Pipeline` that first applies a `sklearn.compose.ColumnTransformer` for preprocessing and then fits a `sklearn.linear_model.LogisticRegression` model (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   **Inside the `ColumnTransformer`**:\n        *   For the numerical features: Apply `SimpleImputer(strategy='mean')` followed by `StandardScaler`.\n        *   For the `color` categorical feature: Apply `SimpleImputer(strategy='most_frequent')` followed by `OneHotEncoder(handle_unknown='ignore')`.\n4. Evaluate the complete pipeline's performance using 5-fold cross-validation (`sklearn.model_selection.cross_val_score`) with `accuracy` as the scoring metric.\n5. Report the mean accuracy and its standard deviation from the cross-validation."
}