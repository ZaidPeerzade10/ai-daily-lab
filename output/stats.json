{
  "2025-11-30": "Given a pandas DataFrame containing at least four numerical features (`feature_A`, `feature_B`, `feature_C`, `feature_D`):\n1.  Create two new interaction features: `interaction_AB` (the product of `feature_A` and `feature_B`) and `interaction_CD` (the product of `feature_C` and `feature_D`).\n2.  Identify all numerical features (original and new) that have a skewness value greater than 0.75.\n3.  For each identified skewed feature, apply a `np.log1p` transformation, replacing the original feature column with its transformed version.\n4.  Display the head of the modified DataFrame and the skewness of all features after transformation.",
  "2025-12-01": "1. Generate a synthetic classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 5 numerical features, and 2 categorical features (one with 3 unique values, one with 5 unique values). Introduce missing values (e.g., `np.nan`) into two of the numerical features.\n2. Create an `sklearn.compose.ColumnTransformer` to preprocess the data:\n    *   For numerical features: Impute missing values with the mean, then apply `StandardScaler`.\n    *   For categorical features: Apply `OneHotEncoder`.\n3. Construct an `sklearn.pipeline.Pipeline` that first applies this `ColumnTransformer` and then trains a `RandomForestClassifier`.\n4. Evaluate the complete pipeline's performance using 5-fold cross-validation (`sklearn.model_selection.cross_val_score`) and report the mean accuracy and its standard deviation.",
  "2025-12-02": "1. Create an in-memory SQLite database using the `sqlite3` module.\n2. Create two tables: `customers` (columns: `customer_id` (INTEGER PRIMARY KEY), `name` (TEXT), `city` (TEXT)) and `orders` (columns: `order_id` (INTEGER PRIMARY KEY), `customer_id` (INTEGER), `amount` (REAL), `order_date` (TEXT)). Ensure `customer_id` in `orders` is a foreign key referencing `customers`.\n3. Insert sample data into both tables (at least 5 distinct customers and 10-15 orders, ensuring some customers have multiple orders).\n4. Using a single SQL query, calculate the total purchase `amount` for each customer, retrieving the customer's `name` and their `total_revenue`, joining the `customers` and `orders` tables.\n5. Retrieve these aggregated results directly into a pandas DataFrame and display the top 3 customers by their `total_revenue`.",
  "2025-12-03": "1. Generate a synthetic regression dataset using `sklearn.datasets.make_regression` with at least 500 samples and 5 features.\n2. Create an `sklearn.pipeline.Pipeline` that first applies `StandardScaler` to the features and then fits a `Ridge` regressor.\n3. Define a hyperparameter grid for the `Ridge` regressor within the pipeline, tuning the `alpha` parameter across at least 3 distinct values (e.g., `[0.1, 1.0, 10.0]`).\n4. Use `sklearn.model_selection.GridSearchCV` with the pipeline and the defined parameter grid to find the best hyperparameters. Use `neg_mean_squared_error` as the scoring metric and apply 3-fold cross-validation.\n5. Report the best hyperparameters found by `GridSearchCV` and the corresponding best score (remembering to convert the `neg_mean_squared_error` back to a positive MSE value).",
  "2025-12-04": "1. Generate a synthetic dataset using `sklearn.datasets.make_blobs` with at least 500 samples, 4 numerical features, and 3 distinct clusters. Convert this into a pandas DataFrame, including the cluster labels as a feature (e.g., `cluster_id`).\n2. Add a new categorical feature to the DataFrame (e.g., `group`) with 2-3 distinct values, randomly assigned.\n3. Using `seaborn` and `matplotlib.pyplot`, create the following visualizations to explore the data:\n    *   A pair plot (`sns.pairplot`) for the numerical features, coloring the points by the `cluster_id`.\n    *   A set of histograms (or KDE plots) for `feature_1` and `feature_2`, separated for each unique value of the newly created `group` categorical feature (e.g., using `sns.FacetGrid` or `sns.histplot` with `hue` and `col`).\n    *   A box plot (or violin plot) showing the distribution of `feature_3` across the different `cluster_id`s.\n4. Ensure all plots have appropriate titles and labels.",
  "2025-12-05": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` (e.g., 1000 samples, 10 features, 2 informative features, 2 classes).\n2. Split the dataset into training and testing sets (e.g., 80/20 split) using `train_test_split`.\n3. Train a `LogisticRegression` model on the training data.\n4. Predict class labels and class probabilities for the positive class on the test set.\n5. Calculate and print the following evaluation metrics for the test set predictions: Accuracy, Precision, Recall, F1-score, and ROC AUC score.\n6. Plot the Receiver Operating Characteristic (ROC) curve for the model using `matplotlib.pyplot`, clearly labeling axes and adding a title. Include the AUC score in the plot legend.",
  "2025-12-06": "1. Generate a synthetic regression dataset using `sklearn.datasets.make_regression` with at least 500 samples, 3 informative features, and a small amount of noise.\n2. Create two distinct `sklearn.pipeline.Pipeline` objects:\n    *   `pipeline_simple`: Consisting of `StandardScaler` followed by `LinearRegression`.\n    *   `pipeline_poly`: Consisting of `PolynomialFeatures` (set `degree=2`), then `StandardScaler`, then `LinearRegression`.\n3. Evaluate both pipelines using `sklearn.model_selection.cross_val_score` with 5-fold cross-validation and `neg_mean_squared_error` as the scoring metric.\n4. Print the mean and standard deviation of the Mean Squared Error (MSE) for both pipelines, clearly indicating which result belongs to which pipeline. (Remember to convert `neg_mean_squared_error` to positive MSE values for interpretability).",
  "2025-12-07": "1. Generate a pandas DataFrame with a `timestamp` column (daily data for 2-3 years) and a `value` column (e.g., synthetic sales data with some trend and seasonality, using `np.sin` or similar).\n2. From the `timestamp` column, create new features: `month` (numerical month), `day_of_week` (name of the day, e.g., 'Monday'), and `is_weekend` (boolean).\n3. Calculate the average `value` aggregated by `month` and by `day_of_week`.\n4. Create two visualizations using `seaborn` and `matplotlib.pyplot`:\n    *   A line plot showing the average `value` trend across months.\n    *   A bar plot showing the average `value` for each `day_of_week`.\n5. Display the head of the DataFrame with the new features and print the aggregated dataframes for both monthly and daily trends.",
  "2025-12-08": "1. Generate a synthetic dataset suitable for clustering using `sklearn.datasets.make_blobs` with at least 700 samples, 6 numerical features, and 4 distinct clusters (do not use the true cluster labels for modeling).\n2. Apply `sklearn.cluster.KMeans` to the generated features to discover 4 clusters. Initialize KMeans with a `random_state` for reproducibility.\n3. Evaluate the quality of the discovered clusters by calculating the `sklearn.metrics.silhouette_score`.\n4. To visualize the clustering, reduce the dimensionality of the original features to 2 using `sklearn.decomposition.PCA`.\n5. Create a scatter plot using `matplotlib.pyplot` or `seaborn` of the 2 principal components, coloring the points based on the clusters identified by KMeans. Title the plot with the calculated Silhouette Score.",
  "2025-12-09": "1. Generate a synthetic binary classification dataset (e.g., using `sklearn.datasets.make_classification`) with at least 1000 samples, 5 numerical features, and 1 conceptual 'high-cardinality' categorical feature. To create this categorical feature, generate a numerical feature with a large number of unique integer values (e.g., 50-100) and then convert it to string type.\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `train_test_split`.\n3. Create two distinct `sklearn.pipeline.Pipeline` objects for preprocessing and modeling:\n    *   `pipeline_onehot_encoding`: Use `sklearn.compose.ColumnTransformer`. For the numerical features, apply `StandardScaler`. For the high-cardinality categorical feature, apply `OneHotEncoder(handle_unknown='ignore')`.\n    *   `pipeline_ordinal_encoding`: Use `sklearn.compose.ColumnTransformer`. For the numerical features, apply `StandardScaler`. For the high-cardinality categorical feature, apply `OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)`.\n4. Both pipelines should then fit a `LogisticRegression` model (using `solver='liblinear'` and a `random_state` for reproducibility).\n5. Train both pipelines on the training data and evaluate their performance on the test set. Report the `accuracy_score` and `f1_score` for each pipeline, clearly stating which encoding strategy yielded which result.",
  "2025-12-10": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_moons` with at least 1000 samples, `noise=0.1`, and `random_state=42`.\n2. Split the dataset into training and testing sets (e.g., 80/20 split) using `sklearn.model_selection.train_test_split`.\n3. Build a simple feedforward neural network using `tf.keras.Sequential`:\n    *   An input `tf.keras.layers.Dense` layer suitable for the number of features.\n    *   A hidden `Dense` layer with 32 units and `relu` activation.\n    *   Another hidden `Dense` layer with 16 units and `relu` activation.\n    *   An output `Dense` layer with 1 unit and `sigmoid` activation.\n4. Compile the model with `optimizer='adam'`, `loss='binary_crossentropy'`, and `metrics=['accuracy']`.\n5. Train the model on the training data for a fixed number of epochs (e.g., 50) with a batch size (e.g., 32), storing the training history.\n6. Evaluate the trained model on the test set and print the test accuracy.\n7. Plot the training and validation accuracy and loss over epochs from the training history using `matplotlib.pyplot`, clearly labeling the axes and providing a title.",
  "2025-12-11": "1. Generate a synthetic regression dataset using `sklearn.datasets.make_regression` with at least 500 samples, 7 features, and a small amount of noise.\n2. Implement a custom `sklearn` transformer (inheriting from `BaseEstimator`, `TransformerMixin`) named `CustomPolynomialFeatures`.\n   This transformer should take a list of feature names (or indices) as an initialization argument. Its `transform` method should apply `PolynomialFeatures` (with `degree=2`, `include_bias=False`) only to the specified features, and pass through other features unchanged.\n3. Create an `sklearn.pipeline.Pipeline` that uses an `sklearn.compose.ColumnTransformer`.\n   *   Apply `StandardScaler` to all numerical features *not* handled by your custom transformer.\n   *   Apply your `CustomPolynomialFeatures` transformer to 3-4 specific numerical features.\n   *   The pipeline should then fit a `Ridge` regressor.\n4. Evaluate the pipeline's performance using `sklearn.model_selection.cross_val_score` with 5-fold cross-validation and `neg_mean_squared_error` as the scoring metric.\n5. Print the mean and standard deviation of the Mean Squared Error (MSE) for the pipeline (remembering to convert `neg_mean_squared_error` to positive MSE values).",
  "2025-12-12": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 4 numerical features, and 2 classes. Convert the features and target into a pandas DataFrame.\n2. Select two of the original numerical features (e.g., `feature_0`, `feature_1`) and apply `pd.cut` to each of them to discretize them into 3-4 bins (e.g., `['low', 'medium', 'high']`). These new binned features should be categorical and added to the DataFrame.\n3. Create an `sklearn.compose.ColumnTransformer` to preprocess the data:\n    *   For the *remaining original* numerical features (those not binned): Apply `StandardScaler`.\n    *   For the *newly binned* categorical features: Apply `OneHotEncoder(handle_unknown='ignore')`.\n4. Construct an `sklearn.pipeline.Pipeline` that first applies this `ColumnTransformer` and then trains a `GradientBoostingClassifier` (set `random_state` for reproducibility).\n5. Evaluate the complete pipeline's performance using 5-fold cross-validation (`sklearn.model_selection.cross_val_score`) with `accuracy` as the scoring metric. Report the mean accuracy and its standard deviation.",
  "2025-12-13": "1. Create an in-memory SQLite database using the `sqlite3` module.\n2. Create a `transactions` table with the following columns: `transaction_id` (INTEGER PRIMARY KEY), `customer_id` (INTEGER), `product_id` (INTEGER), `transaction_date` (TEXT in 'YYYY-MM-DD' format), and `amount` (REAL).\n3. Insert synthetic data into the `transactions` table. Include at least 5 distinct customers, 3 distinct products, and 20-30 transactions spanning a few months.\n4. Write a single SQL query that uses **window functions** to calculate the following for each transaction:\n    *   `customer_monthly_total`: The sum of `amount` for that specific `customer_id` within the month of the `transaction_date`.\n    *   `customer_monthly_avg_transaction`: The average `amount` for that specific `customer_id` within the month of the `transaction_date`.\n    *   `customer_cumulative_total`: The running total of `amount` for that specific `customer_id`, ordered by `transaction_date`.\n5. Retrieve the results of this SQL query into a pandas DataFrame. Display `transaction_date`, `customer_id`, `amount`, `customer_monthly_total`, `customer_monthly_avg_transaction`, and `customer_cumulative_total`. Show the head of the DataFrame.",
  "2025-12-14": "1. Generate a pandas DataFrame with synthetic transaction data, including `customer_id` (5-10 unique), `transaction_date` (spanning 6-12 months of daily data), `amount` (random float), and `product_category` (3-5 unique strings).\n2. For each transaction, calculate two new features using *window functions* (Pandas `groupby` + `rolling` or `expanding`):\n    *   `customer_30d_avg_spend`: The average `amount` for that specific `customer_id` over the *past 30 days*, inclusive of the current transaction date.\n    *   `customer_cumulative_transactions`: The running total count of transactions for that specific `customer_id`, ordered by `transaction_date`.\n3. Aggregate the data to find:\n    *   The total `amount` spent by each `customer_id` for each `month`.\n    *   The `product_category` with the highest total `amount` spent across *all* customers for each `month`.\n4. Display the head of the DataFrame with the new features and print the aggregated monthly customer spending and the top product categories per month.",
  "2025-12-15": "1. Generate a synthetic regression dataset using `sklearn.datasets.make_regression` with 1000 samples, 5 informative features, and a small amount of noise. Additionally, generate 5 completely random, uninformative features (e.g., using `np.random.rand`) and concatenate them to your original features, creating a feature matrix `X` with 10 features.\n2. Create an `sklearn.pipeline.Pipeline` that first applies `StandardScaler`, then uses `sklearn.feature_selection.SelectKBest` (with `f_regression` as the score function) for feature selection, and finally fits a `LinearRegression` model.\n3. Define a hyperparameter grid for `sklearn.model_selection.GridSearchCV` to tune the `k` parameter of `SelectKBest` (e.g., `[3, 5, 7, 10]` to explore different numbers of selected features).\n4. Use `GridSearchCV` with the pipeline and the defined parameter grid. Perform 3-fold cross-validation and use `neg_mean_squared_error` as the scoring metric.\n5. Report the best `k` value found, the corresponding best cross-validation score (converting `neg_mean_squared_error` to positive MSE), and the indices of the features selected by the best model (you might need to extract the `SelectKBest` step from the best estimator).",
  "2025-12-16": "1. Generate a pandas DataFrame with a `date` column (daily data for 2-3 years, starting from a fixed date like '2020-01-01'), a `value` column (synthetic time-series data with a linear trend, seasonality using `np.sin`, and some noise), and two additional numerical features (`feature_A`, `feature_B`) which can be random.\n2. Using `pandas` operations, create the following new features:\n    *   `lag_1_value`: The `value` from the previous day.\n    *   `rolling_7d_mean_feature_A`: A 7-day rolling mean of `feature_A`.\n    *   `day_of_week_num`: Numerical day of the week (0-6).\n    *   `month_num`: Numerical month (1-12).\n3. Handle any `NaN` values introduced by lag/rolling features (e.g., by dropping the first few rows).\n4. Split the dataset into training and testing sets based on time (e.g., use the first 80% of data for training and the remaining 20% for testing).\n5. Construct an `sklearn.pipeline.Pipeline` that first applies `StandardScaler` to all numerical features (including the newly engineered ones) and then trains a `Ridge` regressor.\n6. Train the pipeline on the training data and evaluate its performance on the test set, reporting the Mean Absolute Error (MAE) and R-squared score.",
  "2025-12-17": "1. Generate a synthetic regression dataset using `sklearn.datasets.make_regression` with at least 800 samples, 4 informative features, and a small amount of noise.\n2. Create a new numerical feature named `time_of_day` for each sample, ranging from 0 to 23 (e.g., using `np.random.randint`). Add this feature to your feature matrix `X`.\n3. From `time_of_day`, create two new features: `time_of_day_sin` and `time_of_day_cos`, applying sine and cosine transformations respectively (e.g., `np.sin(2 * np.pi * time_of_day / 24)`).\n4. Create two `sklearn.pipeline.Pipeline` objects:\n    *   `pipeline_raw_tod`: Uses `sklearn.compose.ColumnTransformer` to apply `StandardScaler` to all original `make_regression` features AND the raw `time_of_day` feature. Then fit a `Ridge` regressor.\n    *   `pipeline_cyclical_tod`: Uses `sklearn.compose.ColumnTransformer` to apply `StandardScaler` to all original `make_regression` features AND the `time_of_day_sin` and `time_of_day_cos` features. The raw `time_of_day` feature should *not* be used in this pipeline.\n5. Evaluate both pipelines using `sklearn.model_selection.cross_val_score` with 5-fold cross-validation and `r2` as the scoring metric.\n6. Print the mean and standard deviation of the R-squared scores for both pipelines, clearly indicating the performance difference due to cyclical feature encoding.",
  "2025-12-18": "1. Generate a synthetic multi-class classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 10 features (e.g., `n_informative=5`), and 3 distinct classes (set `random_state` for reproducibility).\n2. Split the dataset into training and testing sets (e.g., 80/20 split) using `sklearn.model_selection.train_test_split`.\n3. Train a `sklearn.ensemble.RandomForestClassifier` on the training data (set `random_state` for reproducibility).\n4. Predict class labels on the test set.\n5. Print a detailed classification report using `sklearn.metrics.classification_report` to show precision, recall, and f1-score for each class.\n6. Plot the confusion matrix for the test set predictions using `sklearn.metrics.ConfusionMatrixDisplay.from_estimator`, ensuring appropriate labels and a title.",
  "2025-12-19": "1. Generate a synthetic regression dataset using `sklearn.datasets.make_regression` with 1000 samples, 6 informative features, and a small amount of noise. Convert the features (`X`) into a pandas DataFrame, assigning generic column names (e.g., `feature_0`, `feature_1`, ..., `feature_5`).\n2. **Feature Engineering**: Create a new interaction feature named `feature_0_x_feature_1` by multiplying `feature_0` and `feature_1`. Add this new feature to a *copy* of your DataFrame to create `X_with_interaction`. Keep the original DataFrame as `X_original`.\n3. Create two `sklearn.pipeline.Pipeline` objects, both consisting of `StandardScaler` followed by `LinearRegression`.\n    *   `pipeline_no_interaction`\n    *   `pipeline_with_interaction`\n4. Evaluate both pipelines using `sklearn.model_selection.cross_val_score` with 5-fold cross-validation and `neg_mean_squared_error` as the scoring metric:\n    *   Evaluate `pipeline_no_interaction` using `X_original` and the target `y`.\n    *   Evaluate `pipeline_with_interaction` using `X_with_interaction` and the target `y`.\n    Print the mean and standard deviation of the Mean Squared Error (MSE) for both, clearly labeling results.\n5. **Feature Importance Visualization**: Train `pipeline_with_interaction` on the *entire* `X_with_interaction` and `y` dataset. Extract the coefficients from the `LinearRegression` model within this trained pipeline. Create a bar plot using `matplotlib.pyplot` or `seaborn` showing the *absolute magnitude* of these coefficients. Map these to their corresponding feature names from `X_with_interaction`. Title the plot appropriately, e.g., 'Linear Regression Coefficients (Absolute Magnitude)'.",
  "2025-12-20": "1. Create an in-memory SQLite database using the `sqlite3` module.\n2. Create two tables:\n    *   `customers` with columns: `customer_id` (INTEGER PRIMARY KEY), `name` (TEXT), `region` (TEXT).\n    *   `orders` with columns: `order_id` (INTEGER PRIMARY KEY), `customer_id` (INTEGER, FOREIGN KEY), `order_date` (TEXT in 'YYYY-MM-DD' format), `total_amount` (REAL).\n3. Insert synthetic data into both tables. Ensure you have at least 5 distinct customers, 3 distinct regions, and 20-30 orders spanning a few months, with some customers having multiple orders.\n4. Write a single SQL query to find the `customer_id`, `name`, `region`, and `total_spent` (sum of all their `total_amount`s) for customers whose `total_spent` is greater than the `average_order_value_per_region` (the average `total_amount` of all orders originating from their specific `region`). Order the results by `total_spent` in descending order.\n5. Retrieve the results of this SQL query into a pandas DataFrame and display the head of the DataFrame.",
  "2025-12-21": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 5 informative features, and a significant class imbalance (e.g., `weights=[0.9, 0.1]` for 90% majority, 10% minority). Set `random_state` for reproducibility.\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Construct an `imblearn.pipeline.Pipeline` that first applies `StandardScaler` to the features, then applies `imblearn.over_sampling.SMOTE` (set `random_state`), and finally fits a `LogisticRegression` model (set `random_state`, `solver='liblinear'`).\n4. Train the pipeline on the *training data*.\n5. Evaluate the trained model's performance on the *test data*. Print the `sklearn.metrics.classification_report` to show precision, recall, and f1-score for each class, paying close attention to the minority class.\n6. Plot the Precision-Recall curve for the minority class on the test set using `sklearn.metrics.PrecisionRecallDisplay.from_estimator`, clearly labeling the plot with a title.",
  "2025-12-22": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 5 numerical features, and 1 conceptual 'high-cardinality' categorical feature. To create this categorical feature, generate a numerical feature with a large number of unique integer values (e.g., 50-100) and then convert it to string type, adding it to your feature DataFrame.\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `train_test_split`.\n3. Create two distinct `sklearn.pipeline.Pipeline` objects for preprocessing and modeling:\n    *   `pipeline_onehot_encoding`: Use `sklearn.compose.ColumnTransformer`. For the numerical features, apply `StandardScaler`. For the high-cardinality categorical feature, apply `OneHotEncoder(handle_unknown='ignore')`.\n    *   `pipeline_feature_hashing`: Use `sklearn.compose.ColumnTransformer`. For the numerical features, apply `StandardScaler`. For the high-cardinality categorical feature, apply `FeatureHasher(n_features=15, input_type='string')` (you may adjust `n_features`).\n4. Both pipelines should then fit a `LogisticRegression` model (using `solver='liblinear'` and a `random_state` for reproducibility).\n5. Train both pipelines on the training data and evaluate their performance on the test set. Report the `accuracy_score` and `f1_score` for each pipeline, clearly stating which encoding strategy yielded which result.\n6. For both pipelines, calculate probability predictions on the test set. Create two plots using `sklearn.calibration.CalibrationDisplay.from_estimator` (one for each pipeline) to visualize model calibration. Arrange them side-by-side or clearly distinguish them with titles indicating the encoding method. Discuss briefly which model appears better calibrated based on the plots.",
  "2025-12-23": "1. Generate a synthetic dataset of 500-1000 short text documents (e.g., short sentences or phrases) belonging to 3 distinct categories. Ensure some keywords are strongly associated with each category.\n2. Split the dataset into training and testing sets (e.g., 70/30 split).\n3. Construct an `sklearn.pipeline.Pipeline` that first applies `sklearn.feature_extraction.text.TfidfVectorizer` to convert text into numerical features, and then trains an `sklearn.linear_model.LogisticRegression` model (set `random_state` and `solver='liblinear'` for reproducibility).\n4. Train the pipeline on the training data and make predictions on the test data.\n5. Print the `sklearn.metrics.classification_report` for the test set predictions.\n6. From the *trained* pipeline, extract the `TfidfVectorizer` and `LogisticRegression` steps. Identify and print the top 5 most important features (words) for *each class* based on the `LogisticRegression` coefficients (e.g., highest positive coefficients for each class). Briefly interpret what these features tell you about each class.",
  "2025-12-24": "1. Generate a synthetic regression dataset: Create a pandas DataFrame `X` with 1000 samples and 3 numerical features (e.g., `feature_A`, `feature_B`, `feature_C`) using `np.random.rand()` for values between 0 and 1. Generate a target variable `y` such that `y = 2 * X['feature_A'] + 3 * (X['feature_B']**2) - X['feature_C'] + np.random.normal(0, 0.5, size=1000)`. \n2. Visualize feature-target relationships: Use `seaborn.jointplot` or `seaborn.pairplot` to explore the relationships between each feature in `X` and the target `y`. Pay close attention to any non-linear patterns that might be present.\n3. Engineer a new feature: Based on your visual inspection from step 2, identify the feature that appears to have a non-linear relationship with `y` and create a new feature by squaring that specific feature (e.g., if `feature_B` shows a quadratic relationship, create `feature_B_squared`). Add this new feature to a copy of your DataFrame, named `X_engineered`.\n4. Build and compare pipelines: Create two `sklearn.pipeline.Pipeline` objects:\n    *   `pipeline_original`: Apply `StandardScaler` to the original DataFrame `X` and then fit a `LinearRegression` model.\n    *   `pipeline_engineered`: Apply `StandardScaler` to the `X_engineered` DataFrame (which includes original features plus the new squared feature) and then fit a `LinearRegression` model.\n5. Evaluate performance: Use `sklearn.model_selection.cross_val_score` with 5-fold cross-validation and `neg_mean_squared_error` as the scoring metric for both pipelines. Print the mean and standard deviation of the Mean Squared Error (MSE) for each, clearly indicating the performance difference achieved by the engineered feature (remember to convert `neg_mean_squared_error` to positive MSE values).",
  "2025-12-25": "1. Generate a synthetic dataset using `sklearn.datasets.make_blobs` with at least 1000 samples, 5 features, and 4 distinct centers (clusters). Set `random_state=42`.\n2. Apply `sklearn.preprocessing.StandardScaler` to the generated features.\n3. Implement the Elbow method and Silhouette analysis: For a range of K values (e.g., from 2 to 8), train `sklearn.cluster.KMeans` (set `random_state=42`). For each K, record the `inertia_` and the `silhouette_score` (using the scaled data and the predicted cluster labels).\n4. Plot both the `inertia_` values (Elbow curve) and `silhouette_score` values against the corresponding K values. Clearly label the axes and provide descriptive titles for both plots.\n5. Based on your plots, identify the optimal number of clusters (K). Train a final `KMeans` model using this optimal K on the scaled data.\n6. Reduce the dimensionality of the *scaled* features to 2 principal components using `sklearn.decomposition.PCA`. Transform the original scaled data and the centroids of your final `KMeans` model into this 2D PCA space.\n7. Visualize the final clusters in the 2D PCA space. Plot the data points, coloring them according to their assigned cluster labels from the final `KMeans` model. Overlay the transformed cluster centroids on the plot. Ensure axes are labeled and the plot has a clear title.",
  "2025-12-26": "1. Generate a pandas DataFrame with 800 samples, including:\n    *   Three numerical features: `amount` (random positive floats, intentionally introduce some high outliers to simulate skewed transactions), `age` (random integers between 18-65), `duration_months` (random integers between 1-120).\n    *   One categorical feature: `region` (e.g., 'North', 'South', 'East', 'West' with varying proportions).\n    *   Ensure `amount` has a right-skewed distribution with some clear outliers (e.g., using `np.random.exponential` or adding a few large values).\n2. Calculate and display comprehensive descriptive statistics for the numerical features, grouped by the `region` categorical feature (e.g., mean, median, standard deviation, min, max, quartiles).\n3. Create a set of subplots (e.g., 1 row, 2 columns) to visualize the distribution of `amount` and `duration_months` across different `region` categories. Use `seaborn.boxplot` or `seaborn.violinplot` for these visualizations. Ensure plots have appropriate titles and labels.\n4. Focus on the `amount` feature. Apply a `log1p` transformation (i.e., `np.log1p(feature)`) to this feature to mitigate its skewness and outliers. Create another set of side-by-side subplots showing:\n    *   A histogram or Kernel Density Estimate (KDE) plot of the *original* `amount` distribution.\n    *   A histogram or KDE plot of the *log1p-transformed* `amount` distribution.\n    Clearly label titles to highlight the effect of the transformation.\n5. Compute the pairwise correlation matrix for all numerical features in the DataFrame (using the *log1p-transformed* `amount` for the correlation calculation). Visualize this matrix using a `seaborn.heatmap` with annotations, ensuring a clear title.",
  "2025-12-27": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 6 numerical features, and 2 classes (set `random_state` for reproducibility). Convert the features (`X`) and target (`y`) into a pandas DataFrame.\n2. Introduce missing values into the DataFrame:\n    *   For `feature_0`: Randomly replace approximately 15% of its values with `np.nan`.\n    *   For `feature_1`: Randomly replace approximately 10% of its values with `np.nan`.\n    *   For `feature_2`: Randomly replace approximately 5% of its values with `np.nan`.\n3. Create an `sklearn.pipeline.Pipeline` that first applies a `sklearn.compose.ColumnTransformer` for preprocessing and then fits a `sklearn.ensemble.RandomForestClassifier` (set `random_state` for reproducibility).\n    *   **Inside the `ColumnTransformer`**:\n        *   For `feature_0`: Apply `SimpleImputer(strategy='mean')` followed by `StandardScaler`.\n        *   For `feature_1`: Apply `KNeighborsImputer(n_neighbors=5)` followed by `StandardScaler`.\n        *   For `feature_2`: Apply `SimpleImputer(strategy='median')` followed by `StandardScaler`.\n        *   For the *remaining numerical features* (`feature_3` to `feature_5`): Apply `StandardScaler` directly (no imputation needed).\n4. Evaluate the complete pipeline's performance using 5-fold cross-validation (`sklearn.model_selection.cross_val_score`) with `accuracy` as the scoring metric.\n5. Report the mean accuracy and its standard deviation from the cross-validation.",
  "2025-12-28": "1. Generate a 2D NumPy array (e.g., 64x64 pixels) initialized with zeros, representing a black grayscale image.\n2. Add several synthetic 'objects' to this image using NumPy slicing and broadcasting:\n    *   A bright square in the center (e.g., value 200).\n    *   A horizontal bright line (e.g., value 150).\n    *   A diagonal bright line (e.g., value 100) from top-left to bottom-right.\n3. Implement a basic 2D convolution function `manual_convolve2d(image, kernel)` using NumPy operations (avoid `scipy.signal.convolve2d` for this task). Your function should handle padding or border effects as you deem appropriate (e.g., 'same' padding).\n4. Define a 3x3 'edge detection' kernel (e.g., a simple Sobel or Laplacian approximation).\n5. Apply your `manual_convolve2d` function with the edge detection kernel to your generated image.\n6. Visualize the original synthetic image and the convolved (edge-detected) image side-by-side using `matplotlib.pyplot.imshow`. Use a 'gray' colormap and ensure both plots have appropriate titles (e.g., 'Original Image', 'Edge Detected Image').",
  "2025-12-29": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 5 informative features, and a significant class imbalance (e.g., `weights=[0.9, 0.1]` for 90% majority, 10% minority). Set `random_state` for reproducibility.\n2. Split the dataset into training and testing sets (e.g., 70/30 split).\n3. Define a custom scoring function using `sklearn.metrics.make_scorer` that prioritizes the F1-score for the *minority class* (class 1). Hint: use `average='binary'` and `pos_label=1` for `f1_score`.\n4. Construct an `sklearn.pipeline.Pipeline` that first applies `StandardScaler` to the features and then fits a `LogisticRegression` model (set `random_state`, `solver='liblinear'`).\n5. Define a hyperparameter distribution for `RandomizedSearchCV` to tune `LogisticRegression`'s `C` parameter (e.g., `scipy.stats.loguniform(1e-3, 1e2)`). Explore at least 10 different parameter settings (`n_iter=10`).\n6. Perform `RandomizedSearchCV` with the pipeline, the defined parameter distributions, 3-fold cross-validation, and your *custom minority class F1-scorer*.\n7. Report the best `C` value found, the corresponding best cross-validation score, and print a full `classification_report` for the test set using the best estimator found by `RandomizedSearchCV`.",
  "2025-12-30": "1. Generate a synthetic regression dataset using `sklearn.datasets.make_regression` with 1000 samples, 3 informative features. Modify the target `y` to introduce a non-linear relationship (e.g., `y_original + 2 * X[:, 0]**2 + np.random.normal(0, 0.5, size=1000)`).\n2. Split the dataset into training and testing sets (e.g., 80/20 split) using `sklearn.model_selection.train_test_split`.\n3. Apply `sklearn.preprocessing.StandardScaler` to the features (`X`) on the training data and transform both training and testing sets.\n4. Build a simple sequential neural network model using `tensorflow.keras.models.Sequential`. The model should have:\n    *   An input layer matching the number of features.\n    *   At least one hidden `Dense` layer with `relu` activation (e.g., 32 units).\n    *   An output `Dense` layer with a single unit and linear activation.\n5. Compile the model using the `adam` optimizer and `mean_squared_error` loss.\n6. Train the model on the scaled training data for a suitable number of epochs (e.g., 50-100) and a batch size.\n7. Evaluate the trained model's performance on the scaled test set, reporting the Mean Squared Error (MSE).\n8. Visualize the model's predictions against the actual test target values using a scatter plot. Add a perfect prediction line (y=x) for comparison and label axes appropriately.",
  "2025-12-31": "1. Generate a synthetic transactional dataset using `pandas`:\n    *   Create a DataFrame named `transactions_df` with 800-1000 rows.\n    *   Columns should include:\n        *   `transaction_id` (unique integer IDs).\n        *   `customer_id` (e.g., 50-100 distinct customer IDs).\n        *   `transaction_date` (daily dates spanning 1-2 years, starting from '2022-01-01', ensuring multiple transactions per day/customer).\n        *   `amount` (random float values, e.g., between 10.0 and 500.0).\n        *   `product_category` (3-5 distinct string categories, e.g., 'Electronics', 'Books', 'Groceries', 'Clothing').\n    *   Sort the DataFrame by `customer_id` and then `transaction_date`.\n2. Create an in-memory SQLite database using `sqlite3` and load the `transactions_df` into a table named `transactions`.\n3. **SQL Analytics (Window Function 1 - Running Total)**: Write an SQL query to calculate the `running_total_amount` for each customer, ordered by their `transaction_date`. The query should return `transaction_id`, `customer_id`, `transaction_date`, `amount`, and the new `running_total_amount` column. Retrieve the results into a pandas DataFrame and display its head.\n4. **SQL Analytics (Window Function 2 - Rank within Group)**: Write a *separate* SQL query to rank transactions by `amount` in descending order *within each `product_category`*. The query should return `transaction_id`, `product_category`, `amount`, and the new `rank_in_category` column. Retrieve these results into another pandas DataFrame and display its head.\n5. Briefly describe what each window function achieves and how it's useful in analytical contexts.",
  "2026-01-01": "1. **Generate Synthetic Time Series Data**: Create a pandas DataFrame `df` with 800 daily entries. It should have a `date` column (daily dates starting from '2023-01-01') and a `sales_amount` column. Populate `sales_amount` with a synthetic time series, for example, a combination of a linear trend, a yearly seasonality (using `np.sin`), and some random noise (using `np.random.normal`).\n2. **Feature Engineering - Lag Features**: Create two new features in the DataFrame: `sales_amount_lag_1` (representing the sales from the previous day) and `sales_amount_lag_7` (representing sales from 7 days prior) using pandas' `shift()` method.\n3. **Feature Engineering - Rolling Statistics**: Create two more new features: `rolling_mean_3_days` (a 3-day rolling mean of `sales_amount`) and `rolling_std_7_days` (a 7-day rolling standard deviation of `sales_amount`) using pandas' `rolling()` method. Ensure you handle potential `NaN` values from rolling operations (e.g., `min_periods=1` if you want to keep early rows, or simply let `NaN`s propagate and drop later).\n4. **Handle Missing Values and Prepare for Modeling**: After creating all engineered features, drop any rows that contain `NaN` values (which will typically appear at the beginning of the DataFrame due to `shift` and `rolling` operations). Then, define the target variable `y` as the `sales_amount` column of the processed DataFrame, and the features `X` as all the engineered lag and rolling features. Display the head of the final `X` and `y` DataFrames to show the prepared dataset.\n5. **Visualize Feature-Target Relationship**: Pick one of the engineered features (e.g., `sales_amount_lag_1`) and plot its relationship with the `sales_amount` target using a `seaborn.lineplot` or `seaborn.scatterplot`. Ensure the plot has appropriate labels and a title.",
  "2026-01-02": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1500 samples, 8 features (e.g., `n_informative=6`), and 2 classes (set `random_state` for reproducibility).\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Create two distinct `sklearn.pipeline.Pipeline` objects for classification:\n    *   `pipeline_lr`: Consisting of `StandardScaler` followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'`).\n    *   `pipeline_gb`: Consisting of `StandardScaler` followed by `GradientBoostingClassifier` (set `random_state=42`).\n4. For each pipeline, define a small, focused hyperparameter grid for `sklearn.model_selection.GridSearchCV`:\n    *   For `pipeline_lr`: Tune `C` (e.g., `[0.1, 1, 10]`).\n    *   For `pipeline_gb`: Tune `n_estimators` (e.g., `[50, 100]`) and `learning_rate` (e.g., `[0.05, 0.1]`).\n5. Perform `GridSearchCV` *separately* for `pipeline_lr` and `pipeline_gb` on the training data. Use 3-fold cross-validation and `scoring='roc_auc'` for both. (Set `n_jobs=-1` for faster execution).\n6. Report the `best_params_` and `best_score_` (ROC AUC) for each model (Logistic Regression and Gradient Boosting Classifier).\n7. Using the *best estimators* obtained from `GridSearchCV` for both models, predict probabilities for the positive class (class 1) on the test set.\n8. Plot the Receiver Operating Characteristic (ROC) curve for *both* models on the *same* plot using `sklearn.metrics.RocCurveDisplay.from_estimator` (or `from_predictions` if preferred). Ensure the plot has a clear title, a legend indicating which curve belongs to which model, and displays the AUC score for each model.",
  "2026-01-03": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 4 informative features, and 2 classes (set `random_state` for reproducibility).\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Standardize the features using `sklearn.preprocessing.StandardScaler` on the training data, then transform both training and testing sets.\n4. Build a simple sequential neural network model using `tensorflow.keras.models.Sequential` for binary classification. The model should have:\n    *   An input layer matching the number of features.\n    *   One hidden `Dense` layer with `relu` activation (e.g., 16-32 units).\n    *   An output `Dense` layer with a single unit and `sigmoid` activation.\n5. Compile the model using the `adam` optimizer and `binary_crossentropy` loss.\n6. Train the model on the scaled training data for a suitable number of epochs (e.g., 30-50) and a batch size.\n7. Predict probabilities on the scaled test set. Convert these probabilities to binary class labels (0 or 1) using a threshold (e.g., 0.5).\n8. Print the `sklearn.metrics.classification_report` for the test set predictions.\n9. Generate and plot a confusion matrix using `sklearn.metrics.ConfusionMatrixDisplay.from_predictions` for the test set, clearly labeling the plot with a title (e.g., 'Confusion Matrix for Keras Binary Classifier').",
  "2026-01-04": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 5 informative features, and 2 classes (set `random_state` for reproducibility).\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Define a function `create_keras_model(optimizer='adam', units=32)` that builds and compiles a `tensorflow.keras.models.Sequential` model. This model should have an input layer matching the number of features, one `Dense` hidden layer with `relu` activation (using `units` as a parameter), and a `Dense` output layer with `sigmoid` activation. Compile it with `binary_crossentropy` loss.\n4. Wrap this Keras model using `tensorflow.keras.wrappers.scikit_learn.KerasClassifier`.\n5. Construct an `sklearn.pipeline.Pipeline` that first applies `sklearn.preprocessing.StandardScaler` and then uses the wrapped `KerasClassifier`.\n6. Define a hyperparameter grid for `sklearn.model_selection.GridSearchCV` to tune the following parameters of the Keras model within the pipeline:\n    *   `kerasclassifier__batch_size` (e.g., `[32, 64]`)\n    *   `kerasclassifier__epochs` (e.g., `[10, 20]`)\n    *   `kerasclassifier__model__units` (e.g., `[16, 32]`)\n    *   `kerasclassifier__optimizer` (e.g., `['adam', 'rmsprop']`)\n7. Perform `GridSearchCV` with 3-fold cross-validation and `scoring='roc_auc'` on the training data. (Set `n_jobs=-1` for faster execution).\n8. Report the `best_params_` and `best_score_` from `GridSearchCV`. Then, using the `best_estimator_`, predict class labels on the test set and print the `sklearn.metrics.classification_report`.",
  "2026-01-05": "1. **Generate Synthetic Time Series Data**: Create a pandas DataFrame `df` with 500-700 daily entries. It should have a `date` column (daily dates starting from '2022-01-01') and a `value` column. Populate `value` with a synthetic time series exhibiting a linear trend, strong weekly seasonality (e.g., higher values on weekends), and some random noise.\n2. **Feature Engineering - Advanced Time-Based Features**: Create the following new features in the DataFrame:\n    *   `day_of_week`: Extract the day of the week (0-6 or names) from the `date` column.\n    *   `is_weekend`: A binary flag (1 if weekend, 0 otherwise).\n    *   `exponential_moving_average_7d`: Calculate a 7-day Exponential Moving Average (EMA) of the `value` column using `df['value'].ewm(span=7, adjust=False).mean()`.\n3. **Handle Missing Values and Prepare for Modeling**: Drop any rows that contain `NaN` values (due to EMA calculation). Define the target variable `y` as the `value` column, and features `X` as `day_of_week`, `is_weekend`, and `exponential_moving_average_7d`. Convert `day_of_week` into one-hot encoded features using `pd.get_dummies()`.\n4. **Visualize Engineered Features**: Create a plot (e.g., using `seaborn.lineplot` or `seaborn.boxplot`) to visualize the relationship between `day_of_week` (or `is_weekend`) and the original `value` to confirm the seasonality. Also, plot the original `value` and the `exponential_moving_average_7d` on the same time-series plot to show the smoothing effect.\n5. **Build and Evaluate a Regression Model**: Split the dataset into training and testing sets (e.g., 80/20 split). Train a `LinearRegression` model using the engineered features. Evaluate its performance on the test set using `sklearn.metrics.mean_absolute_error` and `sklearn.metrics.r2_score`. Report both metrics.",
  "2026-01-06": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 6 informative features, and 2 classes (set `random_state` for reproducibility). Convert the features (`X`) and target (`y`) into a pandas DataFrame.\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Create two distinct `sklearn.pipeline.Pipeline` objects for classification:\n    *   `pipeline_baseline`: Consisting of `StandardScaler` followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   `pipeline_polynomial`: Consisting of `StandardScaler`, then `PolynomialFeatures(degree=2, include_bias=False)`, followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n4. Evaluate both pipelines using 5-fold cross-validation (`sklearn.model_selection.cross_val_score`) on the training data. Use `scoring='roc_auc'` for performance comparison.\n5. Report the mean and standard deviation of the ROC AUC scores for both the `pipeline_baseline` and `pipeline_polynomial`. Based on these results, briefly discuss the impact of including polynomial features on model performance for this dataset.",
  "2026-01-07": "1. **Generate Synthetic Data**: Create two pandas DataFrames:\n    *   `customers_df`: With 100-150 rows. Columns: `customer_id` (unique integers), `customer_name` (e.g., 'Customer A'), `registration_date` (random dates over the last 3 years).\n    *   `orders_df`: With 800-1000 rows. Columns: `order_id` (unique integers), `customer_id` (randomly sampled from `customers_df` IDs, ensuring some customers have no orders and others have many), `order_date` (random dates after their `registration_date`), `amount` (random floats between 10 and 1000).\n2. **Load into SQLite**: Create an in-memory SQLite database using `sqlite3` and load both `customers_df` and `orders_df` into tables named `customers` and `orders` respectively.\n3. **SQL Query - Customer Sales Aggregation and Segmentation**: Write a single SQL query that performs the following:\n    *   **Joins** the `customers` and `orders` tables.\n    *   **Aggregates** to calculate the `total_sales_amount` and `number_of_orders` for each customer. Ensure that customers with no orders are still included in the result, showing 0 for sales and orders.\n    *   **Categorizes** each customer into a `customer_segment` based on their `total_sales_amount`:\n        *   'High-Value' if `total_sales_amount` > 5000\n        *   'Medium-Value' if `total_sales_amount` between 1000 and 5000 (inclusive)\n        *   'Low-Value' if `total_sales_amount` between 1 and 999 (inclusive)\n        *   'No-Orders' if `total_sales_amount` is 0 or NULL.\n    *   The query should return `customer_id`, `customer_name`, `registration_date`, `total_sales_amount`, `number_of_orders`, and `customer_segment`.\n4. **Retrieve and Display**: Fetch the results into a pandas DataFrame. Display its head and then print the count of customers within each `customer_segment` to verify your segmentation logic.",
  "2026-01-08": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with 1000 samples, 4 numerical features, and 2 classes (`random_state=42`). Convert `X` into a pandas DataFrame and add a categorical feature named `color` (e.g., 'red', 'blue', 'green', 'yellow' with random distribution, ensuring a good mix).\n2. Introduce missing values into the `color` feature by randomly replacing approximately 15% of its values with `np.nan`.\n3. Create an `sklearn.pipeline.Pipeline` that first applies a `sklearn.compose.ColumnTransformer` for preprocessing and then fits a `sklearn.linear_model.LogisticRegression` model (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   **Inside the `ColumnTransformer`**:\n        *   For the numerical features: Apply `SimpleImputer(strategy='mean')` followed by `StandardScaler`.\n        *   For the `color` categorical feature: Apply `SimpleImputer(strategy='most_frequent')` followed by `OneHotEncoder(handle_unknown='ignore')`.\n4. Evaluate the complete pipeline's performance using 5-fold cross-validation (`sklearn.model_selection.cross_val_score`) with `accuracy` as the scoring metric.\n5. Report the mean accuracy and its standard deviation from the cross-validation.",
  "2026-01-09": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with at least 1000 samples, 10 features (e.g., `n_informative=5`, `n_redundant=3`, `n_repeated=2`), and 2 classes (set `random_state=42`).\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Create two distinct `sklearn.pipeline.Pipeline` objects:\n    *   `pipeline_no_fs`: Consisting of `StandardScaler` followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   `pipeline_with_fs`: Consisting of `StandardScaler`, then `sklearn.feature_selection.SelectKBest` (e.g., select `k=5` features using `score_func=sklearn.feature_selection.f_classif`), followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n4. Train both `pipeline_no_fs` and `pipeline_with_fs` on the training data.\n5. Predict probabilities for the positive class (class 1) on the test set using both trained pipelines.\n6. Calculate and report the `sklearn.metrics.roc_auc_score` for both models on the test set.\n7. Briefly discuss the impact of including the feature selection step on the model's performance for this dataset, based on the reported ROC AUC scores.",
  "2026-01-10": "1. Generate a synthetic transactional pandas DataFrame (`transactions_df`) with 1200 rows. Columns should include:\n    *   `transaction_id` (unique integers starting from 10000)\n    *   `customer_id` (e.g., 100-150 distinct customer IDs, formatted as 'C' followed by a number)\n    *   `transaction_date` (daily dates spanning 3 years, starting from '2021-01-01', ensuring multiple transactions per customer over time)\n    *   `product_category` (e.g., 5 distinct string categories: 'Electronics', 'Books', 'Groceries', 'Clothing', 'Home Goods' with varying proportions)\n    *   `amount` (random float values between 20.0 and 1000.0)\n    *   Ensure the DataFrame is sorted by `customer_id` and then `transaction_date`.\n2. Calculate the total revenue generated by each unique `customer_id`. Display the top 5 customers with the highest total revenue (customer ID and their total revenue).\n3. Create a pivot table named `monthly_category_sales` that shows the total `amount` for each `product_category` (as columns) for each month and year combination (as index). Fill any missing values in the pivot table with 0. Display the head of this pivot table.\n4. For each `customer_id`, calculate their 3-month rolling average `amount` (mean of `amount`) based on `transaction_date`. Store this as a new column `rolling_avg_3m_spend` in the original `transactions_df`. Ensure the rolling average is calculated *within each customer group* and correctly considers the time ordering. Display the head of the `transactions_df` including this new column.",
  "2026-01-11": "1. Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification` with 1000 samples, 10 informative features, and 2 classes (set `random_state=42`).\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Create two distinct `sklearn.pipeline.Pipeline` objects for classification:\n    *   `pipeline_no_pca`: Consisting of `StandardScaler` followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   `pipeline_with_pca`: Consisting of `StandardScaler`, then `sklearn.decomposition.PCA(n_components=2)`, followed by `LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n4. Train both `pipeline_no_pca` and `pipeline_with_pca` on the training data.\n5. Predict probabilities for the positive class (class 1) on the test set using both trained pipelines. Calculate and report the `sklearn.metrics.roc_auc_score` for both models on the test set.\n6. **Visualize the transformed data**: Apply the `StandardScaler` and `PCA(n_components=2)` steps from `pipeline_with_pca` (using `.fit_transform()` on training and `.transform()` on testing data) to the *test set features*. Create a scatter plot of the two principal components, coloring the points by their actual class labels (`y_test`). Add appropriate titles and labels.\n7. Briefly discuss the impact of including PCA on the model's performance for this dataset and what the visualization reveals about the data separation.",
  "2026-01-12": "1. Generate a synthetic 2D dataset for clustering using `sklearn.datasets.make_blobs` with 1000 samples, 2 features, and 4 cluster centers. Set `random_state=42` for reproducibility.\n2. Apply `sklearn.preprocessing.StandardScaler` to the generated features.\n3. Perform K-Means clustering on the scaled data. Initialize `sklearn.cluster.KMeans` with `n_clusters=4` and `random_state=42` (set `n_init='auto'` or `n_init=10` to suppress warnings for older scikit-learn versions). Fit the model and obtain the cluster labels.\n4. Calculate and report the `sklearn.metrics.silhouette_score` using the scaled features and the obtained cluster labels.\n5. Create a scatter plot of the 2D features (either original or scaled), coloring each data point according to its assigned K-Means cluster. Ensure the plot has appropriate axis labels, a clear title (e.g., 'K-Means Clusters (Silhouette Score: X.XX)'), and a legend (if distinct colors are used).\n6. Briefly explain what the Silhouette Score measures and why it's a useful metric for evaluating clustering results, especially when true labels are not available.",
  "2026-01-13": "1. **Generate Synthetic Time Series Data**: Create a pandas DataFrame `df` with 700-900 daily entries. It should have a `date` column (daily dates starting from '2022-01-01') and a `sales` column. Populate `sales` with a synthetic time series exhibiting a clear linear trend, a strong monthly seasonality (e.g., using `np.sin` or `np.cos` with a 30-day period), and some random noise.\n2. **Feature Engineering for Forecasting**: Create the following new features in the DataFrame:\n    *   `day_of_week`: Integer representing the day of the week (0=Monday, 6=Sunday).\n    *   `month`: Integer representing the month.\n    *   `day_of_year`: Integer representing the day of the year.\n    *   `sales_lag_7`: The `sales` value from 7 days prior.\n3. **Prepare Data and Chronological Split**: Drop any rows that contain `NaN` values resulting from lag feature creation. Define features `X` (all engineered features) and target `y` (`sales`). Then, split the data into training and testing sets *chronologically*, using the last 90 days of data for the test set. Ensure `X_train`, `X_test`, `y_train`, `y_test` are correctly defined.\n4. **Train and Evaluate a Regression Model**: Train an `sklearn.ensemble.RandomForestRegressor` (set `random_state=42`, `n_estimators=100`) on the training data. Predict `sales` for the test set. Calculate and report the `sklearn.metrics.mean_absolute_error` (MAE) and `sklearn.metrics.r2_score` of the model on the test set.\n5. **Visualize the Forecast**: Create a single line plot showing the actual `sales` values for the test period and the model's predicted `sales` values for the same period. Label the axes, add a title like 'Sales Forecast vs. Actuals', and include a legend to distinguish between actual and predicted lines.",
  "2026-01-14": "1. Generate a synthetic imbalanced binary classification dataset using `sklearn.datasets.make_classification` with 1200 samples, 8 features, 2 classes, and `weights=[0.9, 0.1]` to create an imbalance. Set `random_state=42` for reproducibility.\n2. Split the dataset into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n3. Create an `imblearn.pipeline.Pipeline` that includes the following steps:\n    *   `sklearn.preprocessing.StandardScaler`\n    *   `imblearn.over_sampling.SMOTE` (set `random_state=42`)\n    *   `sklearn.linear_model.LogisticRegression` (set `random_state=42`, `solver='liblinear'`)\n4. Train the pipeline on the training data.\n5. Predict probabilities for the positive class (class 1) on the scaled test set. Then, convert these probabilities to binary class labels (0 or 1) using a threshold of 0.5.\n6. Calculate and print the `sklearn.metrics.classification_report` for the test set predictions.\n7. Plot the Precision-Recall curve for the model on the test set using `sklearn.metrics.PrecisionRecallDisplay.from_predictions`. Ensure the plot has a clear title, labels, and displays the average precision score.",
  "2026-01-15": "1. **Generate Synthetic Transactional Data**: Create a pandas DataFrame named `transactions_df` with 1000-1200 rows. Columns should include:\n    *   `transaction_id` (unique integer IDs starting from 10000).\n    *   `customer_id` (e.g., 100-150 distinct customer IDs, formatted as 'C' followed by a number).\n    *   `transaction_date` (daily dates spanning 3 years, starting from '2021-01-01', ensuring multiple transactions per customer over time).\n    *   `amount` (random float values, e.g., between 20.0 and 1000.0).\n    *   Ensure the DataFrame is sorted by `customer_id` and then `transaction_date`.\n2. **Load into SQLite**: Create an in-memory SQLite database using `sqlite3` and load the `transactions_df` into a table named `transactions`.\n3. **SQL Analytics (Advanced Window Functions - LAG/LEAD and Date Difference)**: Write a single SQL query that performs the following for *each customer*, ordered by their `transaction_date`:\n    *   Calculates `previous_transaction_amount`: The `amount` of the immediately preceding transaction.\n    *   Calculates `next_transaction_amount`: The `amount` of the immediately following transaction.\n    *   Calculates `days_since_prev_transaction`: The number of days between the current `transaction_date` and the `transaction_date` of the immediately preceding transaction. If it's the first transaction for a customer, this should be `NULL`.\n    *   The query should return `transaction_id`, `customer_id`, `transaction_date`, `amount`, `previous_transaction_amount`, `next_transaction_amount`, and `days_since_prev_transaction`.\n4. **Retrieve and Display**: Fetch the results into a pandas DataFrame. Display its head and briefly describe what the `LAG` and `LEAD` functions achieve in this context.",
  "2026-01-16": "1. **Generate Synthetic Data**: Create a pandas DataFrame `X_df` with 1000 samples for a binary classification problem using `sklearn.datasets.make_classification` (e.g., 5 informative features, `random_state=42`). Let `y` be the target variable. Add two new columns to `X_df`:\n    *   `product_code`: A categorical string feature with variations like 'TYPE-A_v1', 'type-B-v2', 'TYPE-A v3', 'TYPE_C_v1', 'Type-A'. Ensure a mix of 3-4 distinct 'types' (A, B, C, D) with messy suffixes/formats and inconsistent casing.\n    *   `description`: A free-text string column. Populate it with short descriptions that sometimes include keywords like 'urgent', 'fragile', 'standard' (e.g., 'Product with urgent delivery', 'Standard item', 'Fragile glass product').\n2. **Feature Engineering - String and Text Processing**: Create new features in `X_df` (or a copy) based on the new string columns:\n    *   `product_type_clean`: Extract the clean 'TYPE-X' part from `product_code` (e.g., 'TYPE-A', 'TYPE-B', 'TYPE-C') and convert it to consistent uppercase. (Hint: Use regex to extract 'TYPE-letter').\n    *   `is_urgent`: A binary (0 or 1) feature, 1 if 'urgent' is found in `description` (case-insensitive), 0 otherwise.\n3. **ML Pipeline with ColumnTransformer**: Split the processed `X_df` (including your new engineered features) and `y` into training and testing sets (e.g., 70/30 split) using `train_test_split`.\n    Create an `sklearn.pipeline.Pipeline` that uses `sklearn.compose.ColumnTransformer` for preprocessing:\n    *   For the original numerical features (from `make_classification`): Apply `StandardScaler`.\n    *   For `product_type_clean`: Apply `OneHotEncoder(handle_unknown='ignore')`.\n    *   For `is_urgent`: Pass through (no transformation).\n    *   Follow the `ColumnTransformer` with `sklearn.linear_model.LogisticRegression` (set `random_state=42`, `solver='liblinear'`).\n4. **Evaluate Performance**: Evaluate the complete pipeline's performance using 5-fold cross-validation (`sklearn.model_selection.cross_val_score`) on the *training data*. Use `scoring='roc_auc'` as the metric. Report the mean and standard deviation of the ROC AUC scores.",
  "2026-01-17": "1. **Generate Synthetic Transactional Data**: Create a pandas DataFrame `transactions_df` with 800-1000 rows. Columns should include:\n    *   `transaction_id` (unique integers)\n    *   `transaction_date` (daily dates spanning 2-3 years, starting from '2022-01-01')\n    *   `product_category` (e.g., 4-5 distinct string categories like 'Electronics', 'Books', 'Groceries', 'Clothing', but with inconsistent casing and minor variations, e.g., 'electronics', 'Book', 'groceries item', 'clothes').\n    *   `description` (short text descriptions that sometimes contain keywords like 'discount', 'premium', 'sale').\n    *   `amount` (random float values between 10.0 and 500.0).\n    Ensure the DataFrame is sorted by `product_category` and then `transaction_date`.\n2. **Advanced Feature Engineering**: \n    *   **Clean Categorical**: Create a new column `clean_category` by standardizing the `product_category` names (e.g., 'electronics', 'Electronics', 'ELEC' all map to 'Electronics').\n    *   **Datetime Features**: Extract `month` and `day_of_week` (integer 0-6) from `transaction_date`.\n    *   **Text Feature**: Create `is_discount` (binary: 1 if 'discount' or 'sale' is found in `description` case-insensitively, 0 otherwise).\n    *   **Grouped Lag Feature**: For each `clean_category`, calculate `lagged_amount_1d`, which is the `amount` from the previous day for that *specific category*. Fill `NaN` values (e.g., with 0, or by propagating the last valid observation forward and then filling remaining with 0 if needed for initial values).\n3. **Chronological Data Split**: Define features `X` (all engineered features created in step 2) and target `y` (the original `amount` column). Split the data chronologically, using the last 60 days of data for the test set. Ensure `X_train`, `X_test`, `y_train`, `y_test` are correctly defined.\n4. **ML Pipeline with ColumnTransformer**: Create an `sklearn.pipeline.Pipeline` that uses `sklearn.compose.ColumnTransformer` for preprocessing and `sklearn.ensemble.RandomForestRegressor` as the final estimator (set `random_state=42`, `n_estimators=100`).\n    *   **Inside the `ColumnTransformer`**:\n        *   For numerical features (`lagged_amount_1d`): Apply `SimpleImputer(strategy='mean')` followed by `StandardScaler`.\n        *   For categorical features (`clean_category`, `month`, `day_of_week`): Apply `OneHotEncoder(handle_unknown='ignore')`.\n        *   For the binary feature (`is_discount`): Use `Passthrough`.\n5. **Train, Predict, and Evaluate**: Train the pipeline on the training data (`X_train`, `y_train`). Predict `amount` for the test set (`X_test`). Calculate and report the `sklearn.metrics.mean_absolute_error` (MAE) and `sklearn.metrics.r2_score`.\n6. **Visualize Forecast**: Create a single line plot showing the actual `amount` values for the test period and the model's predicted `amount` values for the same period. Label the axes, add a title like 'Actual vs. Predicted Amounts for Test Set', and include a legend.",
  "2026-01-18": "1. **Generate Synthetic Data (pandas/numpy)**: Create two pandas DataFrames:\n    *   `customer_profiles_df`: With 500 rows. Columns: `customer_id` (unique integers), `age` (random ints 18-70), `income` (random floats 20000-150000), `is_churn` (binary target, 0 or 1, with a slight imbalance, e.g., 80/20 split).\n    *   `transactions_df`: With 2000-3000 rows. Columns: `transaction_id` (unique integers), `customer_id` (randomly sampled from `customer_profiles_df` IDs, ensuring some customers have no transactions and others have many), `transaction_date` (random dates over the last 3 years), `amount` (random floats between 10.0 and 1000.0).\n2. **Load into SQLite**: Create an in-memory SQLite database using `sqlite3` and load `customer_profiles_df` into a table named `customers` and `transactions_df` into `transactions`.\n3. **SQL Feature Engineering**: Write a single SQL query that performs the following:\n    *   **Joins** the `customers` and `transactions` tables.\n    *   **Aggregates** to calculate `total_spend`, `avg_spend_per_transaction`, and `num_transactions` for each customer.\n    *   **Ensures** that customers with no transactions are still included in the result, showing 0 for their aggregated spend/transaction counts.\n    *   Returns `customer_id`, `total_spend`, `avg_spend_per_transaction`, `num_transactions`.\n4. **Retrieve and Merge (pandas)**: Fetch the results of the SQL query into a pandas DataFrame. Then, merge this aggregated DataFrame with the original `customer_profiles_df` on `customer_id`.\n5. **Data Visualization**: Create a histogram or kernel density plot of `total_spend`, differentiated by `is_churn` (e.g., using `hue` in seaborn or `plt.hist` with different colors) to visually inspect the relationship.\n6. **ML Pipeline & Evaluation**: \n    *   Define features `X` (`age`, `income`, `total_spend`, `avg_spend_per_transaction`, `num_transactions`) and target `y` (`is_churn`) from the merged DataFrame.\n    *   Split the data into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n    *   Create an `sklearn.pipeline.Pipeline` consisting of `sklearn.preprocessing.StandardScaler` followed by `sklearn.linear_model.LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   Train the pipeline on the training data. Predict probabilities for the positive class (class 1) on the test set.\n    *   Calculate and print the `sklearn.metrics.roc_auc_score` for the test set predictions.\n    *   Generate and display an ROC curve for the model using `sklearn.metrics.RocCurveDisplay.from_estimator` with the trained pipeline and test data.",
  "2026-01-19": "1. **Generate Synthetic Data**: Create two pandas DataFrames:\n    *   `users_df`: With 500 rows. Columns: `user_id` (unique integers), `signup_date` (random dates over the last 3 years), `region` (e.g., 'North', 'South', 'East', 'West' with random distribution), `plan_type` (e.g., 'Basic', 'Premium', 'Pro' with random distribution), `churn` (binary target, 0 or 1, with an approximate 20% churn rate).\n    *   `events_df`: With 3000-5000 rows. Columns: `event_id` (unique integers), `user_id` (randomly sampled from `users_df` IDs, ensuring some users have many events and a few have no events), `event_date` (random dates occurring *after* their respective `signup_date`), `event_type` (e.g., 'login', 'view_item', 'purchase', 'cancel_subscription' with varying frequencies).\n2. **Load into SQLite**: Create an in-memory SQLite database using `sqlite3` and load `users_df` into a table named `users` and `events_df` into `events`.\n3. **SQL Feature Engineering (User Activity Aggregation)**: First, determine the `analysis_end_date` by finding the maximum `event_date` in your `events_df` (using pandas). Then, write a single SQL query that performs the following:\n    *   **Joins** `users` and `events` tables.\n    *   **Aggregates** user-level features: `total_events` (count of all events), `days_since_last_event` (number of days between the `analysis_end_date` and the user's latest `event_date`), `num_logins` (count of 'login' events), `num_purchases` (count of 'purchase' events).\n    *   **Ensures** that all users are included, even those with no events, showing appropriate default values (e.g., 0 for counts, `NULL` for `days_since_last_event` if no events).\n    *   The query should return `user_id`, `total_events`, `days_since_last_event`, `num_logins`, `num_purchases`.\n4. **Retrieve, Merge, and Final Data Prep (Pandas)**:\n    *   Fetch the SQL query results into a pandas DataFrame.\n    *   Merge this aggregated DataFrame with the original `users_df` on `user_id`.\n    *   Handle `NaN` values resulting from the SQL query: Fill `total_events`, `num_logins`, `num_purchases` with 0 for users with no events. For `days_since_last_event` (for users with no events), fill with a large sentinel value (e.g., `365 * 5` or 1825 to represent 5 years of inactivity).\n    *   Create a new feature `account_age_days`: Calculate the number of days between `signup_date` and the `analysis_end_date` (from step 3).\n    *   Define features `X` (all numerical + `region`, `plan_type`) and target `y` (`churn`). Split into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split`.\n5. **Data Visualization**: Create two separate violin plots (or box plots) to visualize the relationship between key engineered features and churn:\n    *   `days_since_last_event` vs `churn`.\n    *   `num_purchases` vs `churn`.\n    Ensure plots have appropriate labels and titles.\n6. **ML Pipeline & Evaluation**: \n    *   Create an `sklearn.pipeline.Pipeline` with a `ColumnTransformer` for preprocessing:\n        *   For numerical features (e.g., `total_events`, `days_since_last_event`, `num_logins`, `num_purchases`, `account_age_days`): Apply `SimpleImputer(strategy='mean')` followed by `StandardScaler`.\n        *   For categorical features (`region`, `plan_type`): Apply `OneHotEncoder(handle_unknown='ignore')`.\n    *   The final estimator in the pipeline should be `sklearn.linear_model.LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   Train the pipeline on the training data. Predict on the test set.\n    *   Calculate and print the `sklearn.metrics.roc_auc_score` and `sklearn.metrics.accuracy_score` for the test set predictions.",
  "2026-01-20": "1. **Generate Synthetic Data**: Create three pandas DataFrames:\n    *   `products_df`: 100-150 rows. Columns: `product_id` (unique int), `category` (e.g., 'Electronics', 'books', 'Home Goods', 'CLOTHING', 'elec' with inconsistent casing/formats), `base_price` (random floats 50-500).\n    *   `orders_df`: 800-1000 rows. Columns: `order_id` (unique int), `product_id` (randomly sampled from `products_df` IDs), `order_date` (random dates over last 2 years), `quantity` (random int 1-5).\n    *   `reviews_df`: 700-900 rows. Columns: `review_id` (unique int), `order_id` (randomly sampled from `orders_df` IDs, ensuring some orders have no review and some products have multiple reviews), `rating` (random int 1-5, this will be our target), `review_text` (short text containing keywords like 'good', 'excellent', 'fast', 'slow', 'defective', 'broken' randomly, mixed with generic words).\n\n2. **Load into SQLite & SQL Analytics**: Create an in-memory SQLite database and load `products_df`, `orders_df`, and `reviews_df` into tables. Write a single SQL query that joins these three tables. For each review, calculate the `line_item_total` (`orders.quantity * products.base_price`). The query should return `review_id`, `rating`, `review_text`, `order_date`, `category`, and `line_item_total`.\n\n3. **Feature Engineering (Pandas)**: Fetch the SQL query results into a pandas DataFrame. Create the following new features:\n    *   `clean_category`: Standardize `category` names (e.g., 'Electronics', 'Books', 'Home Goods', 'Clothing') by converting to title case and handling variations (e.g., 'elec' -> 'Electronics'). (Hint: Use string methods like `.str.title()` and `.str.replace()` or a custom mapping function).\n    *   `has_positive_feedback`: Binary (1 if 'good' or 'excellent' (case-insensitive) is found in `review_text`, 0 otherwise).\n    *   `has_negative_feedback`: Binary (1 if 'slow', 'defective', or 'broken' (case-insensitive) is found in `review_text`, 0 otherwise).\n    *   `days_since_order`: Calculate the number of days between `order_date` and a fixed `analysis_date` (e.g., `max(order_date)` + 30 days from your generated data, after converting `order_date` to datetime objects).\n\n4. **Data Visualization**: Create two visualizations:\n    *   A histogram showing the distribution of the `rating` column.\n    *   A box plot or violin plot showing the distribution of `rating` for each `clean_category`.\n\n5. **ML Pipeline & Evaluation**: \n    *   Define features `X` (`line_item_total`, `days_since_order`, `clean_category`, `has_positive_feedback`, `has_negative_feedback`) and target `y` (`rating`).\n    *   Split the data into training and testing sets (e.g., 80/20 split) using `sklearn.model_selection.train_test_split` (set `random_state=42`).\n    *   Create an `sklearn.pipeline.Pipeline` with a `ColumnTransformer` for preprocessing:\n        *   Numerical features (`line_item_total`, `days_since_order`): Apply `SimpleImputer(strategy='mean')` followed by `StandardScaler`.\n        *   Categorical features (`clean_category`): Apply `OneHotEncoder(handle_unknown='ignore')`.\n        *   Binary features (`has_positive_feedback`, `has_negative_feedback`): Use `Passthrough`.\n    *   The final estimator in the pipeline should be `sklearn.ensemble.RandomForestRegressor` (set `random_state=42`, `n_estimators=100`).\n    *   Train the pipeline on the training data. Predict `rating` for the test set.\n    *   Calculate and print the `sklearn.metrics.mean_absolute_error` (MAE) and `sklearn.metrics.r2_score` for the test set predictions.",
  "2026-01-21": "1. **Generate Synthetic Data (Pandas/Numpy)**: Create two pandas DataFrames:\n    *   `customers_df`: With 500 rows. Columns: `customer_id` (unique integers), `signup_date` (random dates over the last 5 years), `region` (e.g., 'North', 'South', 'East', 'West' with random distribution), `age` (random integers 18-70).\n    *   `transactions_df`: With 2000-3000 rows. Columns: `transaction_id` (unique integers), `customer_id` (randomly sampled from `customers_df` IDs, ensuring some customers have no transactions and others have many), `transaction_date` (random dates after their `signup_date`), `amount` (random floats between 10.0 and 1000.0).\n\n2. **Load into SQLite**: Create an in-memory SQLite database using `sqlite3` and load `customers_df` into a table named `customers` and `transactions_df` into `transactions`.\n\n3. **SQL Feature Engineering (Customer Value)**: First, determine an `analysis_date` (e.g., the maximum `transaction_date` in `transactions_df` plus 30 days, using pandas). Then, write a single SQL query that performs the following for *each customer*:\n    *   **Joins** `customers` and `transactions` tables.\n    *   **Aggregates** to calculate `total_spend`, `num_transactions`, `avg_transaction_value` (total_spend / num_transactions).\n    *   Calculates `days_since_last_purchase`: The number of days between the `analysis_date` and the customer's `MAX(transaction_date)`.\n    *   Calculates `days_since_first_purchase`: The number of days between the `analysis_date` and the customer's `MIN(transaction_date)`.\n    *   **Ensures** that all customers are included in the result, even those with no transactions, showing 0 or NULL for aggregated values as appropriate.\n    *   The query should return `customer_id`, `region`, `age`, `signup_date`, `total_spend`, `num_transactions`, `avg_transaction_value`, `days_since_last_purchase`, `days_since_first_purchase`.\n\n4. **Retrieve, Merge, and Pandas Feature Engineering**: \n    *   Fetch the SQL query results into a pandas DataFrame.\n    *   Calculate `account_age_days`: The number of days between `signup_date` and the `analysis_date` (from step 3).\n    *   Handle `NaN` values resulting from customers with no transactions:\n        *   Fill `total_spend`, `num_transactions`, `avg_transaction_value` with 0.\n        *   For `days_since_last_purchase` and `days_since_first_purchase` (for customers with no transactions), fill with a large sentinel value, e.g., `account_age_days` + 30 days (or 5 years in days).\n    *   Define features `X` (`age`, `region`, `account_age_days`, `total_spend`, `num_transactions`, `avg_transaction_value`, `days_since_last_purchase`, `days_since_first_purchase`) and target `y` (`total_spend`).\n    *   Split the data into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split` (set `random_state=42`).\n\n5. **Data Visualization**: Create two visualizations:\n    *   A histogram showing the distribution of the target variable `total_spend`.\n    *   A box plot or violin plot showing the distribution of `total_spend` for each `region`.\n\n6. **ML Pipeline & Evaluation**: \n    *   Create an `sklearn.pipeline.Pipeline` with a `ColumnTransformer` for preprocessing:\n        *   For numerical features (e.g., `age`, `account_age_days`, `total_spend`, `num_transactions`, `avg_transaction_value`, `days_since_last_purchase`, `days_since_first_purchase`): Apply `StandardScaler`.\n        *   For categorical features (`region`): Apply `OneHotEncoder(handle_unknown='ignore')`.\n    *   The final estimator in the pipeline should be `sklearn.ensemble.RandomForestRegressor` (set `random_state=42`, `n_estimators=100`).\n    *   Train the pipeline on the training data (`X_train`, `y_train`). Predict `total_spend` for the test set (`X_test`).\n    *   Calculate and print the `sklearn.metrics.mean_absolute_error` (MAE) and `sklearn.metrics.r2_score` for the test set predictions.",
  "2026-01-22": "1. **Generate Synthetic Data (Pandas/Numpy)**: Create two pandas DataFrames:\n    *   `users_df`: With 500 rows. Columns: `user_id` (unique integers), `signup_date` (random dates over the last 3 years), `country` (e.g., 'USA', 'Canada', 'UK', 'Australia' with random distribution), `device_type` (e.g., 'Mobile', 'Web', 'Desktop'), `engagement_level` (binary target, 0 or 1, with an approximate 70/30 split for class 0/1).\n    *   `activities_df`: With 3000-5000 rows. Columns: `activity_id` (unique integers), `user_id` (randomly sampled from `users_df` IDs, ensuring some users have many activities and a few have no activities), `activity_date` (random dates occurring *after* their respective `signup_date`), `activity_type` (e.g., 'login', 'logout', 'view_profile', 'send_message', 'post_content', 'like_content' with varying frequencies).\n2. **Load into SQLite**: Create an in-memory SQLite database using `sqlite3` and load `users_df` into a table named `users` and `activities_df` into `activities`.\n3. **SQL Feature Engineering (User Activity Aggregation)**: First, determine an `analysis_date` by finding the maximum `activity_date` in your `activities_df` (using pandas) and adding 30 days to it. Then, write a single SQL query that performs the following for *each user*:\n    *   **Joins** `users` and `activities` tables.\n    *   **Aggregates** user-level features: `total_activities` (count of all activities), `num_logins` (count of 'login' events), `num_messages_sent` (count of 'send_message' events), `num_content_posts` (count of 'post_content' events).\n    *   Calculates `first_activity_date` and `last_activity_date` for each user.\n    *   **Ensures** that all users are included, even those with no activities, showing appropriate default values (e.g., 0 for counts, `NULL` for dates).\n    *   The query should return `user_id`, `country`, `device_type`, `signup_date`, `total_activities`, `num_logins`, `num_messages_sent`, `num_content_posts`, `first_activity_date`, `last_activity_date`.\n4. **Retrieve, Merge, and Pandas Feature Engineering**: \n    *   Fetch the SQL query results into a pandas DataFrame.\n    *   Merge this aggregated DataFrame with the original `users_df` on `user_id`.\n    *   Handle `NaN` values resulting from the SQL query: Fill `total_activities`, `num_logins`, `num_messages_sent`, `num_content_posts` with 0 for users with no activities.\n    *   Convert all date columns to datetime objects. Calculate the following new features using the `analysis_date` from step 3:\n        *   `account_age_days`: Days between `signup_date` and `analysis_date`.\n        *   `days_since_last_activity`: Days between `last_activity_date` and `analysis_date`. For users with no activities, fill with a large sentinel value (e.g., `account_age_days` + 30).\n        *   `days_since_first_activity`: Days between `first_activity_date` and `analysis_date`. For users with no activities, fill with a large sentinel value (e.g., `account_age_days` + 30).\n    *   Define features `X` (`country`, `device_type`, `account_age_days`, `days_since_last_activity`, `days_since_first_activity`, `total_activities`, `num_logins`, `num_messages_sent`, `num_content_posts`) and target `y` (`engagement_level`). Split into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split` (set `random_state=42`).\n5. **Data Visualization**: Create two separate plots to visually inspect relationships with `engagement_level`:\n    *   A histogram showing the distribution of `total_activities` for each `engagement_level` (e.g., using `hue` in seaborn).\n    *   A bar plot or count plot showing the `engagement_level` counts across different `device_type`s.\n    Ensure plots have appropriate labels and titles.\n6. **ML Pipeline & Evaluation**: \n    *   Create an `sklearn.pipeline.Pipeline` with a `ColumnTransformer` for preprocessing:\n        *   For numerical features (e.g., `account_age_days`, `days_since_last_activity`, `days_since_first_activity`, `total_activities`, etc.): Apply `SimpleImputer(strategy='mean')` followed by `StandardScaler`.\n        *   For categorical features (`country`, `device_type`): Apply `OneHotEncoder(handle_unknown='ignore')`.\n    *   The final estimator in the pipeline should be `sklearn.linear_model.LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   Train the pipeline on the training data. Predict probabilities for the positive class (class 1) on the test set.\n    *   Calculate and print the `sklearn.metrics.roc_auc_score` and a `sklearn.metrics.classification_report` for the test set predictions.",
  "2026-01-23": "1. **Generate Synthetic Data**: Create two pandas DataFrames:\n    *   `products_df`: 100-150 rows. Columns: `product_id` (unique int), `category` (e.g., 'Electronics', 'Books', 'Home Goods', 'Clothing' with varying proportions), `price` (random floats between 50.0 and 500.0).\n    *   `reviews_df`: 800-1200 rows. Columns: `review_id` (unique int), `product_id` (randomly sampled from `products_df` IDs, ensuring multiple reviews per product), `review_text` (short text containing keywords like 'good', 'great', 'excellent', 'bad', 'terrible', 'slow', and '!' randomly, mixed with generic words), `rating` (random integers 1-5, biased towards 4-5).\n\n2. **Load into SQLite & SQL Analytics**: Create an in-memory SQLite database using `sqlite3`. Load `products_df` into a table named `products` and `reviews_df` into a table named `reviews`. Write a single SQL query that performs the following:\n    *   **Joins** `reviews` and `products` tables on `product_id`.\n    *   **Calculates `avg_product_rating`**: For each `review_id`, include the average `rating` for its corresponding `product_id` across *all* reviews for that product (using a window function `AVG(rating) OVER (PARTITION BY product_id)`).\n    *   The query should return `review_id`, `review_text`, `rating`, `category`, `price`, and `avg_product_rating`.\n\n3. **Pandas Feature Engineering & Target Creation**: Fetch the SQL query results into a pandas DataFrame. Create the following new features:\n    *   **Target Variable `sentiment`**: Create a binary target column `sentiment` where 1 if `rating >= 4` (positive) and 0 if `rating < 4` (negative). Drop the original `rating` column.\n    *   **Text Features from `review_text`**:\n        *   `review_length`: The length of the `review_text` string.\n        *   `has_exclamation`: Binary (1 if '!' is found in `review_text`, 0 otherwise).\n        *   `has_positive_word`: Binary (1 if 'good', 'great', or 'excellent' (case-insensitive) is found, 0 otherwise).\n        *   `has_negative_word`: Binary (1 if 'bad', 'terrible', or 'slow' (case-insensitive) is found, 0 otherwise).\n\n4. **Data Visualization**: Create two visualizations:\n    *   A box plot showing the distribution of `price` for each `sentiment` (0 and 1).\n    *   A bar plot (or count plot) showing the distribution of `sentiment` across different `category` values.\n    Ensure plots have appropriate labels and titles.\n\n5. **ML Pipeline & Evaluation**: \n    *   Define features `X` (all numerical, categorical, and binary text features created) and target `y` (`sentiment`) from the engineered DataFrame.\n    *   Split the data into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split` (set `random_state=42`).\n    *   Create an `sklearn.pipeline.Pipeline` with a `ColumnTransformer` for preprocessing:\n        *   For numerical features (`price`, `avg_product_rating`, `review_length`, `has_exclamation`): Apply `sklearn.preprocessing.StandardScaler`.\n        *   For the categorical feature (`category`): Apply `sklearn.preprocessing.OneHotEncoder(handle_unknown='ignore')`.\n        *   For binary features (`has_positive_word`, `has_negative_word`): Use `Passthrough`.\n    *   The final estimator in the pipeline should be `sklearn.linear_model.LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   Train the pipeline on the training data (`X_train`, `y_train`). Predict `sentiment` for the test set (`X_test`).\n    *   Calculate and print the `sklearn.metrics.accuracy_score` and `sklearn.metrics.classification_report` for the test set predictions.",
  "2026-01-24": "1. **Generate Synthetic Data**: Create two pandas DataFrames:\n    *   `users_df`: With 500 rows. Columns: `user_id` (unique integers), `signup_date` (random dates over the last 5 years), `country` (e.g., 'USA', 'Canada', 'UK', 'Germany' with random distribution), `subscription_type` (e.g., 'Free', 'Basic', 'Premium').\n    *   `activities_df`: With 3000-5000 rows. Columns: `activity_id` (unique integers), `user_id` (randomly sampled from `users_df` IDs), `activity_date` (random dates *after* `signup_date`), `activity_type` (e.g., 'login', 'view_item', 'post_comment', 'upgrade_plan' with varying frequencies).\n\n2. **Load into SQLite & SQL Analytics**: Create an in-memory SQLite database. Load `users_df` into a table named `users` and `activities_df` into a table named `activities`. Determine an `analysis_date` (e.g., `max(activity_date)` from `activities_df` + 30 days). Write a single SQL query that performs the following for *each user*:\n    *   **Joins** `users` and `activities` tables.\n    *   **Aggregates** user-level features: `total_activities` (count of all activities), `num_logins` (count of 'login' events), `num_comments_posted` (count of 'post_comment' events).\n    *   Calculates `days_since_last_activity`: Days between `analysis_date` (passed as a parameter or calculated within SQL if possible via date functions) and `MAX(activity_date)`.\n    *   **Ensures** all users are included, showing 0 for counts and `NULL` for `days_since_last_activity` if no activities.\n    *   Returns `user_id`, `country`, `subscription_type`, `signup_date`, `total_activities`, `num_logins`, `num_comments_posted`, `days_since_last_activity`.\n\n3. **Pandas Feature Engineering & Multi-Class Target Creation**: Fetch the SQL query results into a pandas DataFrame. \n    *   Merge with `users_df` (if necessary, to ensure all original user features like `country`, `subscription_type` are present even for users with no activities) using a left join on `user_id`.\n    *   Handle `NaN` values: Fill `total_activities`, `num_logins`, `num_comments_posted` with 0. For `days_since_last_activity` (for users with no activities), fill with a large sentinel value (e.g., `365 * 5` or 1825 days).\n    *   Calculate `account_age_days`: Days between `signup_date` and the `analysis_date` (from step 2).\n    *   **Create the multi-class target `activity_segment`**: Based on `total_activities` and `days_since_last_activity`. First, calculate quantiles for `total_activities`. Then, define segments:\n        *   'High_Activity': `total_activities` is in the top 33% *and* `days_since_last_activity` < 30 days.\n        *   'Medium_Activity': `total_activities` is in the middle 34-66% *or* `days_since_last_activity` between 30 and 90 days (excluding those already classified as 'High_Activity').\n        *   'Low_Activity': All remaining users (including those with 0 activities or `days_since_last_activity` >= 90 days).\n    *   Define features `X` (numerical: `account_age_days`, `total_activities`, `num_logins`, `num_comments_posted`, `days_since_last_activity`; categorical: `country`, `subscription_type`) and target `y` (`activity_segment`). Split into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split` (set `random_state=42`).\n\n4. **Data Visualization**: Create two separate plots to visually inspect relationships with `activity_segment`:\n    *   A violin plot or box plot showing the distribution of `days_since_last_activity` for each `activity_segment`.\n    *   A bar plot or count plot showing the distribution of `activity_segment` across different `subscription_type`s.\n    Ensure plots have appropriate labels and titles.\n\n5. **ML Pipeline & Evaluation (Multi-Class)**: \n    *   Create an `sklearn.pipeline.Pipeline` with a `ColumnTransformer` for preprocessing:\n        *   For numerical features: Apply `StandardScaler`.\n        *   For categorical features: Apply `OneHotEncoder(handle_unknown='ignore')`.\n    *   The final estimator should be `sklearn.ensemble.RandomForestClassifier` (set `random_state=42`, `n_estimators=100`, `class_weight='balanced'` for potential class imbalance).\n    *   Train the pipeline on `X_train`, `y_train`. Predict `activity_segment` for `X_test`.\n    *   Calculate and print the `sklearn.metrics.accuracy_score` and `sklearn.metrics.classification_report` for the test set predictions.",
  "2026-01-25": "1. **Generate Synthetic Data (Pandas/Numpy)**: Create two pandas DataFrames:\n    *   `customers_df`: With 500 rows. Columns: `customer_id` (unique integers), `signup_date` (random dates over the last 5 years), `region` (e.g., 'North', 'South', 'East', 'West'), `age` (random integers 18-70).\n    *   `transactions_df`: With 3000-5000 rows. Columns: `transaction_id` (unique integers), `customer_id` (randomly sampled from `customers_df` IDs, ensuring some customers have only 1 transaction, others 2, and many more), `transaction_date` (random dates *after* their respective `signup_date`), `amount` (random floats between 10.0 and 1000.0), `item_count` (random integers 1-10).\n    Ensure `transactions_df` is sorted by `customer_id` and `transaction_date`.\n\n2. **Load into SQLite & SQL Feature Engineering (Initial Buyer Behavior)**: Create an in-memory SQLite database. Load `customers_df` into a table named `customers` and `transactions_df` into a table named `transactions`. Define an `initial_period_days` (e.g., 30 days).\n    Write a single SQL query that performs the following for *each customer*:\n    *   **Joins** `customers` and `transactions` tables.\n    *   **Aggregates** initial purchase behavior for transactions occurring within the `initial_period_days` from their `signup_date`:\n        *   `initial_total_spend` (sum of `amount`)\n        *   `initial_num_transactions` (count of transactions)\n        *   `initial_avg_item_count` (average `item_count`)\n    *   **Ensures** all customers are included, showing 0 for aggregates if no transactions in the initial period.\n    *   The query should return `customer_id`, `region`, `age`, `signup_date`, `initial_total_spend`, `initial_num_transactions`, `initial_avg_item_count`.\n\n3. **Pandas Feature Engineering & Target Creation (Repeat Buyer)**: Fetch the SQL query results into a pandas DataFrame. Merge it with the original `transactions_df` to get full transaction history (if not already joined properly).\n    *   Handle `NaN` values: Fill `initial_total_spend`, `initial_num_transactions`, `initial_avg_item_count` with 0 for customers with no initial transactions.\n    *   Calculate `account_age_days`: The number of days between `signup_date` and the latest `transaction_date` in the *entire* `transactions_df`.\n    *   **Create Binary Target `is_repeat_buyer`**: A customer is a `repeat_buyer` (1) if they have made at least 2 transactions *and* the `MAX(transaction_date)` is at least 60 days after their `MIN(transaction_date)`. Otherwise, `0`.\n    *   Define features `X` (`region`, `age`, `account_age_days`, `initial_total_spend`, `initial_num_transactions`, `initial_avg_item_count`) and target `y` (`is_repeat_buyer`). Split into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split` (set `random_state=42`).\n\n4. **Data Visualization**: Create two separate plots to visually inspect relationships with `is_repeat_buyer`:\n    *   A violin plot or box plot showing the distribution of `initial_total_spend` for each `is_repeat_buyer` group.\n    *   A bar plot or count plot showing the distribution of `is_repeat_buyer` across different `region`s.\n    Ensure plots have appropriate labels and titles.\n\n5. **ML Pipeline & Evaluation**: Create an `sklearn.pipeline.Pipeline` with a `ColumnTransformer` for preprocessing:\n    *   For numerical features (`age`, `account_age_days`, `initial_total_spend`, `initial_num_transactions`, `initial_avg_item_count`): Apply `sklearn.preprocessing.StandardScaler`.\n    *   For categorical features (`region`): Apply `sklearn.preprocessing.OneHotEncoder(handle_unknown='ignore')`.\n    *   The final estimator should be `sklearn.linear_model.LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   Train the pipeline on `X_train`, `y_train`. Predict probabilities for the positive class (class 1) on `X_test`.\n    *   Calculate and print the `sklearn.metrics.roc_auc_score` and `sklearn.metrics.classification_report` for the test set predictions.",
  "2026-01-26": "1. **Generate Synthetic Data (Pandas/Numpy)**: Create two pandas DataFrames:\n    *   `customers_df`: With 500 rows. Columns: `customer_id` (unique integers), `signup_date` (random dates over the last 5 years), `region` (e.g., 'North', 'South', 'East', 'West'), `age` (random integers 18-70).\n    *   `transactions_df`: With 3000-5000 rows. Columns: `transaction_id` (unique integers), `customer_id` (randomly sampled from `customers_df` IDs, ensuring some customers have many transactions and a few have no transactions), `transaction_date` (random dates *after* their respective `signup_date`), `amount` (random floats between 10.0 and 1000.0).\n\n2. **Load into SQLite & SQL Feature Engineering (RFM)**: Create an in-memory SQLite database. Load `customers_df` into a table named `customers` and `transactions_df` into a table named `transactions`. Determine an `analysis_date` (e.g., the latest `transaction_date` from `transactions_df` + 30 days, using pandas).\n    Write a single SQL query that calculates Recency, Frequency, and Monetary (RFM) values for *each customer*:\n    *   `recency_days`: Number of days between the `analysis_date` and the customer's `MAX(transaction_date)`. If no transactions, this should be `NULL`.\n    *   `frequency`: Total number of transactions for the customer.\n    *   `monetary`: Sum of all `amount`s for the customer.\n    *   **Ensures** all customers are included, showing 0 for `frequency` and `monetary`, and `NULL` for `recency_days` if no transactions.\n    *   The query should return `customer_id`, `region`, `age`, `signup_date`, `recency_days`, `frequency`, `monetary`.\n\n3. **Pandas Feature Engineering & Target Creation**: Fetch the SQL query results into a pandas DataFrame. \n    *   Handle `NaN` values: Fill `frequency` and `monetary` with 0. For `recency_days` (for customers with no transactions), fill with a large sentinel value, e.g., `365 * 5` (1825 days).\n    *   Calculate `account_age_days`: Days between `signup_date` and the `analysis_date` (from step 2).\n    *   **Create Binary Target `is_high_value_customer`**: A customer is 'high value' (1) if their `monetary` value is in the top 30% *and* their `frequency` is in the top 30%. Otherwise, 0. (Hint: Use `quantile()` to find thresholds).\n    *   Define features `X` (`region`, `age`, `account_age_days`, `recency_days`, `frequency`, `monetary`) and target `y` (`is_high_value_customer`). Split into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split` (set `random_state=42`).\n\n4. **Data Visualization**: Create two separate plots to visually inspect relationships with `is_high_value_customer`:\n    *   A violin plot or box plot showing the distribution of `recency_days` for each `is_high_value_customer` group.\n    *   A bar plot or count plot showing the distribution of `is_high_value_customer` across different `region`s.\n    Ensure plots have appropriate labels and titles.\n\n5. **ML Pipeline & Evaluation**: \n    *   Create an `sklearn.pipeline.Pipeline` with a `ColumnTransformer` for preprocessing:\n        *   For numerical features (`age`, `account_age_days`, `recency_days`, `frequency`, `monetary`): Apply `sklearn.preprocessing.StandardScaler`.\n        *   For the categorical feature (`region`): Apply `sklearn.preprocessing.OneHotEncoder(handle_unknown='ignore')`.\n    *   The final estimator in the pipeline should be `sklearn.linear_model.LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   Train the pipeline on the training data (`X_train`, `y_train`). Predict probabilities for the positive class (class 1) on the test set (`X_test`).\n    *   Calculate and print the `sklearn.metrics.roc_auc_score` for the test set predictions.\n    *   Generate and display an ROC curve for the model using `sklearn.metrics.RocCurveDisplay.from_estimator` with the trained pipeline and test data."
}