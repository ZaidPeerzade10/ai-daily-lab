{
  "task": "1. **Generate Synthetic Data (Pandas/Numpy)**: Create two pandas DataFrames:\n    *   `subscriptions_df`: With 500-700 rows. Columns: `user_id` (unique integers), `signup_date` (random dates over the last 3-5 years), `plan_type` (e.g., 'Basic', 'Premium', 'Enterprise' with random distribution), `monthly_charge` (float, e.g., Basic: 10-20, Premium: 30-50, Enterprise: 80-120), `is_churned` (binary target, 0 or 1, with an approximate 20-30% churn rate), `region` (e.g., 'North', 'South', 'East', 'West').\n    *   `usage_logs_df`: With 4000-6000 rows. Columns: `log_id` (unique integers), `user_id` (randomly sampled from `subscriptions_df` IDs, ensuring some users have many logs and a few have no logs), `log_date` (random dates occurring *after* their respective `signup_date` from `subscriptions_df`), `feature_used` (e.g., 'Feature_A', 'Feature_B', 'Feature_C', 'Feature_D' with varying frequencies), `usage_minutes` (random floats 1-60).\n    *   **Simulate churn behavior**: Churned users (`is_churned=1`) should generally have fewer `usage_logs` and their `log_date`s should be concentrated earlier in their subscription history, with few or no logs in the last few months before `analysis_date`.\n\n2. **Load into SQLite & SQL Feature Engineering**: Create an in-memory SQLite database using `sqlite3`. Load `subscriptions_df` into a table named `subscriptions` and `usage_logs_df` into a table named `usage_logs`. Determine an `analysis_date` (e.g., `max(log_date)` from `usage_logs_df` + 30 days, using pandas).\n    Write a single SQL query that performs the following for *each user*:\n    *   **Joins** `subscriptions` and `usage_logs` tables.\n    *   **Aggregates** user-level features:\n        *   `total_usage_minutes` (sum of `usage_minutes`).\n        *   `num_log_entries` (count of all `log_id`s).\n        *   `num_feature_A_uses`, `num_feature_B_uses`, `num_feature_C_uses`, `num_feature_D_uses` (count of `log_id`s for each respective `feature_used`).\n        *   `days_since_last_activity`: Number of days between the `analysis_date` and the user's `MAX(log_date)`.\n    *   **Ensures** all users are included, showing 0 for counts/sums and `NULL` for `days_since_last_activity` if no usage logs.\n    *   The query should return `user_id`, `region`, `plan_type`, `monthly_charge`, `signup_date`, `is_churned`, `total_usage_minutes`, `num_log_entries`, `num_feature_A_uses`, `num_feature_B_uses`, `num_feature_C_uses`, `num_feature_D_uses`, `days_since_last_activity`.\n\n3. **Pandas Feature Engineering & Data Preparation**: Fetch the SQL query results into a pandas DataFrame. \n    *   Handle `NaN` values: Fill `total_usage_minutes`, `num_log_entries`, and all `num_feature_X_uses` columns with 0. For `days_since_last_activity` (for users with no activities), fill with a large sentinel value (e.g., `365 * 5` or 1825 days).\n    *   Calculate `account_age_days`: Days between `signup_date` and the `analysis_date` (from step 2).\n    *   Define features `X` (all numerical and categorical features engineered) and target `y` (`is_churned`). Split into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split` (set `random_state=42`).\n\n4. **Data Visualization**: Create two separate plots to visually inspect relationships with `is_churned`:\n    *   A violin plot (or box plot) showing the distribution of `days_since_last_activity` for users with `is_churned=0` vs. `is_churned=1`.\n    *   A stacked bar chart or count plot showing the distribution of `is_churned` across different `plan_type`s.\n    Ensure plots have appropriate labels and titles.\n\n5. **ML Pipeline & Evaluation (Basic AI Experimentation)**: \n    *   Create an `sklearn.pipeline.Pipeline` with a `ColumnTransformer` for preprocessing:\n        *   For numerical features (e.g., `monthly_charge`, `account_age_days`, `total_usage_minutes`, `num_log_entries`, `days_since_last_activity`, `num_feature_X_uses`): Apply `sklearn.preprocessing.StandardScaler`.\n        *   For categorical features (`region`, `plan_type`): Apply `sklearn.preprocessing.OneHotEncoder(handle_unknown='ignore')`.\n    *   The final estimator in the pipeline should be `sklearn.ensemble.GradientBoostingClassifier` (set `random_state=42`, `n_estimators=100`, `learning_rate=0.1`).\n    *   Train the pipeline on the training data (`X_train`, `y_train`). Predict probabilities for the positive class (class 1) on the test set (`X_test`).\n    *   Calculate and print the `sklearn.metrics.roc_auc_score` and a `sklearn.metrics.classification_report` for the test set predictions.",
  "focus": "SQL analytics (complex aggregations), Pandas feature engineering (datetime, conditional filling), chronological data preparation, ML pipelines with `ColumnTransformer`, advanced classification model (`GradientBoostingClassifier`), and visualization for churn analysis.",
  "dataset": "Synthetic data for subscription users and their feature usage logs, designed to reflect churn behavior.",
  "hint": "For the SQL query in step 2, to count feature-specific usage, you can use `SUM(CASE WHEN feature_used = 'Feature_A' THEN 1 ELSE 0 END)` within your `GROUP BY user_id` aggregation. Remember to handle users with no usage logs gracefully in the SQL `LEFT JOIN` and subsequent pandas `fillna` steps.",
  "date": "2026-01-29",
  "timestamp": "2026-01-29T05:01:54.321596Z"
}