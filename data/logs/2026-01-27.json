{
  "task": "1. **Generate Synthetic A/B Test Data (Pandas/Numpy)**: Create two pandas DataFrames:\n    *   `users_df`: With 500-700 rows. Columns: `user_id` (unique integers), `signup_date` (random dates over the last 3 years), `test_group` (string: 'Control', 'Treatment', with an even split), `device_type` (string: 'Mobile', 'Desktop', 'Tablet').\n    *   `activities_df`: With 3000-5000 rows. Columns: `activity_id` (unique integers), `user_id` (randomly sampled from `users_df` IDs, ensuring some users have many activities and a few have no activities), `activity_date` (random dates occurring *after* their respective `signup_date`), `activity_type` (string: 'page_view', 'add_to_cart', 'purchase', 'login' with varying frequencies).\n    *   **Crucially, simulate an A/B test effect**: Make the 'Treatment' group have a slightly higher proportion of 'purchase' activities than the 'Control' group.\n\n2. **Load into SQLite & SQL Feature Engineering**: Create an in-memory SQLite database using `sqlite3`. Load `users_df` into a table named `users` and `activities_df` into a table named `activities`. Determine an `analysis_date` (e.g., the maximum `activity_date` in `activities_df` + 30 days, using pandas).\n    Write a single SQL query that performs the following for *each user*:\n    *   **Joins** `users` and `activities` tables.\n    *   **Aggregates** user-level features: `total_activities` (count of all activities), `num_logins` (count of 'login' events), `has_purchased` (binary: 1 if any 'purchase' event, 0 otherwise), `first_activity_date`, `last_activity_date`.\n    *   **Ensures** that all users are included (even those with no activities), showing appropriate default values (e.g., 0 for counts, `NULL` for dates).\n    *   The query should return `user_id`, `signup_date`, `test_group`, `device_type`, `total_activities`, `num_logins`, `has_purchased`, `first_activity_date`, `last_activity_date`.\n\n3. **Pandas Feature Engineering & Target Creation**: Fetch the SQL query results into a pandas DataFrame. \n    *   Handle `NaN` values: Fill `total_activities`, `num_logins`, `has_purchased` with 0 for users with no activities. For `first_activity_date` and `last_activity_date` (for users with no activities), fill with their `signup_date`.\n    *   Convert all date columns to datetime objects. Calculate the following new features using the `analysis_date` from step 2:\n        *   `account_age_days`: Days between `signup_date` and `analysis_date`.\n        *   `days_since_last_activity`: Days between `last_activity_date` and `analysis_date`. For users with no activities, fill with a large sentinel value (e.g., `account_age_days` + 30).\n    *   Define features `X` (`test_group`, `device_type`, `account_age_days`, `days_since_last_activity`, `total_activities`, `num_logins`) and target `y` (`has_purchased`). Split into training and testing sets (e.g., 70/30 split) using `sklearn.model_selection.train_test_split` (set `random_state=42`).\n\n4. **Data Visualization**: Create two separate plots to visually inspect the A/B test results and relationships with the target:\n    *   A bar plot comparing the `conversion_rate` (mean of `has_purchased`) between 'Control' and 'Treatment' groups. Include a title like 'Conversion Rate by Test Group'.\n    *   A violin plot (or box plot) showing the distribution of `days_since_last_activity` for users who `has_purchased=0` vs. `has_purchased=1` (e.g., using `hue` in seaborn). Ensure appropriate labels and titles.\n\n5. **ML Pipeline & Evaluation**: \n    *   Create an `sklearn.pipeline.Pipeline` with a `ColumnTransformer` for preprocessing:\n        *   For numerical features (`account_age_days`, `days_since_last_activity`, `total_activities`, `num_logins`): Apply `sklearn.preprocessing.StandardScaler`.\n        *   For categorical features (`test_group`, `device_type`): Apply `sklearn.preprocessing.OneHotEncoder(handle_unknown='ignore')`.\n    *   The final estimator in the pipeline should be `sklearn.linear_model.LogisticRegression` (set `random_state=42`, `solver='liblinear'` for reproducibility).\n    *   Train the pipeline on the training data (`X_train`, `y_train`). Predict probabilities for the positive class (class 1) on the test set (`X_test`).\n    *   Calculate and print the `sklearn.metrics.roc_auc_score` and a `sklearn.metrics.classification_report` for the test set predictions.",
  "focus": "A/B Test Analysis, SQL Aggregation for User Behavior, Feature Engineering, Binary Classification with ML Pipelines, Data Visualization",
  "dataset": "Synthetic user activity data for an A/B test campaign.",
  "hint": "When simulating the A/B test effect, you can create activities for the 'Treatment' group and then for 'Control', or use a conditional probability for 'purchase' events based on the `test_group` in your data generation loop. For SQL `COUNT` with `CASE WHEN`, use `COALESCE` for `NULL` values after aggregation. Remember to convert relevant columns to datetime objects in pandas before calculating date differences.",
  "date": "2026-01-27",
  "timestamp": "2026-01-27T04:40:47.033219Z"
}